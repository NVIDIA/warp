

<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Interoperability &#8212; Warp 1.11.1</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/nvidia-sphinx-theme.css?v=933278ad" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=2ce81c1f" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />



    <script src="../_static/documentation_options.js?v=c8897f99"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=fd10adb8"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'user_guide/interoperability';</script>

    <link rel="icon" href="../_static/favicon.png"/>

    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Configuration" href="configuration.html" />
    <link rel="prev" title="Tiles" href="tiles.html" />


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="1.11.1" />


  </head>

  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>


  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/nvidia-logo-horiz-rgb-blk-for-screen.svg" class="logo__image only-light" alt="Warp 1.11.1 - Home"/>
    <img src="../_static/nvidia-logo-horiz-rgb-wht-for-screen.svg" class="logo__image only-dark pst-js-only" alt="Warp 1.11.1 - Home"/>
  
  
    <p class="title logo__title">Warp 1.11.1</p>
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/NVIDIA/warp" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-square-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/warp-lang" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        



  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/nvidia-logo-horiz-rgb-blk-for-screen.svg" class="logo__image only-light" alt="Warp 1.11.1 - Home"/>
    <img src="../_static/nvidia-logo-horiz-rgb-wht-for-screen.svg" class="logo__image only-dark pst-js-only" alt="Warp 1.11.1 - Home"/>
  
  
    <p class="title logo__title">Warp 1.11.1</p>
  
</a>


  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/NVIDIA/warp" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-square-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/warp-lang" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">



<nav class="bd-docs-nav bd-links"
     aria-label="Table of Contents">
  <p class="bd-links__title" role="heading" aria-level="1">Table of Contents</p>
  <div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="basics.html">Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime.html">Runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="devices.html">Devices</a></li>
<li class="toctree-l1"><a class="reference internal" href="differentiability.html">Differentiability</a></li>
<li class="toctree-l1"><a class="reference internal" href="generics.html">Generics</a></li>
<li class="toctree-l1"><a class="reference internal" href="tiles.html">Tiles</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Interoperability</a></li>
<li class="toctree-l1"><a class="reference internal" href="configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="debugging.html">Debugging</a></li>
<li class="toctree-l1"><a class="reference internal" href="limitations.html">Limitations</a></li>
<li class="toctree-l1"><a class="reference internal" href="contribution_guide.html">Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="publications.html">Publications using Warp</a></li>
<li class="toctree-l1"><a class="reference internal" href="compatibility.html">Compatibility &amp; Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="changelog.html">Changelog</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Dive</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../deep_dive/codegen.html">Code Generation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deep_dive/allocators.html">Allocators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deep_dive/concurrency.html">Concurrency</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deep_dive/profiling.html">Profiling</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Domain Modules</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../domain_modules/sparse.html">Sparse Matrices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../domain_modules/fem.html">FEM Toolkit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../domain_modules/render.html">Rendering</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../api_reference/warp.html">warp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_reference/warp_autograd.html">warp.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_reference/warp_config.html">warp.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_reference/warp_fem.html">warp.fem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_reference/warp_jax_experimental.html">warp.jax_experimental</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_reference/warp_optim.html">warp.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_reference/warp_render.html">warp.render</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_reference/warp_sparse.html">warp.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_reference/warp_types.html">warp.types</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_reference/warp_utils.html">warp.utils</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Language Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../language_reference/builtins.html">Built-Ins</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Project Links</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://github.com/NVIDIA/warp">GitHub</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pypi.org/project/warp-lang">PyPI</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>



      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">Interoperability</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="interoperability">
<h1>Interoperability<a class="headerlink" href="#interoperability" title="Link to this heading">#</a></h1>
<p>Warp can interoperate with other Python-based frameworks such as NumPy through standard interface protocols.</p>
<p>Warp supports passing external arrays to kernels directly, as long as they implement the <code class="docutils literal notranslate"><span class="pre">__array__</span></code>, <code class="docutils literal notranslate"><span class="pre">__array_interface__</span></code>, or <code class="docutils literal notranslate"><span class="pre">__cuda_array_interface__</span></code> protocols.  This works with many common frameworks like NumPy, CuPy, or PyTorch.</p>
<p>For example, we can use NumPy arrays directly when launching Warp kernels on the CPU:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warp</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">wp</span>

<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">saxpy</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span> <span class="n">y</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span> <span class="n">a</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">saxpy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Likewise, we can use CuPy arrays on a CUDA device:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">cupy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">cp</span>

<span class="k">with</span> <span class="n">cp</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Device</span><span class="p">(</span><span class="mi">0</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">cp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">cp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">saxpy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that with CUDA arrays, it’s important to ensure that the device on which the arrays reside is the same as the device on which the kernel is launched.</p>
<p>PyTorch supports both CPU and GPU tensors and both kinds can be passed to Warp kernels on the appropriate device.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">random</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">]):</span>
    <span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda:0&quot;</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">saxpy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<section id="numpy">
<h2>NumPy<a class="headerlink" href="#numpy" title="Link to this heading">#</a></h2>
<p>Warp arrays may be converted to a NumPy array through the <a class="reference internal" href="../api_reference/_generated/warp.array.html#warp.array.numpy" title="warp.array.numpy"><code class="xref py py-meth docutils literal notranslate"><span class="pre">array.numpy()</span></code></a> method. When the Warp array lives on
the <code class="docutils literal notranslate"><span class="pre">cpu</span></code> device this will return a zero-copy view onto the underlying Warp allocation. If the array lives on a
<code class="docutils literal notranslate"><span class="pre">cuda</span></code> device then it will first be copied back to a temporary buffer and copied to NumPy.</p>
<p>Warp CPU arrays also implement  the <code class="docutils literal notranslate"><span class="pre">__array_interface__</span></code> protocol and so can be used to construct NumPy arrays
directly:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="p">[</span><span class="mf">1.</span> <span class="mf">2.</span> <span class="mf">3.</span><span class="p">]</span>
</pre></div>
</div>
<p>Data type conversion utilities are also available for convenience:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">warp_type</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">float32</span>
<span class="o">...</span>
<span class="n">numpy_type</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">dtype_to_numpy</span><span class="p">(</span><span class="n">warp_type</span><span class="p">)</span>
<span class="o">...</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">warp_type</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">numpy_type</span><span class="p">)</span>
</pre></div>
</div>
<p>To create Warp arrays from NumPy arrays, use <a class="reference internal" href="../api_reference/_generated/warp.from_numpy.html#warp.from_numpy" title="warp.from_numpy"><code class="xref py py-func docutils literal notranslate"><span class="pre">warp.from_numpy()</span></code></a>
or pass the NumPy array as the <code class="docutils literal notranslate"><span class="pre">data</span></code> argument of the <a class="reference internal" href="../api_reference/_generated/warp.array.html#warp.array" title="warp.array"><code class="xref py py-class docutils literal notranslate"><span class="pre">warp.array</span></code></a> constructor directly.</p>
</section>
<section id="pytorch">
<span id="pytorch-interop"></span><h2>PyTorch<a class="headerlink" href="#pytorch" title="Link to this heading">#</a></h2>
<p>Warp provides helper functions to convert arrays to/from PyTorch:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

<span class="c1"># convert to Torch tensor</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">to_torch</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>

<span class="c1"># convert from Torch tensor</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</pre></div>
</div>
<p>These helper functions allow the conversion of Warp arrays to/from PyTorch tensors without copying the underlying data.
At the same time, if available, gradient arrays and tensors are converted to/from PyTorch autograd tensors, allowing the use of Warp arrays
in PyTorch autograd computations.</p>
<p>To convert a PyTorch CUDA stream to a Warp CUDA stream and vice versa, Warp provides the following functions:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../api_reference/_generated/warp.stream_from_torch.html#warp.stream_from_torch" title="warp.stream_from_torch"><code class="xref py py-func docutils literal notranslate"><span class="pre">warp.stream_from_torch()</span></code></a></p></li>
<li><p><a class="reference internal" href="../api_reference/_generated/warp.stream_to_torch.html#warp.stream_to_torch" title="warp.stream_to_torch"><code class="xref py py-func docutils literal notranslate"><span class="pre">warp.stream_to_torch()</span></code></a></p></li>
</ul>
<section id="example-optimization-using-warp-from-torch">
<h3>Example: Optimization using <a class="reference internal" href="../api_reference/_generated/warp.from_torch.html#warp.from_torch" title="warp.from_torch"><code class="xref py py-func docutils literal notranslate"><span class="pre">warp.from_torch()</span></code></a><a class="headerlink" href="#example-optimization-using-warp-from-torch" title="Link to this heading">#</a></h3>
<p>An example usage of minimizing a loss function over an array of 2D points written in Warp via PyTorch’s Adam optimizer
using <a class="reference internal" href="../api_reference/_generated/warp.from_torch.html#warp.from_torch" title="warp.from_torch"><code class="xref py py-func docutils literal notranslate"><span class="pre">warp.from_torch()</span></code></a> is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">warp</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">wp</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>


<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span><span class="p">()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">loss</span><span class="p">(</span><span class="n">xs</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">ndim</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">l</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">atomic_add</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">xs</span><span class="p">[</span><span class="n">tid</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">**</span> <span class="mf">2.0</span> <span class="o">+</span> <span class="n">xs</span><span class="p">[</span><span class="n">tid</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="mf">2.0</span><span class="p">)</span>

<span class="c1"># indicate requires_grad so that Warp can accumulate gradients in the grad buffers</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">l</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">xs</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">wp_xs</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
<span class="n">wp_l</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>

<span class="n">tape</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">Tape</span><span class="p">()</span>
<span class="k">with</span> <span class="n">tape</span><span class="p">:</span>
    <span class="c1"># record the loss function kernel launch on the tape</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">wp_xs</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">wp_l</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">wp_xs</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="n">tape</span><span class="o">.</span><span class="n">zero</span><span class="p">()</span>
    <span class="n">tape</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">wp_l</span><span class="p">)</span>  <span class="c1"># compute gradients</span>
    <span class="c1"># now xs.grad will be populated with the gradients computed by Warp</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># update xs (and thereby wp_xs)</span>

    <span class="c1"># these lines are only needed for evaluating the loss</span>
    <span class="c1"># (the optimization just needs the gradient, not the loss value)</span>
    <span class="n">wp_l</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">wp_xs</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">wp_l</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">wp_xs</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="se">\t</span><span class="s2">loss: </span><span class="si">{</span><span class="n">l</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="example-optimization-using-warp-to-torch">
<h3>Example: Optimization using <a class="reference internal" href="../api_reference/_generated/warp.to_torch.html#warp.to_torch" title="warp.to_torch"><code class="xref py py-func docutils literal notranslate"><span class="pre">warp.to_torch()</span></code></a><a class="headerlink" href="#example-optimization-using-warp-to-torch" title="Link to this heading">#</a></h3>
<p>Less code is needed when we declare the optimization variables directly in Warp and use <a class="reference internal" href="../api_reference/_generated/warp.to_torch.html#warp.to_torch" title="warp.to_torch"><code class="xref py py-func docutils literal notranslate"><span class="pre">warp.to_torch()</span></code></a> to convert them to PyTorch tensors.
Here, we revisit the same example from above where now only a single conversion to a PyTorch tensor is needed to supply Adam with the optimization variables:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">warp</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">wp</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>


<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span><span class="p">()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">loss</span><span class="p">(</span><span class="n">xs</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">ndim</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">l</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">atomic_add</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">xs</span><span class="p">[</span><span class="n">tid</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">**</span> <span class="mf">2.0</span> <span class="o">+</span> <span class="n">xs</span><span class="p">[</span><span class="n">tid</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="mf">2.0</span><span class="p">)</span>

<span class="c1"># initialize the optimization variables in Warp</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">l</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># just a single wp.to_torch call is needed, Adam optimizes using the Warp array gradients</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">wp</span><span class="o">.</span><span class="n">to_torch</span><span class="p">(</span><span class="n">xs</span><span class="p">)],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">tape</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">Tape</span><span class="p">()</span>
<span class="k">with</span> <span class="n">tape</span><span class="p">:</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">xs</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">xs</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="n">tape</span><span class="o">.</span><span class="n">zero</span><span class="p">()</span>
    <span class="n">tape</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">l</span><span class="p">)</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="n">l</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">xs</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">xs</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="se">\t</span><span class="s2">loss: </span><span class="si">{</span><span class="n">l</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="example-optimization-using-torch-autograd-function-pytorch-2-3-1">
<h3>Example: Optimization using <code class="docutils literal notranslate"><span class="pre">torch.autograd.function</span></code> (PyTorch &lt;= 2.3.1)<a class="headerlink" href="#example-optimization-using-torch-autograd-function-pytorch-2-3-1" title="Link to this heading">#</a></h3>
<p>One can insert Warp kernel launches in a PyTorch graph by defining a <a class="reference external" href="https://docs.pytorch.org/docs/stable/autograd.html#torch.autograd.Function" title="(in PyTorch v2.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code></a> class, which
requires forward and backward functions to be defined. After mapping incoming PyTorch tensors to Warp arrays, a Warp kernel
may be launched in the usual way. In the backward pass, the same kernel’s adjoint may be launched by
setting <code class="docutils literal notranslate"><span class="pre">adjoint</span> <span class="pre">=</span> <span class="pre">True</span></code> in <a class="reference internal" href="../api_reference/_generated/warp.launch_function.html#warp.launch" title="warp.launch"><code class="xref py py-func docutils literal notranslate"><span class="pre">wp.launch()</span></code></a>. Alternatively, the user may choose to rely on Warp’s tape.
In the following example, we demonstrate how Warp may be used to evaluate the Rosenbrock function in an optimization context.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">warp</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">wp</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># Define the Rosenbrock function</span>
<span class="nd">@wp</span><span class="o">.</span><span class="n">func</span>
<span class="k">def</span><span class="w"> </span><span class="nf">rosenbrock</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span> <span class="o">**</span> <span class="mf">2.0</span> <span class="o">+</span> <span class="mf">100.0</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">x</span><span class="o">**</span><span class="mf">2.0</span><span class="p">)</span> <span class="o">**</span> <span class="mf">2.0</span>

<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">eval_rosenbrock</span><span class="p">(</span>
    <span class="n">xs</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">vec2</span><span class="p">),</span>
    <span class="c1"># outputs</span>
    <span class="n">z</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
<span class="p">):</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">xs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">rosenbrock</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>


<span class="k">class</span><span class="w"> </span><span class="nc">Rosenbrock</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">xy</span><span class="p">,</span> <span class="n">num_points</span><span class="p">):</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">xy</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">xy</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">vec2</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">num_points</span> <span class="o">=</span> <span class="n">num_points</span>

        <span class="c1"># allocate output</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_points</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span>
            <span class="n">kernel</span><span class="o">=</span><span class="n">eval_rosenbrock</span><span class="p">,</span>
            <span class="n">dim</span><span class="o">=</span><span class="n">ctx</span><span class="o">.</span><span class="n">num_points</span><span class="p">,</span>
            <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">ctx</span><span class="o">.</span><span class="n">xy</span><span class="p">],</span>
            <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">ctx</span><span class="o">.</span><span class="n">z</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">wp</span><span class="o">.</span><span class="n">to_torch</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">z</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">adj_z</span><span class="p">):</span>
        <span class="c1"># map incoming Torch grads to our output variables</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">z</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">adj_z</span><span class="p">)</span>

        <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span>
            <span class="n">kernel</span><span class="o">=</span><span class="n">eval_rosenbrock</span><span class="p">,</span>
            <span class="n">dim</span><span class="o">=</span><span class="n">ctx</span><span class="o">.</span><span class="n">num_points</span><span class="p">,</span>
            <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">ctx</span><span class="o">.</span><span class="n">xy</span><span class="p">],</span>
            <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">ctx</span><span class="o">.</span><span class="n">z</span><span class="p">],</span>
            <span class="n">adj_inputs</span><span class="o">=</span><span class="p">[</span><span class="n">ctx</span><span class="o">.</span><span class="n">xy</span><span class="o">.</span><span class="n">grad</span><span class="p">],</span>
            <span class="n">adj_outputs</span><span class="o">=</span><span class="p">[</span><span class="n">ctx</span><span class="o">.</span><span class="n">z</span><span class="o">.</span><span class="n">grad</span><span class="p">],</span>
            <span class="n">adjoint</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

        <span class="c1"># return adjoint w.r.t. inputs</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">wp</span><span class="o">.</span><span class="n">to_torch</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">xy</span><span class="o">.</span><span class="n">grad</span><span class="p">),</span> <span class="kc">None</span><span class="p">)</span>


<span class="n">num_points</span> <span class="o">=</span> <span class="mi">1500</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">5e-2</span>

<span class="n">torch_device</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">device_to_torch</span><span class="p">(</span><span class="n">wp</span><span class="o">.</span><span class="n">get_device</span><span class="p">())</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">xy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_points</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch_device</span><span class="p">)</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">xy</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
    <span class="c1"># step</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">Rosenbrock</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">xy</span><span class="p">,</span> <span class="n">num_points</span><span class="p">)</span>
    <span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>

    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="c1"># minimum at (1, 1)</span>
<span class="n">xy_np</span> <span class="o">=</span> <span class="n">xy</span><span class="o">.</span><span class="n">numpy</span><span class="p">(</span><span class="n">force</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">xy_np</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
</pre></div>
</div>
<p>Note that if Warp code is wrapped in a <a class="reference external" href="https://docs.pytorch.org/docs/stable/autograd.html#torch.autograd.Function" title="(in PyTorch v2.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code></a> that gets called in <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.compile.html#torch.compile" title="(in PyTorch v2.10)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.compile()</span></code></a>, it will automatically
exclude that function from compiler optimizations. If your script uses <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.compile.html#torch.compile" title="(in PyTorch v2.10)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.compile()</span></code></a>,
we recommend using PyTorch version 2.3.0+, which has improvements that address this scenario.</p>
</section>
<section id="example-optimization-using-pytorch-custom-operators-pytorch-2-4-0">
<span id="pytorch-custom-ops-example"></span><h3>Example: Optimization using PyTorch custom operators (PyTorch &gt;= 2.4.0)<a class="headerlink" href="#example-optimization-using-pytorch-custom-operators-pytorch-2-4-0" title="Link to this heading">#</a></h3>
<p>PyTorch 2.4+ introduced <a class="reference external" href="https://pytorch.org/tutorials/advanced/python_custom_ops.html#python-custom-ops-tutorial">custom operators</a> to replace
PyTorch autograd functions. These treat arbitrary Python functions (including Warp calls) as opaque callables, which prevents
<a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.compile.html#torch.compile" title="(in PyTorch v2.10)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.compile()</span></code></a> from tracing into them. This means that forward PyTorch graph evaluations that include Warp kernel launches can be safely accelerated with
<a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.compile.html#torch.compile" title="(in PyTorch v2.10)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.compile()</span></code></a>. We can re-write the previous example using custom operators as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">warp</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">wp</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># Define the Rosenbrock function</span>
<span class="nd">@wp</span><span class="o">.</span><span class="n">func</span>
<span class="k">def</span><span class="w"> </span><span class="nf">rosenbrock</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span> <span class="o">**</span> <span class="mf">2.0</span> <span class="o">+</span> <span class="mf">100.0</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">x</span><span class="o">**</span><span class="mf">2.0</span><span class="p">)</span> <span class="o">**</span> <span class="mf">2.0</span>


<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">eval_rosenbrock</span><span class="p">(</span>
    <span class="n">xy</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">vec2</span><span class="p">),</span>
    <span class="c1"># outputs</span>
    <span class="n">z</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
<span class="p">):</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">xy</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">rosenbrock</span><span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>


<span class="nd">@torch</span><span class="o">.</span><span class="n">library</span><span class="o">.</span><span class="n">custom_op</span><span class="p">(</span><span class="s2">&quot;wp::warp_rosenbrock&quot;</span><span class="p">,</span> <span class="n">mutates_args</span><span class="o">=</span><span class="p">())</span>
<span class="k">def</span><span class="w"> </span><span class="nf">warp_rosenbrock</span><span class="p">(</span><span class="n">xy</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">num_points</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">wp_xy</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">xy</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">vec2</span><span class="p">)</span>
    <span class="n">wp_z</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_points</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="n">eval_rosenbrock</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">num_points</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">wp_xy</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">wp_z</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">wp</span><span class="o">.</span><span class="n">to_torch</span><span class="p">(</span><span class="n">wp_z</span><span class="p">)</span>


<span class="nd">@warp_rosenbrock</span><span class="o">.</span><span class="n">register_fake</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_</span><span class="p">(</span><span class="n">xy</span><span class="p">,</span> <span class="n">num_points</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">num_points</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>


<span class="nd">@torch</span><span class="o">.</span><span class="n">library</span><span class="o">.</span><span class="n">custom_op</span><span class="p">(</span><span class="s2">&quot;wp::warp_rosenbrock_backward&quot;</span><span class="p">,</span> <span class="n">mutates_args</span><span class="o">=</span><span class="p">())</span>
<span class="k">def</span><span class="w"> </span><span class="nf">warp_rosenbrock_backward</span><span class="p">(</span>
    <span class="n">xy</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">num_points</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">adj_z</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">wp_xy</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">xy</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">vec2</span><span class="p">)</span>
    <span class="n">wp_z</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">wp_adj_z</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">adj_z</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span>
        <span class="n">kernel</span><span class="o">=</span><span class="n">eval_rosenbrock</span><span class="p">,</span>
        <span class="n">dim</span><span class="o">=</span><span class="n">num_points</span><span class="p">,</span>
        <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">wp_xy</span><span class="p">],</span>
        <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">wp_z</span><span class="p">],</span>
        <span class="n">adj_inputs</span><span class="o">=</span><span class="p">[</span><span class="n">wp_xy</span><span class="o">.</span><span class="n">grad</span><span class="p">],</span>
        <span class="n">adj_outputs</span><span class="o">=</span><span class="p">[</span><span class="n">wp_adj_z</span><span class="p">],</span>
        <span class="n">adjoint</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">wp</span><span class="o">.</span><span class="n">to_torch</span><span class="p">(</span><span class="n">wp_xy</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>


<span class="nd">@warp_rosenbrock_backward</span><span class="o">.</span><span class="n">register_fake</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_</span><span class="p">(</span><span class="n">xy</span><span class="p">,</span> <span class="n">num_points</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">adj_z</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">xy</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">adj_z</span><span class="p">):</span>
    <span class="n">ctx</span><span class="o">.</span><span class="n">xy</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">warp_rosenbrock_backward</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">xy</span><span class="p">,</span> <span class="n">ctx</span><span class="o">.</span><span class="n">num_points</span><span class="p">,</span> <span class="n">ctx</span><span class="o">.</span><span class="n">z</span><span class="p">,</span> <span class="n">adj_z</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ctx</span><span class="o">.</span><span class="n">xy</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="kc">None</span>


<span class="k">def</span><span class="w"> </span><span class="nf">setup_context</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
    <span class="n">ctx</span><span class="o">.</span><span class="n">xy</span><span class="p">,</span> <span class="n">ctx</span><span class="o">.</span><span class="n">num_points</span> <span class="o">=</span> <span class="n">inputs</span>
    <span class="n">ctx</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">output</span>


<span class="n">warp_rosenbrock</span><span class="o">.</span><span class="n">register_autograd</span><span class="p">(</span><span class="n">backward</span><span class="p">,</span> <span class="n">setup_context</span><span class="o">=</span><span class="n">setup_context</span><span class="p">)</span>

<span class="n">num_points</span> <span class="o">=</span> <span class="mi">1500</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">5e-2</span>

<span class="n">torch_device</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">device_to_torch</span><span class="p">(</span><span class="n">wp</span><span class="o">.</span><span class="n">get_device</span><span class="p">())</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">xy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_points</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch_device</span><span class="p">)</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">xy</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

<span class="nd">@torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">fullgraph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">():</span>
    <span class="k">global</span> <span class="n">xy</span><span class="p">,</span> <span class="n">num_points</span>

    <span class="n">z</span> <span class="o">=</span> <span class="n">warp_rosenbrock</span><span class="p">(</span><span class="n">xy</span><span class="p">,</span> <span class="n">num_points</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">z</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
    <span class="c1"># step</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">forward</span><span class="p">()</span>
    <span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="c1"># minimum at (1, 1)</span>
<span class="n">xy_np</span> <span class="o">=</span> <span class="n">xy</span><span class="o">.</span><span class="n">numpy</span><span class="p">(</span><span class="n">force</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">xy_np</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="performance-notes">
<h3>Performance Notes<a class="headerlink" href="#performance-notes" title="Link to this heading">#</a></h3>
<p>The <a class="reference internal" href="../api_reference/_generated/warp.from_torch.html#warp.from_torch" title="warp.from_torch"><code class="xref py py-func docutils literal notranslate"><span class="pre">wp.from_torch()</span></code></a> function creates a Warp array object that shares data with a PyTorch tensor.
Although this function does not copy the data, there is always some CPU overhead during the conversion.
If these conversions happen frequently, the overall program performance may suffer.
As a general rule, repeated conversions of the same tensor should be avoided.  Instead of:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">y_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">x_w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">x_t</span><span class="p">)</span>
    <span class="n">y_w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">y_t</span><span class="p">)</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">saxpy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x_w</span><span class="p">,</span> <span class="n">y_w</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<p>Try converting the arrays only once and reuse them:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">y_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="n">x_w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">x_t</span><span class="p">)</span>
<span class="n">y_w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">y_t</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">saxpy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x_w</span><span class="p">,</span> <span class="n">y_w</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<p>If reusing arrays is not possible (e.g., a new PyTorch tensor is constructed on every iteration), passing <code class="docutils literal notranslate"><span class="pre">return_ctype=True</span></code>
to <a class="reference internal" href="../api_reference/_generated/warp.from_torch.html#warp.from_torch" title="warp.from_torch"><code class="xref py py-func docutils literal notranslate"><span class="pre">wp.from_torch()</span></code></a> should yield better performance.
Setting this argument to <code class="docutils literal notranslate"><span class="pre">True</span></code> avoids constructing a <a class="reference internal" href="../api_reference/_generated/warp.array.html#warp.array" title="warp.array"><code class="xref py py-class docutils literal notranslate"><span class="pre">wp.array</span></code></a> object and instead returns a low-level array descriptor.
This descriptor is a simple C structure that can be passed to Warp kernels instead of a <a class="reference internal" href="../api_reference/_generated/warp.array.html#warp.array" title="warp.array"><code class="xref py py-class docutils literal notranslate"><span class="pre">wp.array</span></code></a>,
but cannot be used in other places that require a <a class="reference internal" href="../api_reference/_generated/warp.array.html#warp.array" title="warp.array"><code class="xref py py-class docutils literal notranslate"><span class="pre">wp.array</span></code></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">):</span>
    <span class="c1"># get Torch tensors for this iteration</span>
    <span class="n">x_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">y_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># get Warp array descriptors</span>
    <span class="n">x_ctype</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">return_ctype</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">y_ctype</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">y_t</span><span class="p">,</span> <span class="n">return_ctype</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">saxpy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x_ctype</span><span class="p">,</span> <span class="n">y_ctype</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<p>An alternative approach is to pass the PyTorch tensors to Warp kernels directly.
This avoids constructing temporary Warp arrays by leveraging standard array interfaces (like <code class="docutils literal notranslate"><span class="pre">__cuda_array_interface__</span></code>) supported by both PyTorch and Warp.
The main advantage of this approach is convenience, since there is no need to call any conversion functions.
The main limitation is that it does not handle gradients, because gradient information is not included in the standard array interfaces.
This technique is therefore most suitable for algorithms that do not involve differentiation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">saxpy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>warp.examples.benchmarks.benchmark_interop_torch
</pre></div>
</div>
<p>Sample output:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">5095</span> <span class="n">ms</span>  <span class="n">from_torch</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="mi">2113</span> <span class="n">ms</span>  <span class="n">from_torch</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">return_ctype</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="mi">2950</span> <span class="n">ms</span>  <span class="n">direct</span> <span class="kn">from</span><span class="w"> </span><span class="nn">torch</span>
</pre></div>
</div>
<p>The default <a class="reference internal" href="../api_reference/_generated/warp.from_torch.html#warp.from_torch" title="warp.from_torch"><code class="xref py py-func docutils literal notranslate"><span class="pre">wp.from_torch()</span></code></a> conversion is the slowest.
Passing <code class="docutils literal notranslate"><span class="pre">return_ctype=True</span></code> is the fastest, because it skips creating temporary Warp array objects.
Passing PyTorch tensors to Warp kernels directly falls somewhere in between.
It skips creating temporary Warp arrays, but accessing the <code class="docutils literal notranslate"><span class="pre">__cuda_array_interface__</span></code> attributes of PyTorch tensors
adds overhead because they are initialized on-demand.</p>
</section>
<section id="case-study-pytorch-deferred-gradient-allocation-and-warp-interoperability">
<h3>Case Study: PyTorch Deferred Gradient Allocation and Warp Interoperability<a class="headerlink" href="#case-study-pytorch-deferred-gradient-allocation-and-warp-interoperability" title="Link to this heading">#</a></h3>
<p>When writing custom PyTorch autograd functions that use Warp kernels, whether using analytic gradient kernels or
the Warp tape, PyTorch’s deferred gradient allocation can cause unexpected synchronization delays.
This case study demonstrates the problem and provides practical solutions.</p>
<section id="the-problem-deferred-gradient-allocation">
<h4>The Problem: Deferred Gradient Allocation<a class="headerlink" href="#the-problem-deferred-gradient-allocation" title="Link to this heading">#</a></h4>
<p>PyTorch employs a deferred allocation strategy for gradient tensors. When you create a tensor with <code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code>,
PyTorch does not immediately allocate memory for the gradient. Instead, gradients are allocated on-demand during
the backward pass.</p>
<p>However, when <a class="reference internal" href="../api_reference/_generated/warp.from_torch.html#warp.from_torch" title="warp.from_torch"><code class="xref py py-func docutils literal notranslate"><span class="pre">wp.from_torch()</span></code></a> encounters a tensor with <code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code> but no allocated gradient,
it forces the gradient to be allocated immediately. This creates overhead that can significantly impact performance.</p>
<p>When PyTorch later discovers that an external
framework has allocated its gradient tensors, it must perform an expensive device-wide synchronization to ensure
correctness. This synchronization overhead can significantly impact performance.</p>
<p>Here’s an example that demonstrates the problem:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">warp</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">wp</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="n">device</span> <span class="o">=</span> <span class="s1">&#39;cuda&#39;</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">300_000_000</span>

<span class="n">wp</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>

<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span><span class="p">(</span><span class="n">enable_backward</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">forward_kernel</span><span class="p">(</span>
    <span class="n">a</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
    <span class="n">b</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
    <span class="n">output</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
<span class="p">):</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">output</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="o">*</span><span class="n">y</span>


<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span><span class="p">(</span><span class="n">enable_backward</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">backward_kernel</span><span class="p">(</span>
    <span class="n">grad_output</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
    <span class="n">a</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
    <span class="n">b</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
    <span class="n">grad_a</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
    <span class="n">grad_b</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
<span class="p">):</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">adj_z</span> <span class="o">=</span> <span class="n">grad_output</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

    <span class="n">grad_a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="n">adj_z</span>
    <span class="n">grad_b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="n">y</span> <span class="o">*</span> <span class="n">adj_z</span>


<span class="k">class</span><span class="w"> </span><span class="nc">WarpFunction</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

        <span class="n">device</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">device_from_torch</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span>
            <span class="n">kernel</span><span class="o">=</span><span class="n">forward_kernel</span><span class="p">,</span>
            <span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">),</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">inputs</span><span class="o">=</span><span class="p">[</span>
                <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">a</span><span class="p">),</span>      <span class="c1"># ⚠️ Triggers gradient allocation</span>
                <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">b</span><span class="p">),</span>      <span class="c1"># ⚠️ Triggers gradient allocation</span>
                <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">output</span><span class="p">),</span>
            <span class="p">],</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
        <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>

        <span class="n">device</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">device_from_torch</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="n">grad_a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="n">grad_b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

        <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span>
            <span class="n">kernel</span><span class="o">=</span><span class="n">backward_kernel</span><span class="p">,</span>
            <span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">),</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">inputs</span><span class="o">=</span><span class="p">[</span>
                <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">grad_output</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()),</span>
                <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">a</span><span class="p">),</span>
                <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">b</span><span class="p">),</span>
                <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">grad_a</span><span class="p">),</span>
                <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">grad_b</span><span class="p">),</span>
            <span class="p">],</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">grad_a</span><span class="p">,</span> <span class="n">grad_b</span>


<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">TRIALS</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">wp</span><span class="o">.</span><span class="n">ScopedTimer</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;TRIAL </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">use_nvtx</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">synchronize</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">wp</span><span class="o">.</span><span class="n">ScopedTimer</span><span class="p">(</span><span class="s2">&quot;Create Tensors&quot;</span><span class="p">,</span> <span class="n">use_nvtx</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">synchronize</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
            <span class="n">a_torch</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">b_torch</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">wp</span><span class="o">.</span><span class="n">ScopedTimer</span><span class="p">(</span><span class="s2">&quot;Forward&quot;</span><span class="p">,</span> <span class="n">use_nvtx</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">synchronize</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
            <span class="n">output_warp</span> <span class="o">=</span> <span class="n">WarpFunction</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a_torch</span><span class="p">,</span> <span class="n">b_torch</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">wp</span><span class="o">.</span><span class="n">ScopedTimer</span><span class="p">(</span><span class="s2">&quot;Loss&quot;</span><span class="p">,</span> <span class="n">use_nvtx</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">synchronize</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
            <span class="n">loss_warp</span> <span class="o">=</span> <span class="n">output_warp</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">wp</span><span class="o">.</span><span class="n">ScopedTimer</span><span class="p">(</span><span class="s2">&quot;Backward&quot;</span><span class="p">,</span> <span class="n">use_nvtx</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">synchronize</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
            <span class="n">loss_warp</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<p>When profiling this code with NVIDIA Nsight Systems, significant gaps appear in the GPU timeline, indicating
device-wide synchronization events:</p>
<figure class="align-default" id="id3">
<a class="reference internal image-reference" href="../_images/torch_sync_overhead.png"><img alt="Nsight Systems capture showing synchronization gaps between kernel launches" src="../_images/torch_sync_overhead.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-text">NVIDIA Nsight Systems timeline showing synchronization gaps between Warp kernel launches and PyTorch operations.</span><a class="headerlink" href="#id3" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The gaps represent device-wide synchronizations triggered when PyTorch discovers externally allocated gradients.</p>
<p>This problem is particularly severe in this benchmark because new tensors (<code class="docutils literal notranslate"><span class="pre">a_torch</span></code> and <code class="docutils literal notranslate"><span class="pre">b_torch</span></code>) are created
on each iteration via <code class="docutils literal notranslate"><span class="pre">.clone().detach().requires_grad_(True)</span></code>. Since these fresh tensors have <code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code>
but no pre-allocated gradients, the synchronization penalty is incurred <strong>on every single iteration</strong>.</p>
</section>
<section id="solutions">
<h4>Solutions<a class="headerlink" href="#solutions" title="Link to this heading">#</a></h4>
<p>There are three approaches to avoid this synchronization overhead, depending on your use case:</p>
<p><strong>Solution A: Disable Gradient Tracking in wp.from_torch()</strong></p>
<p>The simplest solution is to pass <code class="docutils literal notranslate"><span class="pre">requires_grad=False</span></code> to <a class="reference internal" href="../api_reference/_generated/warp.from_torch.html#warp.from_torch" title="warp.from_torch"><code class="xref py py-func docutils literal notranslate"><span class="pre">wp.from_torch()</span></code></a>, preventing Warp from
auto-allocating gradients:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">device_from_torch</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span>
        <span class="n">kernel</span><span class="o">=</span><span class="n">forward_kernel</span><span class="p">,</span>
        <span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">),</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">inputs</span><span class="o">=</span><span class="p">[</span>
            <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>      <span class="c1"># ✓ No gradient allocation</span>
            <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>      <span class="c1"># ✓ No gradient allocation</span>
            <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
        <span class="p">],</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span>

<span class="nd">@staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">device_from_torch</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">grad_a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">grad_b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span>
        <span class="n">kernel</span><span class="o">=</span><span class="n">backward_kernel</span><span class="p">,</span>
        <span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">),</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">inputs</span><span class="o">=</span><span class="p">[</span>
            <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">grad_output</span><span class="o">.</span><span class="n">contiguous</span><span class="p">(),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">grad_a</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">grad_b</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
        <span class="p">],</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">grad_a</span><span class="p">,</span> <span class="n">grad_b</span>
</pre></div>
</div>
<p>This approach works well when you’re managing forward and gradient tensors separately and don’t need
Warp to track gradients automatically.</p>
<p><strong>Solution B: Detach Tensors from the PyTorch Graph</strong></p>
<p>When managing gradients outside PyTorch’s autograd graph, detaching tensors is a clean approach:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="c1"># Store detached tensors - we&#39;ll manage gradients manually</span>
    <span class="n">ctx</span><span class="o">.</span><span class="n">a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
    <span class="n">ctx</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

    <span class="n">device</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">device_from_torch</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span>
        <span class="n">kernel</span><span class="o">=</span><span class="n">forward_kernel</span><span class="p">,</span>
        <span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">),</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">inputs</span><span class="o">=</span><span class="p">[</span>
            <span class="n">ctx</span><span class="o">.</span><span class="n">a</span><span class="p">,</span>  <span class="c1"># ✓ Detached, no requires_grad</span>
            <span class="n">ctx</span><span class="o">.</span><span class="n">b</span><span class="p">,</span>  <span class="c1"># ✓ Detached, no requires_grad</span>
            <span class="n">output</span><span class="p">,</span>
        <span class="p">],</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span>

<span class="nd">@staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">device_from_torch</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">a</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">grad_a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">a</span><span class="p">)</span>
    <span class="n">grad_b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>

    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span>
        <span class="n">kernel</span><span class="o">=</span><span class="n">backward_kernel</span><span class="p">,</span>
        <span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">),</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">inputs</span><span class="o">=</span><span class="p">[</span>
            <span class="n">grad_output</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span>
            <span class="n">ctx</span><span class="o">.</span><span class="n">a</span><span class="p">,</span>  <span class="c1"># ✓ Already detached</span>
            <span class="n">ctx</span><span class="o">.</span><span class="n">b</span><span class="p">,</span>  <span class="c1"># ✓ Already detached</span>
            <span class="n">grad_a</span><span class="p">,</span>
            <span class="n">grad_b</span><span class="p">,</span>
        <span class="p">],</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">grad_a</span><span class="p">,</span> <span class="n">grad_b</span>
</pre></div>
</div>
<p>Detaching removes tensors from PyTorch’s computational graph (and clears <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code>), making it clear
that gradient management happens outside PyTorch’s autograd system.</p>
<p><strong>Solution C: Pre-allocate Gradients with PyTorch</strong></p>
<p>Alternatively, you can pre-allocate gradients using PyTorch’s allocator before passing tensors to Warp.
This approach works for both analytic gradient kernels and when using the Warp tape.</p>
<p><em>Variant 1: With Analytic Gradient Kernels</em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="c1"># Pre-allocate gradients using PyTorch&#39;s allocator</span>
    <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">a</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">b</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

    <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

    <span class="n">device</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">device_from_torch</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span>
        <span class="n">kernel</span><span class="o">=</span><span class="n">forward_kernel</span><span class="p">,</span>
        <span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">),</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">inputs</span><span class="o">=</span><span class="p">[</span>
            <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">a</span><span class="p">),</span>
            <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">b</span><span class="p">),</span>
            <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">output</span><span class="p">),</span>
        <span class="p">],</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span>

<span class="nd">@staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">device_from_torch</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Now we can use a.grad and b.grad directly</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span>
        <span class="n">kernel</span><span class="o">=</span><span class="n">backward_kernel</span><span class="p">,</span>
        <span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">),</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">inputs</span><span class="o">=</span><span class="p">[</span>
            <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">grad_output</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()),</span>
            <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">a</span><span class="p">),</span>
            <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">b</span><span class="p">),</span>
            <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">grad</span><span class="p">),</span>  <span class="c1"># ✓ Allocated by PyTorch</span>
            <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">grad</span><span class="p">),</span>  <span class="c1"># ✓ Allocated by PyTorch</span>
        <span class="p">],</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">a</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
<p><em>Variant 2: With Warp’s Tape (Automatic Differentiation)</em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">device_from_torch</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Pre-allocate gradients using PyTorch&#39;s allocator</span>
    <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">a</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">b</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

    <span class="n">wp_output</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">wp</span><span class="o">.</span><span class="n">Tape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span>
            <span class="n">kernel</span><span class="o">=</span><span class="n">forward_kernel</span><span class="p">,</span>
            <span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">),</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">inputs</span><span class="o">=</span><span class="p">[</span>
                <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">a</span><span class="p">),</span>
                <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">b</span><span class="p">),</span>
                <span class="n">wp_output</span><span class="p">,</span>
            <span class="p">],</span>
        <span class="p">)</span>

    <span class="n">ctx</span><span class="o">.</span><span class="n">tape</span> <span class="o">=</span> <span class="n">tape</span>
    <span class="n">ctx</span><span class="o">.</span><span class="n">wp_output</span> <span class="o">=</span> <span class="n">wp_output</span>
    <span class="n">ctx</span><span class="o">.</span><span class="n">a</span> <span class="o">=</span> <span class="n">a</span>
    <span class="n">ctx</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">b</span>

    <span class="k">return</span> <span class="n">output</span>

<span class="nd">@staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
    <span class="n">ctx</span><span class="o">.</span><span class="n">tape</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">grads</span><span class="o">=</span><span class="p">{</span><span class="n">ctx</span><span class="o">.</span><span class="n">wp_output</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">grad_output</span><span class="o">.</span><span class="n">contiguous</span><span class="p">())})</span>

    <span class="c1"># Grab gradients before clearing references</span>
    <span class="n">grad_a</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">a</span><span class="o">.</span><span class="n">grad</span>
    <span class="n">grad_b</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">b</span><span class="o">.</span><span class="n">grad</span>

    <span class="c1"># Clear context to break reference cycles and free memory</span>
    <span class="n">ctx</span><span class="o">.</span><span class="n">tape</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">ctx</span><span class="o">.</span><span class="n">wp_output</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">return</span> <span class="n">grad_a</span><span class="p">,</span> <span class="n">grad_b</span>
</pre></div>
</div>
<p>This approach ensures gradients are allocated using PyTorch’s caching allocator, which properly tracks
memory and stream dependencies. Pre-allocation can also be done earlier in the pipeline:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># At tensor creation time</span>
<span class="n">a_torch</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">b_torch</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Pre-allocate gradients immediately</span>
<span class="n">a_torch</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">a_torch</span><span class="p">)</span>
<span class="n">b_torch</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">b_torch</span><span class="p">)</span>

<span class="c1"># Now safe to use in WarpFunction</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">WarpFunction</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a_torch</span><span class="p">,</span> <span class="n">b_torch</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="performance-comparison">
<h4>Performance Comparison<a class="headerlink" href="#performance-comparison" title="Link to this heading">#</a></h4>
<p>Benchmarking these approaches on a workload with N=300,000,000 elements shows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Baseline</span> <span class="p">(</span><span class="k">with</span> <span class="n">synchronization</span> <span class="n">overhead</span><span class="p">):</span>  <span class="mf">98.02</span> <span class="n">ms</span>
<span class="n">Solution</span> <span class="n">A</span> <span class="p">(</span><span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>          <span class="mf">22.59</span> <span class="n">ms</span>  <span class="p">(</span><span class="mf">4.3</span><span class="n">x</span> <span class="n">faster</span><span class="p">)</span>
<span class="n">Solution</span> <span class="n">B</span> <span class="p">(</span><span class="n">detach</span><span class="p">):</span>                       <span class="mf">22.11</span> <span class="n">ms</span>  <span class="p">(</span><span class="mf">4.4</span><span class="n">x</span> <span class="n">faster</span><span class="p">)</span>
<span class="n">Solution</span> <span class="n">C</span> <span class="p">(</span><span class="n">pre</span><span class="o">-</span><span class="n">allocate</span><span class="p">):</span>                 <span class="mf">28.62</span> <span class="n">ms</span>  <span class="p">(</span><span class="mf">3.4</span><span class="n">x</span> <span class="n">faster</span><span class="p">)</span>
</pre></div>
</div>
<p>All three solutions eliminate the synchronization overhead:</p>
<ul class="simple">
<li><p><strong>Solutions A and B</strong> are fastest because they allocate gradients in the backward pass as simple standalone tensors</p></li>
<li><p><strong>Solution C</strong> is slightly slower because it allocates gradients in the forward pass and attaches them as <code class="docutils literal notranslate"><span class="pre">.grad</span></code>
attributes, which involves PyTorch’s autograd bookkeeping</p></li>
</ul>
<p>Choose based on your workflow:</p>
<ul class="simple">
<li><p><strong>Solution A</strong>: Most explicit about disabling gradient tracking</p></li>
<li><p><strong>Solution B</strong>: Cleanest for manual gradient management</p></li>
<li><p><strong>Solution C</strong>: Required when using Warp’s tape or when you need <code class="docutils literal notranslate"><span class="pre">.grad</span></code> access</p></li>
</ul>
<p>These solutions are primarily needed when working with <strong>newly created tensors</strong> in scenarios like:</p>
<ul class="simple">
<li><p>Training loops that create fresh tensors each iteration</p></li>
<li><p>Repeated inference with dynamically allocated tensors</p></li>
<li><p>Any workflow using <code class="docutils literal notranslate"><span class="pre">.clone().detach().requires_grad_(True)</span></code> patterns</p></li>
</ul>
<p>If you recycle the same tensors across iterations (whose gradients have already been allocated),
there will be no need for gradient allocation (deferred or otherwise) and therefore no synchronization overhead.</p>
</section>
</section>
</section>
<section id="cupy-numba">
<h2>CuPy/Numba<a class="headerlink" href="#cupy-numba" title="Link to this heading">#</a></h2>
<p>Warp GPU arrays support the <code class="docutils literal notranslate"><span class="pre">__cuda_array_interface__</span></code> protocol for sharing data with other Python GPU frameworks.
This allows frameworks like CuPy to use Warp GPU arrays directly.</p>
<p>Likewise, Warp arrays can be created from any object that exposes the <code class="docutils literal notranslate"><span class="pre">__cuda_array_interface__</span></code>.
Such objects can also be passed to Warp kernels directly without creating a Warp array object.</p>
</section>
<section id="jax">
<span id="jax-interop"></span><h2>JAX<a class="headerlink" href="#jax" title="Link to this heading">#</a></h2>
<p>Interoperability with JAX arrays is supported through the following methods.
Internally these use the DLPack protocol to exchange data in a zero-copy way with JAX:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">warp_array</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_jax</span><span class="p">(</span><span class="n">jax_array</span><span class="p">)</span>
<span class="n">jax_array</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">to_jax</span><span class="p">(</span><span class="n">warp_array</span><span class="p">)</span>
</pre></div>
</div>
<p>It may be preferable to use the <a class="reference internal" href="#dlpack"><span class="std std-ref">DLPack</span></a> protocol directly for better performance and control over stream synchronization .</p>
<section id="using-warp-kernels-as-jax-primitives">
<span id="jax-ffi"></span><h3>Using Warp kernels as JAX primitives<a class="headerlink" href="#using-warp-kernels-as-jax-primitives" title="Link to this heading">#</a></h3>
<p>Warp kernels can be used as JAX primitives, which allows calling them inside of jitted JAX functions:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">warp</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">wp</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jnp</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">warp.jax_experimental</span><span class="w"> </span><span class="kn">import</span> <span class="n">jax_kernel</span>

<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">triple_kernel</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span> <span class="n">output</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">output</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="mf">3.0</span> <span class="o">*</span> <span class="nb">input</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span>

<span class="c1"># create a Jax primitive from a Warp kernel</span>
<span class="n">jax_triple</span> <span class="o">=</span> <span class="n">jax_kernel</span><span class="p">(</span><span class="n">triple_kernel</span><span class="p">)</span>

<span class="c1"># use the Warp kernel in a Jax jitted function</span>
<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">():</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jax_triple</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="p">())</span>
</pre></div>
</div>
<section id="input-and-output-semantics">
<h4>Input and Output Semantics<a class="headerlink" href="#input-and-output-semantics" title="Link to this heading">#</a></h4>
<p>Input arguments must come before output arguments in the kernel definition.
At least one output array is required, but it’s ok to have kernels with no inputs.
The number of outputs can be specified using the <code class="docutils literal notranslate"><span class="pre">num_outputs</span></code> argument, which defaults to one.</p>
<p>Here’s a kernel with two inputs and one output:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">jax</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jnp</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">warp</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">wp</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">warp.jax_experimental</span><span class="w"> </span><span class="kn">import</span> <span class="n">jax_kernel</span>

<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">add_kernel</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">),</span>
               <span class="n">b</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">),</span>
               <span class="n">output</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">output</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span>

<span class="n">jax_add</span> <span class="o">=</span> <span class="n">jax_kernel</span><span class="p">(</span><span class="n">add_kernel</span><span class="p">)</span>

<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">():</span>
    <span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jax_add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="p">())</span>
</pre></div>
</div>
<p>One input and two outputs:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">math</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">jax</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jnp</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">warp</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">wp</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">warp.jax_experimental</span><span class="w"> </span><span class="kn">import</span> <span class="n">jax_kernel</span>

<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">sincos_kernel</span><span class="p">(</span><span class="n">angle</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
                  <span class="c1"># outputs</span>
                  <span class="n">sin_out</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
                  <span class="n">cos_out</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">sin_out</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">angle</span><span class="p">[</span><span class="n">tid</span><span class="p">])</span>
    <span class="n">cos_out</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">angle</span><span class="p">[</span><span class="n">tid</span><span class="p">])</span>

<span class="n">jax_sincos</span> <span class="o">=</span> <span class="n">jax_kernel</span><span class="p">(</span><span class="n">sincos_kernel</span><span class="p">,</span> <span class="n">num_outputs</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># specify multiple outputs</span>

<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">():</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jax_sincos</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="n">s</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">f</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
<p>Here is a kernel with no inputs that initializes an array of 3x3 matrices with the diagonal values (1, 2, 3).
With no inputs, specifying the launch dimensions is required to determine the shape of the output array:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">diagonal_kernel</span><span class="p">(</span><span class="n">output</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">mat33</span><span class="p">)):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">output</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">mat33</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">)</span>

<span class="n">jax_diagonal</span> <span class="o">=</span> <span class="n">jax_kernel</span><span class="p">(</span><span class="n">diagonal_kernel</span><span class="p">)</span>

<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">():</span>
    <span class="c1"># launch dimensions determine the output shape</span>
    <span class="k">return</span> <span class="n">jax_diagonal</span><span class="p">(</span><span class="n">launch_dims</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="p">())</span>
</pre></div>
</div>
<section id="scalar-inputs">
<h5>Scalar Inputs<a class="headerlink" href="#scalar-inputs" title="Link to this heading">#</a></h5>
<p>Scalar input arguments are supported, although there are some limitations. Currently, scalars passed to Warp kernels must be constant or static values in JAX:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">scale_kernel</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
                 <span class="n">s</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>  <span class="c1"># scalar input</span>
                 <span class="n">output</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">output</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">*</span> <span class="n">s</span>


<span class="n">jax_scale</span> <span class="o">=</span> <span class="n">jax_kernel</span><span class="p">(</span><span class="n">scale_kernel</span><span class="p">)</span>

<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">():</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jax_scale</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">)</span>  <span class="c1"># ok: constant scalar argument</span>

<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="p">())</span>
</pre></div>
</div>
<p>Trying to use a traced scalar value will result in an exception:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jax_scale</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>  <span class="c1"># ERROR: traced scalar argument</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">))</span>
</pre></div>
</div>
<p>JAX static arguments to the rescue:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># make scalar arguments static</span>
<span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnames</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;s&quot;</span><span class="p">])</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jax_scale</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>  <span class="c1"># ok: static scalar argument</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">))</span>
</pre></div>
</div>
</section>
</section>
<section id="kernel-launch-and-output-dimensions">
<h4>Kernel Launch and Output Dimensions<a class="headerlink" href="#kernel-launch-and-output-dimensions" title="Link to this heading">#</a></h4>
<p>By default, the launch dimensions are inferred from the shape of the first input array.
When that’s not appropriate, the <code class="docutils literal notranslate"><span class="pre">launch_dims</span></code> argument can be used to override this behavior.
The launch dimensions also determine the shape of the output arrays.</p>
<p>Here is a simple matrix multiplication kernel that multiplies an NxK matrix by a KxM matrix.
The launch dimensions and output shape must be (N, M), which is different than the shape of the input arrays:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">matmul_kernel</span><span class="p">(</span>
    <span class="n">a</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array2d</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>  <span class="c1"># NxK input</span>
    <span class="n">b</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array2d</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>  <span class="c1"># KxM input</span>
    <span class="n">c</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array2d</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>  <span class="c1"># NxM output</span>
<span class="p">):</span>
    <span class="c1"># launch dimensions should be (N, M)</span>
    <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">M</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="ow">and</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">M</span><span class="p">:</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">b</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
        <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">s</span>

<span class="c1"># no need to specify launch dims here</span>
<span class="n">jax_matmul</span> <span class="o">=</span> <span class="n">jax_kernel</span><span class="p">(</span><span class="n">matmul_kernel</span><span class="p">)</span>

<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">():</span>
    <span class="n">N1</span><span class="p">,</span> <span class="n">M1</span><span class="p">,</span> <span class="n">K1</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span>
    <span class="n">a1</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">N1</span><span class="p">,</span> <span class="n">K1</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">K1</span><span class="p">,</span> <span class="n">M1</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="c1"># use custom launch dims</span>
    <span class="n">result1</span> <span class="o">=</span> <span class="n">jax_matmul</span><span class="p">(</span><span class="n">a1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">launch_dims</span><span class="o">=</span><span class="p">(</span><span class="n">N1</span><span class="p">,</span> <span class="n">M1</span><span class="p">))</span>

    <span class="n">N2</span><span class="p">,</span> <span class="n">M2</span><span class="p">,</span> <span class="n">K2</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span>
    <span class="n">a2</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">N2</span><span class="p">,</span> <span class="n">K2</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">K2</span><span class="p">,</span> <span class="n">M2</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="c1"># use custom launch dims</span>
    <span class="n">result2</span> <span class="o">=</span> <span class="n">jax_matmul</span><span class="p">(</span><span class="n">a2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">launch_dims</span><span class="o">=</span><span class="p">(</span><span class="n">N2</span><span class="p">,</span> <span class="n">M2</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">result1</span><span class="p">,</span> <span class="n">result2</span>

<span class="n">r1</span><span class="p">,</span> <span class="n">r2</span> <span class="o">=</span> <span class="n">f</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">r1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">r2</span><span class="p">)</span>
</pre></div>
</div>
<p>By default, output array shapes are determined from the launch dimensions, but it’s possible to specify custom output
dimensions using the <code class="docutils literal notranslate"><span class="pre">output_dims</span></code> argument. Consider a kernel like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">funky_kernel</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
                 <span class="c1"># outputs</span>
                 <span class="n">b</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
                 <span class="n">c</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)):</span>
    <span class="o">...</span>

<span class="n">jax_funky</span> <span class="o">=</span> <span class="n">jax_kernel</span><span class="p">(</span><span class="n">funky_kernel</span><span class="p">,</span> <span class="n">num_outputs</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>Specify a custom output shape used for all outputs:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">b</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">jax_funky</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">output_dims</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
</pre></div>
</div>
<p>Specify different output dimensions for each output using a dictionary:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">b</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">jax_funky</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">output_dims</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="n">n</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">:</span> <span class="n">m</span><span class="p">})</span>
</pre></div>
</div>
<p>Specify custom launch and output dimensions together:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">b</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">jax_funky</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">launch_dims</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">output_dims</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="n">n</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">:</span> <span class="n">m</span><span class="p">})</span>
</pre></div>
</div>
<p>One-dimensional shapes can be specified using an integer. Multi-dimensional shapes can be specified using tuples or lists of integers.</p>
<section id="vector-and-matrix-arrays">
<h5>Vector and Matrix Arrays<a class="headerlink" href="#vector-and-matrix-arrays" title="Link to this heading">#</a></h5>
<p>Arrays of Warp vector and matrix types are supported.
Since JAX does not have corresponding data types, the components are packed into extra inner dimensions of JAX arrays.
For example, a Warp array of <a class="reference internal" href="../api_reference/_generated/warp.vec3.html#warp.vec3" title="warp.vec3"><code class="xref py py-class docutils literal notranslate"><span class="pre">wp.vec3</span></code></a> will have a JAX array shape of (…, 3)
and a Warp array of <a class="reference internal" href="../api_reference/_generated/warp.mat22.html#warp.mat22" title="warp.mat22"><code class="xref py py-class docutils literal notranslate"><span class="pre">wp.mat22</span></code></a> will have a JAX array shape of (…, 2, 2):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">vecmat_kernel</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
                  <span class="n">b</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">vec3</span><span class="p">),</span>
                  <span class="n">c</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">mat22</span><span class="p">),</span>
                  <span class="c1"># outputs</span>
                  <span class="n">d</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
                  <span class="n">e</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">vec3</span><span class="p">),</span>
                  <span class="n">f</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">mat22</span><span class="p">)):</span>
    <span class="o">...</span>

<span class="n">jax_vecmat</span> <span class="o">=</span> <span class="n">jax_kernel</span><span class="p">(</span><span class="n">vecmat_kernel</span><span class="p">,</span> <span class="n">num_outputs</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">():</span>
    <span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>          <span class="c1"># scalar array</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>     <span class="c1"># vec3 array</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># mat22 array</span>

    <span class="n">d</span><span class="p">,</span> <span class="n">e</span><span class="p">,</span> <span class="n">f</span> <span class="o">=</span> <span class="n">jax_vecmat</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
<p>It’s important to recognize that the Warp and JAX array shapes are different for vector and matrix types.
In the above snippet, Warp sees <code class="docutils literal notranslate"><span class="pre">a</span></code>, <code class="docutils literal notranslate"><span class="pre">b</span></code>, and <code class="docutils literal notranslate"><span class="pre">c</span></code> as one-dimensional arrays of <a class="reference internal" href="../api_reference/_generated/warp.float32.html#warp.float32" title="warp.float32"><code class="xref py py-class docutils literal notranslate"><span class="pre">wp.float32</span></code></a>,
<a class="reference internal" href="../api_reference/_generated/warp.vec3.html#warp.vec3" title="warp.vec3"><code class="xref py py-class docutils literal notranslate"><span class="pre">wp.vec3</span></code></a>, and <a class="reference internal" href="../api_reference/_generated/warp.mat22.html#warp.mat22" title="warp.mat22"><code class="xref py py-class docutils literal notranslate"><span class="pre">wp.mat22</span></code></a>, respectively.
In JAX, <code class="docutils literal notranslate"><span class="pre">a</span></code> is a one-dimensional array with length <code class="docutils literal notranslate"><span class="pre">n</span></code>, <code class="docutils literal notranslate"><span class="pre">b</span></code> is a two-dimensional array
with shape <code class="docutils literal notranslate"><span class="pre">(n,</span> <span class="pre">3)</span></code>, and <code class="docutils literal notranslate"><span class="pre">c</span></code> is a three-dimensional array with shape <code class="docutils literal notranslate"><span class="pre">(n,</span> <span class="pre">2,</span> <span class="pre">2)</span></code>.</p>
<p>When specifying custom output dimensions, it’s possible to use either convention. The following calls are equivalent:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">d</span><span class="p">,</span> <span class="n">e</span><span class="p">,</span> <span class="n">f</span> <span class="o">=</span> <span class="n">jax_vecmat</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">output_dims</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">d</span><span class="p">,</span> <span class="n">e</span><span class="p">,</span> <span class="n">f</span> <span class="o">=</span> <span class="n">jax_vecmat</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">output_dims</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;d&quot;</span><span class="p">:</span> <span class="n">n</span><span class="p">,</span> <span class="s2">&quot;e&quot;</span><span class="p">:</span> <span class="n">n</span><span class="p">,</span> <span class="s2">&quot;f&quot;</span><span class="p">:</span> <span class="n">n</span><span class="p">})</span>
<span class="n">d</span><span class="p">,</span> <span class="n">e</span><span class="p">,</span> <span class="n">f</span> <span class="o">=</span> <span class="n">jax_vecmat</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">output_dims</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;d&quot;</span><span class="p">:</span> <span class="n">n</span><span class="p">,</span> <span class="s2">&quot;e&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="s2">&quot;f&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)})</span>
</pre></div>
</div>
<p>This is a convenience feature meant to simplify writing code.
For example, when Warp expects the arrays to be of the same shape, we only need to specify the shape once without
worrying about the extra vector and matrix dimensions required by JAX:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">d</span><span class="p">,</span> <span class="n">e</span><span class="p">,</span> <span class="n">f</span> <span class="o">=</span> <span class="n">jax_vecmat</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">output_dims</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
</pre></div>
</div>
<p>On the other hand, JAX dimensions are also accepted to allow passing shapes directly from JAX:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">d</span><span class="p">,</span> <span class="n">e</span><span class="p">,</span> <span class="n">f</span> <span class="o">=</span> <span class="n">jax_vecmat</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">output_dims</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;d&quot;</span><span class="p">:</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;e&quot;</span><span class="p">:</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;f&quot;</span><span class="p">:</span> <span class="n">c</span><span class="o">.</span><span class="n">shape</span><span class="p">})</span>
</pre></div>
</div>
<p>See <a class="reference external" href="https://github.com/NVIDIA/warp/tree/main/warp/examples/interop/example_jax_kernel.py">example_jax_kernel.py</a> for examples.</p>
</section>
</section>
<section id="jax-vmap-support">
<h4>JAX VMAP Support<a class="headerlink" href="#jax-vmap-support" title="Link to this heading">#</a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">vmap_method</span></code> argument can be used to specify how the callback transforms under <a class="reference external" href="https://docs.jax.dev/en/latest/_autosummary/jax.vmap.html#jax.vmap" title="(in JAX)"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax.vmap()</span></code></a>.
The default is <code class="docutils literal notranslate"><span class="pre">&quot;broadcast_all&quot;</span></code>.
This argument can be passed to <a class="reference internal" href="../api_reference/_generated/warp.jax_experimental.ffi.jax_kernel.html#warp.jax_experimental.ffi.jax_kernel" title="warp.jax_experimental.ffi.jax_kernel"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax_kernel()</span></code></a>,
and it can also be passed to each call:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># set default vmap behavior</span>
<span class="n">jax_callback</span> <span class="o">=</span> <span class="n">jax_kernel</span><span class="p">(</span><span class="n">my_kernel</span><span class="p">,</span> <span class="n">vmap_method</span><span class="o">=</span><span class="s2">&quot;sequential&quot;</span><span class="p">)</span>

<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">():</span>
    <span class="o">...</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">jax_callback</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>  <span class="c1"># uses &quot;sequential&quot;</span>
    <span class="o">...</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">jax_callback</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">vmap_method</span><span class="o">=</span><span class="s2">&quot;expand_dims&quot;</span><span class="p">)</span>  <span class="c1"># uses &quot;expand_dims&quot;</span>
    <span class="o">...</span>
</pre></div>
</div>
</section>
<section id="jax-automatic-differentiation">
<h4>JAX Automatic Differentiation<a class="headerlink" href="#jax-automatic-differentiation" title="Link to this heading">#</a></h4>
<p>Warp kernels can be given JAX gradients using a convenience wrapper that wires a custom VJP around a kernel and its adjoint.
To enable autodiff, pass the <code class="docutils literal notranslate"><span class="pre">enable_backward=True</span></code> argument to <a class="reference internal" href="../api_reference/_generated/warp.jax_experimental.ffi.jax_kernel.html#warp.jax_experimental.ffi.jax_kernel" title="warp.jax_experimental.ffi.jax_kernel"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax_kernel()</span></code></a>.</p>
<p>Basic example (one output):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">functools</span><span class="w"> </span><span class="kn">import</span> <span class="n">partial</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jnp</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warp</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">wp</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">warp.jax_experimental</span><span class="w"> </span><span class="kn">import</span> <span class="n">jax_kernel</span>

<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">scale_sum_square</span><span class="p">(</span>
    <span class="n">a</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
    <span class="n">b</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
    <span class="n">s</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="n">out</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
<span class="p">):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">out</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">*</span> <span class="n">s</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">tid</span><span class="p">])</span> <span class="o">**</span> <span class="mf">2.0</span>

<span class="n">jax_scale</span> <span class="o">=</span> <span class="n">jax_kernel</span><span class="p">(</span><span class="n">scale_sum_square</span><span class="p">,</span> <span class="n">num_outputs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">enable_backward</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># scalars must be static</span>
<span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnames</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;s&quot;</span><span class="p">])</span>
<span class="k">def</span><span class="w"> </span><span class="nf">loss</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
    <span class="p">(</span><span class="n">out</span><span class="p">,)</span> <span class="o">=</span> <span class="n">jax_scale</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">s</span> <span class="o">=</span> <span class="mf">2.0</span>

<span class="c1"># gradients w.r.t. array inputs</span>
<span class="n">da</span><span class="p">,</span> <span class="n">db</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">da</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">db</span><span class="p">)</span>
</pre></div>
</div>
<p>Multiple outputs:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">jax</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jnp</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warp</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">wp</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">warp.jax_experimental</span><span class="w"> </span><span class="kn">import</span> <span class="n">jax_kernel</span>

<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">multi_output</span><span class="p">(</span>
    <span class="n">a</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
    <span class="n">b</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
    <span class="n">s</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="n">c</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
    <span class="n">d</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
<span class="p">):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">c</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">**</span> <span class="mf">2.0</span>
    <span class="n">d</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">*</span> <span class="n">b</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">*</span> <span class="n">s</span>

<span class="n">jax_multi</span> <span class="o">=</span> <span class="n">jax_kernel</span><span class="p">(</span><span class="n">multi_output</span><span class="p">,</span> <span class="n">num_outputs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">enable_backward</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">caller</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
    <span class="n">c</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">c</span> <span class="o">+</span> <span class="n">d</span><span class="p">)</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">s</span> <span class="o">=</span> <span class="mf">2.0</span>

<span class="c1"># differentiate a batched scalar objective over two inputs</span>
<span class="n">da</span><span class="p">,</span> <span class="n">db</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">s</span><span class="p">:</span> <span class="n">caller</span><span class="p">(</span><span class="n">jax_multi</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">s</span><span class="p">),</span> <span class="n">argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">da</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">db</span><span class="p">)</span>
</pre></div>
</div>
<p>Vector and matrix arrays also work. Inner component dimensions are packed in the JAX array and handled automatically:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">functools</span><span class="w"> </span><span class="kn">import</span> <span class="n">partial</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jnp</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warp</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">wp</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">warp.jax_experimental</span><span class="w"> </span><span class="kn">import</span> <span class="n">jax_kernel</span>

<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">scale_vec2</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">vec2</span><span class="p">),</span> <span class="n">s</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">out</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">vec2</span><span class="p">)):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">out</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">*</span> <span class="n">s</span>

<span class="n">jax_vec</span> <span class="o">=</span> <span class="n">jax_kernel</span><span class="p">(</span><span class="n">scale_vec2</span><span class="p">,</span> <span class="n">num_outputs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">enable_backward</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnames</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;s&quot;</span><span class="p">])</span>
<span class="k">def</span><span class="w"> </span><span class="nf">vec_loss</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
    <span class="p">(</span><span class="n">out</span><span class="p">,)</span> <span class="o">=</span> <span class="n">jax_vec</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

<span class="n">a2</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>  <span class="c1"># vec2 payload shape</span>
<span class="p">(</span><span class="n">da2</span><span class="p">,)</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">vec_loss</span><span class="p">,</span> <span class="n">argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))(</span><span class="n">a2</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">da2</span><span class="p">)</span>
</pre></div>
</div>
<section id="limitations">
<h5>Limitations<a class="headerlink" href="#limitations" title="Link to this heading">#</a></h5>
<p>The autodiff functionality is considered experimental and is still a work in progress.</p>
<ul class="simple">
<li><p>Scalar inputs must be static arguments in JAX.</p></li>
<li><p>Gradients are returned for differentiable array inputs (static scalars are excluded from the gradient tuple).</p></li>
<li><p>Input-output arguments (<code class="docutils literal notranslate"><span class="pre">in_out_argnames</span></code>) are not supported when <code class="docutils literal notranslate"><span class="pre">enable_backward=True</span></code>, because in-place modifications are not differentiable.</p></li>
<li><p>Custom launch and output dimensions (<code class="docutils literal notranslate"><span class="pre">launch_dims</span></code>, <code class="docutils literal notranslate"><span class="pre">output_dims</span></code>) are not currently supported when <code class="docutils literal notranslate"><span class="pre">enable_backward=True</span></code>, but the goal is to support them in the future. Launch dimensions are inferred from the shape of the first array argument, thus at least one input array is required.</p></li>
</ul>
</section>
</section>
<section id="calling-annotated-python-functions">
<h4>Calling Annotated Python Functions<a class="headerlink" href="#calling-annotated-python-functions" title="Link to this heading">#</a></h4>
<p>The <a class="reference internal" href="../api_reference/_generated/warp.jax_experimental.ffi.jax_kernel.html#warp.jax_experimental.ffi.jax_kernel" title="warp.jax_experimental.ffi.jax_kernel"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax_kernel()</span></code></a> mechanism can be used to launch a single Warp kernel
from JAX, but it’s also possible to call a Python function that launches multiple kernels.
The target Python function should have argument type annotations as if it were a Warp kernel.
To call this function from JAX, use <a class="reference internal" href="../api_reference/_generated/warp.jax_experimental.ffi.jax_callable.html#warp.jax_experimental.ffi.jax_callable" title="warp.jax_experimental.ffi.jax_callable"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax_callable()</span></code></a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">warp.jax_experimental</span><span class="w"> </span><span class="kn">import</span> <span class="n">jax_callable</span>

<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">scale_kernel</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span> <span class="n">s</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">output</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">output</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">*</span> <span class="n">s</span>

<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">scale_vec_kernel</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">vec2</span><span class="p">),</span> <span class="n">s</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">output</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">vec2</span><span class="p">)):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">output</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">*</span> <span class="n">s</span>


<span class="c1"># The Python function to call.</span>
<span class="c1"># Note the argument type annotations, just like Warp kernels.</span>
<span class="k">def</span><span class="w"> </span><span class="nf">example_func</span><span class="p">(</span>
    <span class="c1"># inputs</span>
    <span class="n">a</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
    <span class="n">b</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">vec2</span><span class="p">),</span>
    <span class="n">s</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="c1"># outputs</span>
    <span class="n">c</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
    <span class="n">d</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">vec2</span><span class="p">),</span>
<span class="p">):</span>
    <span class="c1"># launch multiple kernels</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">scale_kernel</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">a</span><span class="p">,</span> <span class="n">s</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">c</span><span class="p">])</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">scale_vec_kernel</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">s</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">d</span><span class="p">])</span>


<span class="n">jax_func</span> <span class="o">=</span> <span class="n">jax_callable</span><span class="p">(</span><span class="n">example_func</span><span class="p">,</span> <span class="n">num_outputs</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">():</span>
    <span class="c1"># inputs</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>  <span class="c1"># wp.vec2</span>
    <span class="n">s</span> <span class="o">=</span> <span class="mf">2.0</span>

    <span class="c1"># output shapes</span>
    <span class="n">output_dims</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;c&quot;</span><span class="p">:</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;d&quot;</span><span class="p">:</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">}</span>

    <span class="n">c</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">jax_func</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">output_dims</span><span class="o">=</span><span class="n">output_dims</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span>

<span class="n">r1</span><span class="p">,</span> <span class="n">r2</span> <span class="o">=</span> <span class="n">f</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">r1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">r2</span><span class="p">)</span>
</pre></div>
</div>
<p>The input and output semantics of <a class="reference internal" href="../api_reference/_generated/warp.jax_experimental.ffi.jax_callable.html#warp.jax_experimental.ffi.jax_callable" title="warp.jax_experimental.ffi.jax_callable"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax_callable()</span></code></a> are similar to
<a class="reference internal" href="../api_reference/_generated/warp.jax_experimental.ffi.jax_kernel.html#warp.jax_experimental.ffi.jax_kernel" title="warp.jax_experimental.ffi.jax_kernel"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax_kernel()</span></code></a>, so we won’t recap everything here,
just focus on the differences:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../api_reference/_generated/warp.jax_experimental.ffi.jax_callable.html#warp.jax_experimental.ffi.jax_callable" title="warp.jax_experimental.ffi.jax_callable"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax_callable()</span></code></a> does not take a <code class="docutils literal notranslate"><span class="pre">launch_dims</span></code> argument,
since the target function is responsible for launching kernels using appropriate dimensions.</p></li>
<li><p><a class="reference internal" href="../api_reference/_generated/warp.jax_experimental.ffi.jax_callable.html#warp.jax_experimental.ffi.jax_callable" title="warp.jax_experimental.ffi.jax_callable"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax_callable()</span></code></a> takes an optional <code class="docutils literal notranslate"><span class="pre">graph_mode</span></code> argument, which determines how the callable can be captured in a CUDA graph.
Graphs are generally desirable, since they can greatly improve the application performance.
<code class="docutils literal notranslate"><span class="pre">GraphMode.JAX</span></code> (default) lets JAX capture the graph, which may be used as a subgraph in an enclosing capture for maximal benefit.
<code class="docutils literal notranslate"><span class="pre">GraphMode.WARP</span></code> lets Warp capture the graph. Use this mode when the callable cannot be used as a subgraph, such as when the callable uses conditional graph nodes.
<code class="docutils literal notranslate"><span class="pre">GraphMode.NONE</span></code> disables graph capture. Use this mode if the callable performs operations that are not allowed during graph capture, such as host synchronization.</p></li>
</ul>
<p>See <a class="reference external" href="https://github.com/NVIDIA/warp/tree/main/warp/examples/interop/example_jax_callable.py">example_jax_callable.py</a> for examples.</p>
</section>
<section id="generic-jax-ffi-callbacks">
<h4>Generic JAX FFI Callbacks<a class="headerlink" href="#generic-jax-ffi-callbacks" title="Link to this heading">#</a></h4>
<p>Another way to call Python functions is to use
<a class="reference internal" href="../api_reference/_generated/warp.jax_experimental.ffi.register_ffi_callback.html#warp.jax_experimental.ffi.register_ffi_callback" title="warp.jax_experimental.ffi.register_ffi_callback"><code class="xref py py-func docutils literal notranslate"><span class="pre">register_ffi_callback()</span></code></a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">warp.jax_experimental</span><span class="w"> </span><span class="kn">import</span> <span class="n">register_ffi_callback</span>
</pre></div>
</div>
<p>This allows calling functions that don’t have Warp-style type annotations, but must have the form:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">func</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">attrs</span><span class="p">,</span> <span class="n">ctx</span><span class="p">)</span>
</pre></div>
</div>
<p>where:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">inputs</span></code> is a list of input buffers.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">outputs</span></code> is a list of output buffers.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">attrs</span></code> is a dictionary of attributes.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ctx</span></code> is the execution context, including the CUDA stream.</p></li>
</ul>
<p>The input and output buffers are neither JAX nor Warp arrays.
They are objects that expose the <code class="docutils literal notranslate"><span class="pre">__cuda_array_interface__</span></code>, which can be passed to Warp kernels directly.
Here is an example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">warp.jax_experimental</span><span class="w"> </span><span class="kn">import</span> <span class="n">register_ffi_callback</span>

<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">scale_kernel</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span> <span class="n">s</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">output</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">output</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">*</span> <span class="n">s</span>

<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">scale_vec_kernel</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">vec2</span><span class="p">),</span> <span class="n">s</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">output</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">vec2</span><span class="p">)):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">output</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">*</span> <span class="n">s</span>

<span class="c1"># the Python function to call</span>
<span class="k">def</span><span class="w"> </span><span class="nf">warp_func</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">attrs</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
    <span class="c1"># input arrays</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># scalar attributes</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">attrs</span><span class="p">[</span><span class="s2">&quot;scale&quot;</span><span class="p">]</span>

    <span class="c1"># output arrays</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">device</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">device_from_jax</span><span class="p">(</span><span class="n">get_jax_device</span><span class="p">())</span>
    <span class="n">stream</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">Stream</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">cuda_stream</span><span class="o">=</span><span class="n">ctx</span><span class="o">.</span><span class="n">stream</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">wp</span><span class="o">.</span><span class="n">ScopedStream</span><span class="p">(</span><span class="n">stream</span><span class="p">):</span>
        <span class="c1"># launch with arrays of scalars</span>
        <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">scale_kernel</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">a</span><span class="p">,</span> <span class="n">s</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">c</span><span class="p">])</span>

        <span class="c1"># launch with arrays of vec2</span>
        <span class="c1"># NOTE: the input shapes are from JAX arrays, so we need to strip the inner dimension for vec2 arrays</span>
        <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">scale_vec_kernel</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">s</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">d</span><span class="p">])</span>

<span class="c1"># register callback</span>
<span class="n">register_ffi_callback</span><span class="p">(</span><span class="s2">&quot;warp_func&quot;</span><span class="p">,</span> <span class="n">warp_func</span><span class="p">)</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>

<span class="c1"># inputs</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">n</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>  <span class="c1"># array of wp.vec2</span>
<span class="n">s</span> <span class="o">=</span> <span class="mf">2.0</span>

<span class="c1"># set up the call</span>
<span class="n">out_types</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">jax</span><span class="o">.</span><span class="n">ShapeDtypeStruct</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
    <span class="n">jax</span><span class="o">.</span><span class="n">ShapeDtypeStruct</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>  <span class="c1"># array of wp.vec2</span>
<span class="p">]</span>
<span class="n">call</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">ffi</span><span class="o">.</span><span class="n">ffi_call</span><span class="p">(</span><span class="s2">&quot;warp_func&quot;</span><span class="p">,</span> <span class="n">out_types</span><span class="p">)</span>

<span class="c1"># call it</span>
<span class="n">c</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">call</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">s</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
</pre></div>
</div>
<p>This is a more low-level approach to JAX FFI callbacks.
A proposal was made to incorporate such a mechanism in JAX, but for now we have a prototype here.
This approach leaves a lot of work up to the user, such as verifying argument types and shapes,
but it can be used when other utilities like <a class="reference internal" href="../api_reference/_generated/warp.jax_experimental.ffi.jax_kernel.html#warp.jax_experimental.ffi.jax_kernel" title="warp.jax_experimental.ffi.jax_kernel"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax_kernel()</span></code></a> and
<a class="reference internal" href="../api_reference/_generated/warp.jax_experimental.ffi.jax_callable.html#warp.jax_experimental.ffi.jax_callable" title="warp.jax_experimental.ffi.jax_callable"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax_callable()</span></code></a> are not sufficient.</p>
<p>See <a class="reference external" href="https://github.com/NVIDIA/warp/tree/main/warp/examples/interop/example_jax_ffi_callback.py">example_jax_ffi_callback.py</a> for examples.</p>
</section>
</section>
<section id="distributed-computation">
<h3>Distributed Computation<a class="headerlink" href="#distributed-computation" title="Link to this heading">#</a></h3>
<p>Warp can be used in conjunction with JAX’s <a class="reference external" href="https://jax.readthedocs.io/en/latest/jep/14273-shard-map.html">shard_map</a>
to perform distributed multi-GPU computations.</p>
<p>To achieve this, the JAX distributed environment must be initialized
(see <a class="reference external" href="https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html">Distributed Arrays and Automatic Parallelization</a>
for more details):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">jax</span>
<span class="n">jax</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">initialize</span><span class="p">()</span>
</pre></div>
</div>
<p>This initialization must be called at the beginning of your program, before any other JAX operations.</p>
<p>Here’s an example of how to use <code class="docutils literal notranslate"><span class="pre">shard_map</span></code> with a Warp kernel:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">warp</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">wp</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jnp</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">jax.sharding</span><span class="w"> </span><span class="kn">import</span> <span class="n">PartitionSpec</span> <span class="k">as</span> <span class="n">P</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">jax.experimental.multihost_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">process_allgather</span> <span class="k">as</span> <span class="n">allgather</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">jax.experimental.shard_map</span><span class="w"> </span><span class="kn">import</span> <span class="n">shard_map</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">warp.jax_experimental</span><span class="w"> </span><span class="kn">import</span> <span class="n">jax_kernel</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># Initialize JAX distributed environment</span>
<span class="n">jax</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">initialize</span><span class="p">()</span>
<span class="n">num_gpus</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>

<span class="k">def</span><span class="w"> </span><span class="nf">print_on_process_0</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">jax</span><span class="o">.</span><span class="n">process_index</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="n">print_on_process_0</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Running on </span><span class="si">{</span><span class="n">num_gpus</span><span class="si">}</span><span class="s2"> GPU(s)&quot;</span><span class="p">)</span>

<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">multiply_by_two_kernel</span><span class="p">(</span>
    <span class="n">a_in</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
    <span class="n">a_out</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
<span class="p">):</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">a_out</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">a_in</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">*</span> <span class="mf">2.0</span>

<span class="n">jax_warp_multiply</span> <span class="o">=</span> <span class="n">jax_kernel</span><span class="p">(</span><span class="n">multiply_by_two_kernel</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">warp_multiply</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">jax_warp_multiply</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>

    <span class="c1"># a_in here is the full sharded array with shape (M,)</span>
    <span class="c1"># The output will also be a sharded array with shape (M,)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">warp_distributed_operator</span><span class="p">(</span><span class="n">a_in</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_sharded_operator</span><span class="p">(</span><span class="n">a_in</span><span class="p">):</span>
        <span class="c1"># Inside the sharded operator, a_in is a local shard on each device</span>
        <span class="c1"># If we have N devices and input size M, each shard has shape (M/N,)</span>

        <span class="c1"># warp_multiply applies the Warp kernel to the local shard</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">warp_multiply</span><span class="p">(</span><span class="n">a_in</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># result has the same shape as the input shard (M/N,)</span>
        <span class="k">return</span> <span class="n">result</span>

    <span class="c1"># shard_map distributes the computation across devices</span>
    <span class="k">return</span> <span class="n">shard_map</span><span class="p">(</span>
        <span class="n">_sharded_operator</span><span class="p">,</span>
        <span class="n">mesh</span><span class="o">=</span><span class="n">jax</span><span class="o">.</span><span class="n">sharding</span><span class="o">.</span><span class="n">Mesh</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">devices</span><span class="p">()),</span> <span class="s2">&quot;x&quot;</span><span class="p">),</span>
        <span class="n">in_specs</span><span class="o">=</span><span class="p">(</span><span class="n">P</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">),),</span>  <span class="c1"># Input is sharded along the &#39;x&#39; axis</span>
        <span class="n">out_specs</span><span class="o">=</span><span class="n">P</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">),</span>    <span class="c1"># Output is also sharded along the &#39;x&#39; axis</span>
        <span class="n">check_rep</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)(</span><span class="n">a_in</span><span class="p">)</span>

<span class="n">print_on_process_0</span><span class="p">(</span><span class="s2">&quot;Test distributed multiplication using JAX + Warp&quot;</span><span class="p">)</span>

<span class="n">devices</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">devices</span><span class="p">()</span>
<span class="n">mesh</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">sharding</span><span class="o">.</span><span class="n">Mesh</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">devices</span><span class="p">),</span> <span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">sharding_spec</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">sharding</span><span class="o">.</span><span class="n">NamedSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">P</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">))</span>

<span class="n">input_size</span> <span class="o">=</span> <span class="n">num_gpus</span> <span class="o">*</span> <span class="mi">5</span>  <span class="c1"># 5 elements per device</span>
<span class="n">single_device_arrays</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># Define the shape of the input array based on the total input size</span>
<span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_size</span><span class="p">,)</span>

<span class="c1"># Create a list of arrays by distributing the single_device_arrays across the available devices</span>
<span class="c1"># Each device will receive a portion of the input data</span>
<span class="n">arrays</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">jax</span><span class="o">.</span><span class="n">device_put</span><span class="p">(</span><span class="n">single_device_arrays</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">d</span><span class="p">)</span>  <span class="c1"># Place each element on the corresponding device</span>
    <span class="k">for</span> <span class="n">d</span><span class="p">,</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">sharding_spec</span><span class="o">.</span><span class="n">addressable_devices_indices_map</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
<span class="p">]</span>

<span class="c1"># Combine the individual device arrays into a single sharded array</span>
<span class="n">sharded_array</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">make_array_from_single_device_arrays</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">sharding_spec</span><span class="p">,</span> <span class="n">arrays</span><span class="p">)</span>

<span class="c1"># sharded_array has shape (input_size,) but is distributed across devices</span>
<span class="n">print_on_process_0</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input array: </span><span class="si">{</span><span class="n">allgather</span><span class="p">(</span><span class="n">sharded_array</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># warp_result has the same shape and sharding as sharded_array</span>
<span class="n">warp_result</span> <span class="o">=</span> <span class="n">warp_distributed_operator</span><span class="p">(</span><span class="n">sharded_array</span><span class="p">)</span>

<span class="c1"># allgather collects results from all devices, resulting in a full array of shape (input_size,)</span>
<span class="n">print_on_process_0</span><span class="p">(</span><span class="s2">&quot;Warp Output:&quot;</span><span class="p">,</span> <span class="n">allgather</span><span class="p">(</span><span class="n">warp_result</span><span class="p">))</span>
</pre></div>
</div>
<p>In this example, <code class="docutils literal notranslate"><span class="pre">shard_map</span></code> is used to distribute the computation across available devices.
The input array <code class="docutils literal notranslate"><span class="pre">a_in</span></code> is sharded along the ‘x’ axis, and each device processes its local shard.
The Warp kernel <code class="docutils literal notranslate"><span class="pre">multiply_by_two_kernel</span></code> is applied to each shard, and the results are combined to form the final output.</p>
<p>This approach allows for efficient parallel processing of large arrays, as each device works on a portion of the data simultaneously.</p>
<p>To run this program on multiple GPUs, you must have Open MPI installed.
You can consult the <a class="reference external" href="https://docs.open-mpi.org/en/main/installing-open-mpi/quickstart.html">OpenMPI installation guide</a>
for instructions on how to install it.
Once Open MPI is installed, you can use <code class="docutils literal notranslate"><span class="pre">mpirun</span></code> with the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mpirun<span class="w"> </span>-np<span class="w"> </span>&lt;NUM_OF_GPUS&gt;<span class="w"> </span>python<span class="w"> </span>&lt;filename&gt;.py
</pre></div>
</div>
</section>
</section>
<section id="dlpack">
<span id="id1"></span><h2>DLPack<a class="headerlink" href="#dlpack" title="Link to this heading">#</a></h2>
<p>Warp supports the DLPack protocol included in the Python Array API standard v2022.12.
See the <a class="reference external" href="https://dmlc.github.io/dlpack/latest/python_spec.html">Python Specification for DLPack</a> for reference.</p>
<p>The canonical way to import an external array into Warp is using the <a class="reference internal" href="../api_reference/_generated/warp.from_dlpack.html#warp.from_dlpack" title="warp.from_dlpack"><code class="xref py py-func docutils literal notranslate"><span class="pre">warp.from_dlpack()</span></code></a> function:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">warp_array</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">external_array</span><span class="p">)</span>
</pre></div>
</div>
<p>The external array can be a PyTorch tensor, Jax array, or any other array type compatible with this version of the DLPack protocol.
For CUDA arrays, this approach requires the producer to perform stream synchronization which ensures that operations on the array
are ordered correctly.  The <a class="reference internal" href="../api_reference/_generated/warp.from_dlpack.html#warp.from_dlpack" title="warp.from_dlpack"><code class="xref py py-func docutils literal notranslate"><span class="pre">warp.from_dlpack()</span></code></a> function asks the producer to synchronize the current Warp stream on the device where
the array resides.  Thus it should be safe to use the array in Warp kernels on that device without any additional synchronization.</p>
<p>The canonical way to export a Warp array to an external framework is to use the <code class="docutils literal notranslate"><span class="pre">from_dlpack()</span></code> function in that framework:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">jax_array</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">dlpack</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">warp_array</span><span class="p">)</span>
<span class="n">torch_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">dlpack</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">warp_array</span><span class="p">)</span>
<span class="n">paddle_tensor</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">dlpack</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">warp_array</span><span class="p">)</span>
</pre></div>
</div>
<p>For CUDA arrays, this will synchronize the current stream of the consumer framework with the current Warp stream on the array’s device.
Thus it should be safe to use the wrapped array in the consumer framework, even if the array was previously used in a Warp kernel
on the device.</p>
<p>Alternatively, arrays can be shared by explicitly creating PyCapsules using a <code class="docutils literal notranslate"><span class="pre">to_dlpack()</span></code> function provided by the producer framework.
This approach may be used for older versions of frameworks that do not support the v2022.12 standard:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">warp_array1</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">jax_array</span><span class="p">)</span>
<span class="n">warp_array2</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">dlpack</span><span class="o">.</span><span class="n">to_dlpack</span><span class="p">(</span><span class="n">torch_tensor</span><span class="p">))</span>
<span class="n">warp_array3</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">paddle</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">dlpack</span><span class="o">.</span><span class="n">to_dlpack</span><span class="p">(</span><span class="n">paddle_tensor</span><span class="p">))</span>

<span class="n">jax_array</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">dlpack</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">wp</span><span class="o">.</span><span class="n">to_dlpack</span><span class="p">(</span><span class="n">warp_array</span><span class="p">))</span>
<span class="n">torch_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">dlpack</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">wp</span><span class="o">.</span><span class="n">to_dlpack</span><span class="p">(</span><span class="n">warp_array</span><span class="p">))</span>
<span class="n">paddle_tensor</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">dlpack</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">wp</span><span class="o">.</span><span class="n">to_dlpack</span><span class="p">(</span><span class="n">warp_array</span><span class="p">))</span>
</pre></div>
</div>
<p>This approach is generally faster because it skips any stream synchronization, but another solution must be used to ensure correct
ordering of operations.  In situations where no synchronization is required, using this approach can yield better performance.
This may be a good choice in situations like these:</p>
<ul class="simple">
<li><p>The external framework is using the synchronous CUDA default stream.</p></li>
<li><p>Warp and the external framework are using the same CUDA stream.</p></li>
<li><p>Another synchronization mechanism is already in place.</p></li>
</ul>
</section>
<section id="paddle">
<span id="paddle-interop"></span><h2>Paddle<a class="headerlink" href="#paddle" title="Link to this heading">#</a></h2>
<p>Warp provides helper functions to convert arrays to/from Paddle:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

<span class="c1"># convert to Paddle tensor</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">to_paddle</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>

<span class="c1"># convert from Paddle tensor</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_paddle</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</pre></div>
</div>
<p>These helper functions allow the conversion of Warp arrays to/from Paddle tensors without copying the underlying data.
At the same time, if available, gradient arrays and tensors are converted to/from Paddle autograd tensors, allowing the use of Warp arrays
in Paddle autograd computations.</p>
<p>To convert a Paddle CUDA stream to a Warp CUDA stream and vice versa, Warp provides the following function:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../api_reference/_generated/warp.stream_from_paddle.html#warp.stream_from_paddle" title="warp.stream_from_paddle"><code class="xref py py-func docutils literal notranslate"><span class="pre">warp.stream_from_paddle()</span></code></a></p></li>
</ul>
<section id="example-optimization-using-warp-from-paddle">
<h3>Example: Optimization using <a class="reference internal" href="../api_reference/_generated/warp.from_paddle.html#warp.from_paddle" title="warp.from_paddle"><code class="xref py py-func docutils literal notranslate"><span class="pre">warp.from_paddle()</span></code></a><a class="headerlink" href="#example-optimization-using-warp-from-paddle" title="Link to this heading">#</a></h3>
<p>An example usage of minimizing a loss function over an array of 2D points written in Warp via Paddle’s Adam optimizer
using <a class="reference internal" href="../api_reference/_generated/warp.from_paddle.html#warp.from_paddle" title="warp.from_paddle"><code class="xref py py-func docutils literal notranslate"><span class="pre">warp.from_paddle()</span></code></a> is as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">warp</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">wp</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">paddle</span>

<span class="c1"># init warp context at beginning</span>
<span class="n">wp</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>

<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span><span class="p">()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">loss</span><span class="p">(</span><span class="n">xs</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">ndim</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">l</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">atomic_add</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">xs</span><span class="p">[</span><span class="n">tid</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">**</span> <span class="mf">2.0</span> <span class="o">+</span> <span class="n">xs</span><span class="p">[</span><span class="n">tid</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="mf">2.0</span><span class="p">)</span>

<span class="c1"># indicate requires_grad so that Warp can accumulate gradients in the grad buffers</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">randn</span><span class="p">([</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">xs</span><span class="o">.</span><span class="n">stop_gradient</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">l</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
<span class="n">l</span><span class="o">.</span><span class="n">stop_gradient</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">parameters</span><span class="o">=</span><span class="p">[</span><span class="n">xs</span><span class="p">])</span>

<span class="n">wp_xs</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_paddle</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
<span class="n">wp_l</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_paddle</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>

<span class="n">tape</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">Tape</span><span class="p">()</span>
<span class="k">with</span> <span class="n">tape</span><span class="p">:</span>
    <span class="c1"># record the loss function kernel launch on the tape</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">wp_xs</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">wp_l</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">wp_xs</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="n">tape</span><span class="o">.</span><span class="n">zero</span><span class="p">()</span>
    <span class="n">tape</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">wp_l</span><span class="p">)</span>  <span class="c1"># compute gradients</span>
    <span class="c1"># now xs.grad will be populated with the gradients computed by Warp</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># update xs (and thereby wp_xs)</span>

    <span class="c1"># these lines are only needed for evaluating the loss</span>
    <span class="c1"># (the optimization just needs the gradient, not the loss value)</span>
    <span class="n">wp_l</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">wp_xs</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">wp_l</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">wp_xs</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="se">\t</span><span class="s2">loss: </span><span class="si">{</span><span class="n">l</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="example-optimization-using-warp-to-paddle">
<h3>Example: Optimization using <a class="reference internal" href="../api_reference/_generated/warp.to_paddle.html#warp.to_paddle" title="warp.to_paddle"><code class="xref py py-func docutils literal notranslate"><span class="pre">warp.to_paddle()</span></code></a><a class="headerlink" href="#example-optimization-using-warp-to-paddle" title="Link to this heading">#</a></h3>
<p>Less code is needed when we declare the optimization variables directly in Warp and use <a class="reference internal" href="../api_reference/_generated/warp.to_paddle.html#warp.to_paddle" title="warp.to_paddle"><code class="xref py py-func docutils literal notranslate"><span class="pre">warp.to_paddle()</span></code></a> to convert them to Paddle tensors.
Here, we revisit the same example from above where now only a single conversion to a Paddle tensor is needed to supply Adam with the optimization variables:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">warp</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">wp</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">paddle</span>

<span class="c1"># init warp context at beginning</span>
<span class="n">wp</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>

<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span><span class="p">()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">loss</span><span class="p">(</span><span class="n">xs</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">ndim</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">l</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">atomic_add</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">xs</span><span class="p">[</span><span class="n">tid</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">**</span> <span class="mf">2.0</span> <span class="o">+</span> <span class="n">xs</span><span class="p">[</span><span class="n">tid</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="mf">2.0</span><span class="p">)</span>

<span class="c1"># initialize the optimization variables in Warp</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">l</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># just a single wp.to_paddle call is needed, Adam optimizes using the Warp array gradients</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">parameters</span><span class="o">=</span><span class="p">[</span><span class="n">wp</span><span class="o">.</span><span class="n">to_paddle</span><span class="p">(</span><span class="n">xs</span><span class="p">)])</span>

<span class="n">tape</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">Tape</span><span class="p">()</span>
<span class="k">with</span> <span class="n">tape</span><span class="p">:</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">xs</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">xs</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="n">tape</span><span class="o">.</span><span class="n">zero</span><span class="p">()</span>
    <span class="n">tape</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">l</span><span class="p">)</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="n">l</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">xs</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">xs</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="se">\t</span><span class="s2">loss: </span><span class="si">{</span><span class="n">l</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id2">
<h3>Performance Notes<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<p>The <a class="reference internal" href="../api_reference/_generated/warp.from_paddle.html#warp.from_paddle" title="warp.from_paddle"><code class="xref py py-func docutils literal notranslate"><span class="pre">wp.from_paddle()</span></code></a> function creates a Warp array object that shares data with a Paddle tensor.
Although this function does not copy the data, there is always some CPU overhead during the conversion.
If these conversions happen frequently, the overall program performance may suffer.
As a general rule, it’s good to avoid repeated conversions of the same tensor.
Instead of:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_t</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">device_to_paddle</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
<span class="n">y_t</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">n</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">device_to_paddle</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">x_w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_paddle</span><span class="p">(</span><span class="n">x_t</span><span class="p">)</span>
    <span class="n">y_w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_paddle</span><span class="p">(</span><span class="n">y_t</span><span class="p">)</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">saxpy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x_w</span><span class="p">,</span> <span class="n">y_w</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<p>Try converting the arrays only once and reuse them:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_t</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">device_to_paddle</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
<span class="n">y_t</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">n</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">device_to_paddle</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>

<span class="n">x_w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_paddle</span><span class="p">(</span><span class="n">x_t</span><span class="p">)</span>
<span class="n">y_w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_paddle</span><span class="p">(</span><span class="n">y_t</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">saxpy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x_w</span><span class="p">,</span> <span class="n">y_w</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<p>If reusing arrays is not possible (e.g., a new Paddle tensor is constructed on every iteration), passing <code class="docutils literal notranslate"><span class="pre">return_ctype=True</span></code> to
<a class="reference internal" href="../api_reference/_generated/warp.from_paddle.html#warp.from_paddle" title="warp.from_paddle"><code class="xref py py-func docutils literal notranslate"><span class="pre">wp.from_paddle()</span></code></a> should yield faster performance.
Setting this argument to <code class="docutils literal notranslate"><span class="pre">True</span></code> avoids constructing a <a class="reference internal" href="../api_reference/_generated/warp.array.html#warp.array" title="warp.array"><code class="xref py py-class docutils literal notranslate"><span class="pre">wp.array</span></code></a> object and instead returns a low-level array descriptor.
This descriptor is a simple C structure that can be passed to Warp kernels instead of a <a class="reference internal" href="../api_reference/_generated/warp.array.html#warp.array" title="warp.array"><code class="xref py py-class docutils literal notranslate"><span class="pre">wp.array</span></code></a>, but cannot be used in other places that require a <a class="reference internal" href="../api_reference/_generated/warp.array.html#warp.array" title="warp.array"><code class="xref py py-class docutils literal notranslate"><span class="pre">wp.array</span></code></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">):</span>
    <span class="c1"># get Paddle tensors for this iteration</span>
    <span class="n">x_t</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">device_to_paddle</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
    <span class="n">y_t</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">n</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">device_to_paddle</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>

    <span class="c1"># get Warp array descriptors</span>
    <span class="n">x_ctype</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_paddle</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">return_ctype</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">y_ctype</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_paddle</span><span class="p">(</span><span class="n">y_t</span><span class="p">,</span> <span class="n">return_ctype</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">saxpy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x_ctype</span><span class="p">,</span> <span class="n">y_ctype</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<p>An alternative approach is to pass the Paddle tensors to Warp kernels directly.
This avoids constructing temporary Warp arrays by leveraging standard array interfaces (like <code class="docutils literal notranslate"><span class="pre">__cuda_array_interface__</span></code>) supported by both Paddle and Warp.
The main advantage of this approach is convenience, since there is no need to call any conversion functions.
The main limitation is that it does not handle gradients, because gradient information is not included in the standard array interfaces.
This technique is therefore most suitable for algorithms that do not involve differentiation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">device_to_paddle</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">n</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">device_to_paddle</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">saxpy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>warp.examples.benchmarks.benchmark_interop_paddle
</pre></div>
</div>
<p>Sample output:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">13990</span> <span class="n">ms</span>  <span class="n">from_paddle</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
 <span class="mi">5990</span> <span class="n">ms</span>  <span class="n">from_paddle</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">return_ctype</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="mi">35167</span> <span class="n">ms</span>  <span class="n">direct</span> <span class="kn">from</span><span class="w"> </span><span class="nn">paddle</span>
</pre></div>
</div>
<p>The default <a class="reference internal" href="../api_reference/_generated/warp.from_paddle.html#warp.from_paddle" title="warp.from_paddle"><code class="xref py py-func docutils literal notranslate"><span class="pre">wp.from_paddle()</span></code></a> conversion is the slowest.
Passing <code class="docutils literal notranslate"><span class="pre">return_ctype=True</span></code> is the fastest, because it skips creating temporary Warp array objects.
Passing Paddle tensors to Warp kernels directly falls somewhere in between.
It skips creating temporary Warp arrays, but accessing the <code class="docutils literal notranslate"><span class="pre">__cuda_array_interface__</span></code> attributes of Paddle tensors adds overhead because they are initialized on-demand.</p>
</section>
</section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="tiles.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Tiles</p>
      </div>
    </a>
    <a class="right-next"
       href="configuration.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Configuration</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            


              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#numpy">NumPy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch">PyTorch</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-optimization-using-warp-from-torch">Example: Optimization using <code class="xref py py-func docutils literal notranslate"><span class="pre">warp.from_torch()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-optimization-using-warp-to-torch">Example: Optimization using <code class="xref py py-func docutils literal notranslate"><span class="pre">warp.to_torch()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-optimization-using-torch-autograd-function-pytorch-2-3-1">Example: Optimization using <code class="docutils literal notranslate"><span class="pre">torch.autograd.function</span></code> (PyTorch &lt;= 2.3.1)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-optimization-using-pytorch-custom-operators-pytorch-2-4-0">Example: Optimization using PyTorch custom operators (PyTorch &gt;= 2.4.0)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-notes">Performance Notes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#case-study-pytorch-deferred-gradient-allocation-and-warp-interoperability">Case Study: PyTorch Deferred Gradient Allocation and Warp Interoperability</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-problem-deferred-gradient-allocation">The Problem: Deferred Gradient Allocation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#solutions">Solutions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-comparison">Performance Comparison</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cupy-numba">CuPy/Numba</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#jax">JAX</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-warp-kernels-as-jax-primitives">Using Warp kernels as JAX primitives</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#input-and-output-semantics">Input and Output Semantics</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#scalar-inputs">Scalar Inputs</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-launch-and-output-dimensions">Kernel Launch and Output Dimensions</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-and-matrix-arrays">Vector and Matrix Arrays</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#jax-vmap-support">JAX VMAP Support</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#jax-automatic-differentiation">JAX Automatic Differentiation</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations">Limitations</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#calling-annotated-python-functions">Calling Annotated Python Functions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#generic-jax-ffi-callbacks">Generic JAX FFI Callbacks</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distributed-computation">Distributed Computation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dlpack">DLPack</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#paddle">Paddle</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-optimization-using-warp-from-paddle">Example: Optimization using <code class="xref py py-func docutils literal notranslate"><span class="pre">warp.from_paddle()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-optimization-using-warp-to-paddle">Example: Optimization using <code class="xref py py-func docutils literal notranslate"><span class="pre">warp.to_paddle()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Performance Notes</a></li>
</ul>
</li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">

  
  <div class="tocsection editthispage">
    <a href="https://github.com/NVIDIA/warp/edit/v1.11.1/docs/user_guide/interoperability.rst">
      <i class="fa-solid fa-pencil"></i>
      
      
        
          Edit on GitHub
        
      
    </a>
  </div>
</div>

</div></div>
              
            

          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  

  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>


  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">
<a class="footer-brand logo" href="https://www.nvidia.com">
  <img src="../_static/nvidia-logo-horiz-rgb-1c-blk-for-screen.svg" class="logo__image only-light" alt="NVIDIA"/>
  <img src="../_static/nvidia-logo-horiz-rgb-1c-wht-for-screen.svg" class="logo__image only-dark" alt="NVIDIA"/>
</a></div>
      
        <div class="footer-item">




  <p class="copyright">
    
      Copyright © 2022-2026 NVIDIA Corporation.
      <br/>
    
  </p>
</div>
      
    </div>
  
  
  
</div>

  </footer>
  </body>
</html>