# SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# ==============================================================================
# Warp+CUDA 13 Build and Testing Child Pipeline
#
# This child pipeline is used to build Warp when the library is built with the
# CUDA 13 Toolkit (release mode). No testing until GitLab runners have CUDA 13
# drivers.
#
# This pipeline can be triggered from the main GitLab pipeline under specific
# circumstances. See the child pipelines defined in /.gitlab-ci.yml for the
# trigger conditions. It is not automatically run in merge request pipelines.
# ==============================================================================

include:
  - /.gitlab/ci/common.yml

workflow:
  rules:
    - if: $CI_PIPELINE_SOURCE == "parent_pipeline"

stages:
  - build
  - quality
  - package
  - deploy

# ==============================================================================
# Build Jobs
# ==============================================================================

linux-aarch64 build:
  stage: build
  image: quay.io/pypa/manylinux_2_34_aarch64:latest
  extends:
    - .omni_devplat_arm_compute
    - .save_warp_bin_artifact
  variables:
    PM_PYTHON_EXT: "python"
  before_script:
    - echo -e "\\e[0Ksection_start:`date +%s`:install_dependencies[collapsed=true]\\r\\e[0KInstalling dependencies"
    - yum update -y && yum install curl wget -y
    - wget -O Miniforge3.sh "https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh"
    - bash Miniforge3.sh -b -p "${CI_PROJECT_DIR}/conda"
    - source "${CI_PROJECT_DIR}/conda/etc/profile.d/conda.sh"
    - conda activate
    - python -m pip install --upgrade pip
    - python -m pip install --upgrade numpy
    - tools/packman/packman pull --platform linux-aarch64 --include-tag cuda-13 deps/cuda-toolkit-deps.packman.xml
    - echo -e "\\e[0Ksection_end:`date +%s`:install_dependencies\\r\\e[0K"
  script:
    - python build_lib.py --cuda_path $CUDA
    - mkdir -p warp/bin/linux-aarch64
    - mv warp/bin/warp.so warp/bin/linux-aarch64
    - mv warp/bin/warp-clang.so warp/bin/linux-aarch64

windows-x86_64 build:
  stage: build
  extends:
    - .save_warp_bin_artifact
    - .runner-build-windows-x86_64
  before_script:
    - powershell -command "Get-Volume | Format-Table -AutoSize"
  script:
    - ./tools/ci/building/build-windows-x86_64/build.bat --cuda 13

# ==============================================================================
# Linting and Code Quality Jobs
#
# The jobs here are meant to assist with code quality analysis.
# They can run immediately without waiting for the build jobs to complete.
# ==============================================================================

check un-namespaced symbols linux-x86_64:
  stage: quality
  image: python:3.12-bullseye
  needs:
    - pipeline: $PARENT_PIPELINE_ID
      job: linux-x86_64 cuda 13 build
  extends:
    - .omni_nvks_micro_runner
  script:
    - |
      echo "Checking for un-namespaced symbols..."
      LIBRARIES_TO_CHECK="warp/bin/linux-x86_64/warp.so warp/bin/linux-x86_64/warp-clang.so"
      found_symbols=""

      for lib in $LIBRARIES_TO_CHECK; do
        echo "--> Checking $lib"
        output=$(readelf -Ws "$lib" | awk '$7 ~ /^[0-9]+$/ && $8 !~ /^_?wp_/ {print $8}' | grep -Ev '^(_Z|__|\.)' || true)
        
        if [ -n "$output" ]; then
          echo "ERROR: Found un-namespaced symbols in $lib:"
          echo "$output"
          found_symbols="true"
        fi
      done

      if [ -n "$found_symbols" ]; then
        echo "FAIL: Un-namespaced symbols detected. Please prefix them with 'wp_' or '_wp_'."
        exit 1
      fi

      echo "SUCCESS: All symbols are correctly namespaced."

# ==============================================================================
# Packaging Jobs
# ==============================================================================

# Creates wheel files for PyPI
# Note that compared to the job in .gitlab-ci.yml, there is no aarch64 job
# due to problems building a debug binary
create pypi wheels:
  stage: package
  image: ghcr.io/astral-sh/uv:python3.12-bookworm-slim
  needs:
    - linux-aarch64 build
    - windows-x86_64 build
    - pipeline: $PARENT_PIPELINE_ID
      job: linux-x86_64 cuda 13 build
  extends:
    - .omni_nvks_micro_runner
  before_script:
    # Move binaries into platform-specific folders. Already done in the build jobs for Linux.
    - mkdir -p warp/bin/windows-x86_64
    - mv warp/bin/warp.dll warp/bin/windows-x86_64/
    - mv warp/bin/warp-clang.dll warp/bin/windows-x86_64/
  script:
    - sed -i "s/^\(.*\)$/\1+cu13/" VERSION.md # Modify VERSION.md with +cu13
    - uv build --wheel -C--build-option=-Pwindows-x86_64
    - uv build --wheel -C--build-option=-Plinux-x86_64 -C--build-option=-Mmanylinux2014
    - uv build --wheel -C--build-option=-Plinux-aarch64 -C--build-option=-Mmanylinux2014
    - find . -type f -exec chmod 664 {} +
    - find . -type d -exec chmod 775 {} +
  artifacts:
    name: $PARENT_COMMIT_REF_SLUG-$PARENT_COMMIT_SHORT_SHA
    expose_as: "Python Wheels"
    paths:
      - "dist/"
    when: always

# ==============================================================================
# Deployment Jobs
#
# This section currently contains jobs that publish files to the internal
# GitLab service.
# ==============================================================================

publish wheels to gitlab pypi registry:
  stage: deploy
  image: ghcr.io/astral-sh/uv:python3.12-bookworm-slim
  needs: ["create pypi wheels"]
  extends:
    - .omni_nvks_micro_runner
  rules:
    - if: $PARENT_COMMIT_TAG
    - if: $PARENT_COMMIT_BRANCH =~ /release-.*/
    - when: manual # Can be triggered in all other scenarios
      allow_failure: true
  script:
    - TWINE_PASSWORD=${CI_JOB_TOKEN} TWINE_USERNAME=gitlab-ci-token uvx twine upload --verbose --skip-existing --non-interactive --repository-url ${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/packages/pypi dist/*

# Uploads the wheels to the internal GitLab package registry in the Warp project
# Generated files will be in a branch/tag-specific folder
publish wheels to gitlab package registry:
  stage: deploy
  needs: ["create pypi wheels"]
  extends:
    - .omni_nvks_micro_runner
  rules:
    - if: $PARENT_COMMIT_TAG
    - if: $PARENT_COMMIT_BRANCH =~ /release-.*/
    - when: manual # Can be triggered in all other scenarios
      allow_failure: true
  before_script:
    - apt-get update && apt-get install curl --no-install-recommends -y
  script:
    - |
      for file in $(find . -name '*.whl'); do
          filename=$(basename -- "$file")
          curl --header "JOB-TOKEN: $CI_JOB_TOKEN" --upload-file "$file" "${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/packages/generic/warp/${CI_COMMIT_REF_SLUG}/${filename}"
      done
    - echo "See the published files at $CI_PROJECT_URL/-/packages"

publish wheels to github release:
  stage: deploy
  image: ubuntu:22.04
  needs: ["create pypi wheels"]
  extends:
    - .omni_nvks_micro_runner
  rules:
    - if: $PARENT_COMMIT_TAG
  before_script:
    - echo -e "\\e[0Ksection_start:`date +%s`:install_dependencies[collapsed=true]\\r\\e[0KInstalling dependencies"
    - apt-get update && apt-get install wget ca-certificates --no-install-recommends -y
    - wget -O Miniforge3.sh "https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh"
    - bash Miniforge3.sh -b -p "${HOME}/conda"
    - source "${HOME}/conda/etc/profile.d/conda.sh"
    - conda activate
    - conda install gh --channel conda-forge
    - echo -e "\\e[0Ksection_end:`date +%s`:install_dependencies\\r\\e[0K"
  script:
    - gh release upload $PARENT_COMMIT_TAG ./dist/*.whl

publish tag wheels to artifactory:
  stage: deploy
  image: releases-docker.jfrog.io/jfrog/jfrog-cli-v2-jf
  needs: ["create pypi wheels"]
  extends:
    - .omni_nvks_micro_runner
  rules:
    - if: $PARENT_COMMIT_TAG
  allow_failure: true
  before_script:
    - export VERSION=$(head -n 1 dist/VERSION.md)
  script:
    - cd dist
    - >
      jf rt u --fail-no-op --url=$ARTIFACTORY_BASE_URL --access-token $ARTIFACTORY_SVC_ACCESS_TOKEN
      --target-props 'component_name=warp-lang;release_approver=ershi;release_status=ready'
      '*.whl' sw-warp-pypi-local/warp/$PARENT_COMMIT_REF_NAME/$VERSION/
    # Set build-specific properties on individual artifacts
    - >
      jf rt sp --fail-no-op --url=$ARTIFACTORY_BASE_URL --access-token $ARTIFACTORY_SVC_ACCESS_TOKEN
      sw-warp-pypi-local/warp/$PARENT_COMMIT_REF_NAME/$VERSION/*-win_amd64.whl
      'arch=x86_64;os=windows'
    - >
      jf rt sp --fail-no-op --url=$ARTIFACTORY_BASE_URL --access-token $ARTIFACTORY_SVC_ACCESS_TOKEN
      sw-warp-pypi-local/warp/$PARENT_COMMIT_REF_NAME/$VERSION/*-manylinux_2_34_aarch64.whl
      'arch=aarch64;os=linux'
    - >
      jf rt sp --fail-no-op --url=$ARTIFACTORY_BASE_URL --access-token $ARTIFACTORY_SVC_ACCESS_TOKEN
      sw-warp-pypi-local/warp/$PARENT_COMMIT_REF_NAME/$VERSION/*-manylinux_2_34_x86_64.whl
      'arch=x86_64;os=linux'
    # Set additional common properties on all artifacts
    - >
      jf rt sp --fail-no-op --url=$ARTIFACTORY_BASE_URL --access-token $ARTIFACTORY_SVC_ACCESS_TOKEN
      sw-warp-pypi-local/warp/$PARENT_COMMIT_REF_NAME/$VERSION/*.whl
      version=$VERSION
    - >
      jf rt sp --fail-no-op --url=$ARTIFACTORY_BASE_URL --access-token $ARTIFACTORY_SVC_ACCESS_TOKEN
      sw-warp-pypi-local/warp/$PARENT_COMMIT_REF_NAME/$VERSION/*.whl
      branch=$PARENT_COMMIT_REF_NAME
