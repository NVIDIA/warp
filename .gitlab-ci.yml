# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# ==============================================================================
# CI/CD Pipeline Configuration
# ==============================================================================

include:
  - /.gitlab/ci/common.yml
  - project: "omniverse/warp-gitlab-ci"
    ref: "main"
    file: "/.gitlab-ci.yml"

workflow:
  rules:
    - if: $CI_PROJECT_ROOT_NAMESPACE != "omniverse" # Prevent pipelines that can't access the runners
      when: never
    - if: $CI_PIPELINE_SOURCE == 'merge_request_event'
    - if: $CI_MERGE_REQUEST_TARGET_BRANCH_SHA
    - if: $CI_PIPELINE_SOURCE == "schedule" || $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
      auto_cancel:
        on_new_commit: none
        on_job_failure: none
    - if: $CI_COMMIT_TAG # Run for tagged releases
    - if: $CI_COMMIT_BRANCH =~ /^release-.*/
    - if: $CI_PIPELINE_SOURCE == "web" # Run if triggered from the UI

variables:
  PM_PACKAGES_ROOT: "$CI_PROJECT_DIR/packman-repo"
  PIP_CACHE_DIR: "$CI_PROJECT_DIR/.cache/pip"
  WARP_CACHE_ROOT: "$CI_PROJECT_DIR/.cache/warp" # Used by the parallel test runner
  GIT_DEPTH: 1
  UV_LINK_MODE: copy
  UV_HTTP_TIMEOUT: 120
  UV_PYTHON_PREFERENCE: only-managed
  
  # Standard container images (pinned with SHA256 for supply chain security)
  UV_TRIXIE_IMAGE: "ghcr.io/astral-sh/uv:0.10.0-trixie@sha256:b6fa78dd57c8a1a6c5da74a36a13e309f67f0e925bc89bbd16d6f0628475e052"
  UV_TRIXIE_SLIM_IMAGE: "ghcr.io/astral-sh/uv:0.10.0-trixie-slim@sha256:ea4142f234c115bda9927e49c63efc8a5a7c1db35b2355dc8bc45080fa299bf5"
  UV_TRIXIE_SLIM_ARM_IMAGE: "ghcr.io/astral-sh/uv:0.10.0-trixie-slim@sha256:12c27f31bbaec60e4e98900bcad6bc775a3dc182ef1a79bc1a3ed6248d3f2fb5"
  MINIFORGE_IMAGE: "condaforge/miniforge3:25.11.0-0@sha256:d2fadb08950045962fd1cfb1eeb6224442027f03095862e725f4c1c7b17a3caf"
  WARP_BUILDER_IMAGE_CU12: "${CI_REGISTRY_IMAGE}/warp-builder:cuda12.9.1-llvm21-latest"
  WARP_BUILDER_IMAGE_CU13: "${CI_REGISTRY_IMAGE}/warp-builder:cuda13.0.2-llvm21-latest"
  
  UV_PYTHON:
    value: "3.12"
    options:
      - "3.14"
      - "3.13"
      - "3.12"
      - "3.11"
      - "3.10"
      - "3.9"
    description: "The default UV Python version used in the main testing jobs."

stages:
  - build
  - quality
  - test
  - child pipelines
  - package
  - deploy

# ==============================================================================
# Build Jobs
#
# Compiles Warp binaries for all platforms (Linux, Windows, macOS) using CUDA 12
# by default. Also includes CUDA 13 build for compatibility testing.
# ==============================================================================

linux-aarch64 build:
  stage: build
  image: $WARP_BUILDER_IMAGE_CU12
  extends:
    - .lnx-aarch64-cpu-medium
    - .save_warp_bin_artifact
  before_script:
    - gcc --version
  script:
    - uv run build_lib.py --cuda-path=/usr/local/cuda
    - mkdir -p warp/bin/linux-aarch64
    - mv warp/bin/warp.so warp/bin/linux-aarch64
    - mv warp/bin/warp-clang.so warp/bin/linux-aarch64

linux-x86_64 build:
  stage: build
  image: $WARP_BUILDER_IMAGE_CU12
  extends:
    - .save_warp_bin_artifact
    - .ipp_lnx_x86_64_cpu_medium
  before_script:
    - gcc --version
  script:
    - uv run build_lib.py --cuda-path=/usr/local/cuda
    - mkdir -p warp/bin/linux-x86_64
    - mv warp/bin/warp.so warp/bin/linux-x86_64
    - mv warp/bin/warp-clang.so warp/bin/linux-x86_64

windows-x86_64 build:
  stage: build
  extends:
    - .save_warp_bin_artifact
    - .runner-build-windows-x86_64
  before_script:
    - !reference [.snippets, install-uv-windows]
    - !reference [.snippets, setup-packman-config-windows]
    - tools\packman\packman pull --platform windows-x86_64 deps\cuda-toolkit-deps.packman.xml --verbose --include-tag "cuda-12"
    - tools\packman\packman install -l _build\host-deps\winsdk winsdk 10.17763
    - tools\packman\packman install -l _build\host-deps\msvc msvc 2019-16.11.24
  script:
    - |
      $env:PATH = "$env:CI_PROJECT_DIR\_uv;$env:PATH"
      uv run build_lib.py --msvc-path=_build\host-deps\msvc\VC\Tools\MSVC\14.29.30133 --sdk-path=_build\host-deps\winsdk --cuda-path=_build\target-deps\cuda

mac-aarch64 build:
  stage: build
  extends:
    - .save_warp_bin_artifact
    - .runner-build-macos-universal
    - .macos_warp_tags
  before_script:
    - df -h
    - curl -LsSf https://astral.sh/uv/install.sh | env UV_UNMANAGED_INSTALL="$CI_PROJECT_DIR/_uv" sh
    - export PATH="$CI_PROJECT_DIR/_uv/bin:$PATH"
  script:
    - uv run build_lib.py

linux-x86_64 cuda 13 build:
  stage: build
  image: $WARP_BUILDER_IMAGE_CU13
  extends:
    - .save_warp_bin_artifact
    - .ipp_lnx_x86_64_cpu_medium
  script:
    - uv run build_lib.py --cuda-path=/usr/local/cuda
    - mkdir -p warp/bin/linux-x86_64
    - mv warp/bin/warp.so warp/bin/linux-x86_64
    - mv warp/bin/warp-clang.so warp/bin/linux-x86_64

# ==============================================================================
# Code Quality and Documentation Checks
#
# Linting, static analysis, documentation testing, and symbol validation.
# Most jobs run independently without waiting for builds (except those that
# need compiled binaries for testing or validation).
# ==============================================================================

pre-commit checks:
  stage: quality
  image: $UV_TRIXIE_IMAGE
  needs: []
  extends:
    - .ipp_lnx_x86_64_cpu_micro
  script:
    - uvx pre-commit run --all-files --show-diff-on-failure

pyright stubs:
  stage: quality
  image: $UV_TRIXIE_IMAGE
  needs: []
  timeout: 5m
  variables:
    GIT_LFS_SKIP_SMUDGE: "1"
    GIT_DEPTH: "1"
  extends:
    - .ipp_lnx_x86_64_cpu_micro
  script:
    - uvx pyright warp/__init__.pyi

# This job tests code snippets in the documentation
doctest:
  stage: quality
  image: $UV_TRIXIE_IMAGE
  needs: [linux-x86_64 build]
  extends:
    - .ipp_lnx_x86_64_gpu
  before_script:
    - df -h
    - !reference [.snippets, move-linux-x86_64-binaries]
  script:
    - uv run --extra docs build_docs.py --doctest --no-html

# The only purpose of this job is to make sure documentation can be built on Windows.
# The output does not get published anywhere, but the website can be viewed in the
# artifacts.
windows-x86_64 docs:
  stage: quality
  needs: [windows-x86_64 build]
  extends:
    - .runner-utility-windows-x86_64
  artifacts:
    paths:
      - public
  before_script:
    - !reference [.snippets, define-powershell-GetTime]
    - !reference [.snippets, powershell-section-start-install-deps]
    - !reference [.snippets, install-uv-windows]
    - !reference [.snippets, powershell-section-end-install-deps]
  script:
    - |
      $env:PATH = "$env:CI_PROJECT_DIR\_uv;$env:PATH"
      uv run --extra docs build_docs.py
    - mv docs/_build/html/ ./public/
  after_script:
    - echo "View the website at https://$CI_PROJECT_ROOT_NAMESPACE.$CI_PAGES_DOMAIN/-/$CI_PROJECT_NAME/-/jobs/$CI_JOB_ID/artifacts/public/index.html"

check un-namespaced symbols linux-x86_64:
  stage: quality
  image: python:3.12-bullseye
  needs: [linux-x86_64 build]
  extends:
    - .ipp_lnx_x86_64_cpu_micro
  script:
    - |
      echo "Checking for un-namespaced symbols..."
      LIBRARIES_TO_CHECK="warp/bin/linux-x86_64/warp.so warp/bin/linux-x86_64/warp-clang.so"
      found_symbols=""

      for lib in $LIBRARIES_TO_CHECK; do
        echo "--> Checking $lib"
        if [ ! -f "$lib" ]; then
          echo "ERROR: $lib not found"
          found_symbols="true"
          continue
        fi
        output=$(readelf -Ws "$lib" | awk '$7 ~ /^[0-9]+$/ && $8 !~ /^_?wp_/ {print $8}' | grep -Ev '^(_Z|__|\.)' || true)
        
        if [ -n "$output" ]; then
          echo "ERROR: Found un-namespaced symbols in $lib:"
          echo "$output"
          found_symbols="true"
        fi
      done

      if [ -n "$found_symbols" ]; then
        echo "FAIL: Un-namespaced symbols detected. Please prefix them with 'wp_' or '_wp_'."
        exit 1
      fi

      echo "SUCCESS: All symbols are correctly namespaced."

cppcheck:
  stage: quality
  image: $MINIFORGE_IMAGE
  needs: []
  extends:
    - .ipp_lnx_x86_64_cpu_medium
  before_script:
    - echo -e "\\e[0Ksection_start:`date +%s`:install_dependencies[collapsed=true]\\r\\e[0KInstalling dependencies"
    - df -h
    - conda install cppcheck --channel conda-forge
    - python -m pip install -U cppcheck-codequality
    - mkdir -p _build/.cppcheck
    - cppcheck --version
    - echo -e "\\e[0Ksection_end:`date +%s`:install_dependencies\\r\\e[0K"
  script:
    # Pass 1: For the CI log and job failure.
    - cppcheck_exit_code=0
    - >-
      cppcheck
      --inline-suppr
      --cppcheck-build-dir=_build/.cppcheck
      --enable=warning
      --quiet
      --error-exitcode=1
      -j $(nproc)
      --suppress=*:warp/native/nanovdb/*
      --suppress=normalCheckLevelMaxBranches
      warp
      || cppcheck_exit_code=$?
    # Pass 2: For the Code Quality report.
    - >-
      cppcheck
      --inline-suppr
      --cppcheck-build-dir=_build/.cppcheck
      --enable=warning
      --quiet
      --xml
      --xml-version=2
      -j $(nproc)
      --suppress=*:warp/native/nanovdb/*
      --suppress=normalCheckLevelMaxBranches
      warp
      2> cppcheck-report.xml
    - cppcheck-codequality --input-file=cppcheck-report.xml --output-file=gl-code-quality-report.json
    - exit $cppcheck_exit_code
  artifacts:
    when: always
    reports:
      codequality: gl-code-quality-report.json
    paths:
      - cppcheck-report.xml

# ==============================================================================
# Unit Testing Jobs
#
# Comprehensive test suite across multiple platforms (Linux x86_64/aarch64,
# Windows, specialized hardware like Orin/Thor/Blackwell), Python versions
# (3.9-3.14), and optional integrations (MuJoCo, JAX, PyTorch). Some jobs
# compute code coverage, others focus on compatibility and performance (ASV).
# ==============================================================================

.test_common_with_coverage:
  stage: test
  extends:
    - .basic_test_changes_rules
  coverage: '/(?i)total.*? (100(?:\.0+)?\%|[1-9]?\d(?:\.\d+)?\%)$/'
  artifacts:
    when: always
    paths:
      - rspec.xml
      - coverage.xml
    reports:
      junit: rspec.xml
      coverage_report:
        coverage_format: cobertura
        path: coverage.xml

linux-aarch64 test:
  image: $UV_TRIXIE_SLIM_ARM_IMAGE
  needs: [linux-aarch64 build]
  extends:
    - .test_common_with_coverage
  before_script:
    - df -h
    - !reference [.snippets, move-linux-aarch64-binaries]
  script:
    - uv run --extra dev -m warp.tests --junit-report-xml rspec.xml --coverage --coverage-xml coverage.xml -s autodetect --failfast
  tags:
    - lnx-aarch64-gpu-1x-580.95.05
    - gpu/L40
    - gpu/rtx

linux-aarch64 test orin:
  needs: [linux-aarch64 build]
  image: $UV_TRIXIE_SLIM_ARM_IMAGE
  extends:
    - .save_test_report_artifact
  rules:
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
    - if: $CI_PIPELINE_SOURCE == "schedule"
    - if: $CI_COMMIT_TAG
    - if: $CI_COMMIT_BRANCH =~ /^release-.*/
    - if: $CI_PIPELINE_SOURCE == "web"
    - when: manual # Can be triggered in all other scenarios
      allow_failure: true
  before_script:
    - !reference [.snippets, section-start-install-deps]
    - df -h
    - !reference [.snippets, move-linux-aarch64-binaries]
    - uv sync --extra dev --no-install-package psutil
    - !reference [.snippets, section-end-install-deps]
  script:
    - uv run -m warp.tests --maxjobs 4 --junit-report-xml rspec.xml -s autodetect --failfast
  tags:
    - gpu/orin

linux-aarch64 test thor:
  needs: [linux-aarch64 build]
  image: $UV_TRIXIE_SLIM_ARM_IMAGE
  extends:
    - .save_test_report_artifact
    - .basic_test_changes_rules
  before_script:
    - !reference [.snippets, section-start-install-deps]
    - df -h
    - !reference [.snippets, move-linux-aarch64-binaries]
    - uv sync --extra dev
    - !reference [.snippets, section-end-install-deps]
  script:
    - uv run --extra dev -m warp.tests --junit-report-xml rspec.xml -s autodetect --failfast
  tags:
    - gpu/thor

linux-x86_64 test:
  needs: [linux-x86_64 build]
  image: $UV_TRIXIE_IMAGE
  extends:
    - .ipp_lnx_x86_64_gpu_2x
    - .test_common_with_coverage
  before_script:
    - df -h
    - !reference [.snippets, move-linux-x86_64-binaries]
    # HACK: disable P2P tests due to misbehaving agents
    - export WARP_DISABLE_P2P_TESTS=1
  script:
    - uv run --extra dev --extra torch-cu12 --with "jax[cuda12]" -m warp.tests --junit-report-xml rspec.xml --coverage --coverage-xml coverage.xml -s autodetect --failfast
    # Upload report to Codecov
    - git config --global --add safe.directory $CI_PROJECT_DIR
    - uvx --from codecov-cli codecovcli --verbose upload-process --git-service github --disable-search -t $CODECOV_TOKEN -r NVIDIA/warp --plugin pycoverage -F unittests -f coverage.xml
    - uvx --from codecov-cli codecovcli --verbose upload-process --git-service github --disable-search -t $CODECOV_TOKEN -r NVIDIA/warp --report-type test_results -f rspec.xml

# A temporary job for testing on Blackwell until more runners are available
linux-x86_64-blackwell test:
  needs: [linux-x86_64 build]
  image: $UV_TRIXIE_SLIM_IMAGE
  extends:
    - .test_common_with_coverage
  before_script:
    - df -h
    - !reference [.snippets, move-linux-x86_64-binaries]
  script:
    - uv run --extra dev --extra torch-cu12 --with "jax[cuda12]" -m warp.tests --junit-report-xml rspec.xml --coverage --coverage-xml coverage.xml -s autodetect --failfast
  tags:
    - arch/x86_64
    - gpu/B40
    - os/linux
    - platform/horde

# Note that for throughput reasons this runs on a single-GPU runner
windows-x86_64 test:
  stage: test
  needs: [windows-x86_64 build]
  timeout: 45m
  extends:
    - .runner-test-windows-x86_64-gpu
    - .test_common_with_coverage
  before_script:
    - !reference [.snippets, define-powershell-GetTime]
    - !reference [.snippets, powershell-section-start-install-deps]
    - !reference [.snippets, install-uv-windows]
    - !reference [.snippets, powershell-section-end-install-deps]
  script:
    # Locking usd-core to 25.5.1 to work around a frequent loading crash
    - |
      $env:PATH = "$env:CI_PROJECT_DIR\_uv;$env:PATH"
      uv run --extra dev --extra torch-cu12 --with usd-core==25.5.1 -m warp.tests --junit-report-xml rspec.xml --coverage --coverage-xml coverage.xml -s autodetect --failfast
      # Upload report to Codecov
      git config --global --add safe.directory $env:CI_PROJECT_DIR
      & "$env:CI_PROJECT_DIR\_uv\uvx.exe" --from codecov-cli codecovcli --verbose upload-process --git-service github --disable-search -t $CODECOV_TOKEN -r NVIDIA/warp --plugin pycoverage -F unittests -f coverage.xml
      & "$env:CI_PROJECT_DIR\_uv\uvx.exe" --from codecov-cli codecovcli --verbose upload-process --git-service github --disable-search -t $CODECOV_TOKEN -r NVIDIA/warp --report-type test_results -f rspec.xml

# Optional multi-GPU Windows job (likely longer queue time)
windows-x86_64 test mgpu:
  stage: test
  needs: [windows-x86_64 build]
  extends:
    - .save_test_report_artifact
  timeout: 45m
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
    - if: $CI_COMMIT_TAG
    - when: manual # Can be triggered in all other scenarios
      allow_failure: true
  allow_failure: true # GitLab runners are too unstable to run this job
  before_script:
    - !reference [.snippets, define-powershell-GetTime]
    - !reference [.snippets, powershell-section-start-install-deps]
    - !reference [.snippets, install-uv-windows]
    - !reference [.snippets, powershell-section-end-install-deps]
  script:
    # Locking usd-core to 25.5.1 to work around a frequent loading crash
    - |
      $env:PATH = "$env:CI_PROJECT_DIR\_uv;$env:PATH"
      uv run --extra dev --extra torch-cu12 --with usd-core==25.5.1 -m warp.tests --junit-report-xml rspec.xml -s autodetect --failfast
  tags:
    - win-x86_64-gpu-2x-573.42

# Note: This will no longer be needed once Warp auto-initializes upon import
linux-x86_64 test warp-init:
  stage: test
  image: $UV_TRIXIE_SLIM_IMAGE
  needs: [linux-x86_64 build]
  extends:
    - .ipp_lnx_x86_64_gpu
    - .save_test_report_artifact
    - .basic_test_changes_rules
  timeout: 10m
  before_script:
    - df -h
    - !reference [.snippets, move-linux-x86_64-binaries]
  script:
    - uv run --extra dev -m warp.tests --junit-report-xml rspec.xml -s autodetect --disable-process-pooling --disable-concurrent-futures --level test -p 'test_implicit_init.py'

# Optional job to test MuJoCo Warp
linux-x86_64 test mujoco warp:
  stage: test
  needs: [linux-x86_64 build]
  image: $UV_TRIXIE_IMAGE
  extends:
    - .ipp_lnx_x86_64_gpu
    - .save_test_report_artifact
  variables:
    UV_PYTHON: "3.12"
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule" || $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
    - if: $CI_COMMIT_TAG
    - if: $CI_COMMIT_BRANCH =~ /^release-.*/
    - if: $CI_PIPELINE_SOURCE == "web"
    - if: $CI_PIPELINE_SOURCE == 'merge_request_event'
      changes:
        - .gitlab-ci.yml
        - .gitlab/ci/*
        - warp/**/*
        - pyproject.toml
        - tools/**/*
        - deps/*
        - build_lib.py
        - build_llvm.py
      allow_failure: true # Don't want to block MRs
    - when: manual # If not auto-triggered, allow any pipeline to run this job manually
      allow_failure: true # Don't want to block MRs
  before_script:
    - !reference [.snippets, move-linux-x86_64-binaries]
    # Python environment
    - uv venv
    - uv pip install -e .
    - uv pip install --pre mujoco -f https://py.mujoco.org/
    - uv pip install "git+https://github.com/google-deepmind/mujoco_warp@main"
    - uv pip install pytest pytest-xdist "jax[cuda12]"
    - source .venv/bin/activate
    - echo -e "\\e[0Ksection_end:`date +%s`:install_dependencies\\r\\e[0K"
  script:
    - pytest -n 4 --pyargs mujoco_warp --junit-xml=rspec.xml

# Ensure minimum and maximum Python versions are supported
linux-x86_64 python matrix test:
  stage: test
  image: $UV_TRIXIE_SLIM_IMAGE
  needs: [linux-x86_64 build]
  extends:
    - .ipp_lnx_x86_64_gpu
    - .save_test_report_artifact
    - .basic_test_changes_rules
  parallel:
    matrix:
      - UV_PYTHON: "3.9"
      - UV_PYTHON: "3.10"
      - UV_PYTHON: "3.13"
      - UV_PYTHON: "3.14"
  before_script:
    - !reference [.snippets, section-start-install-deps]
    - df -h
    # Python 3.14 requires a C compiler for some dependencies
    - if [ "$UV_PYTHON" = "3.14" ]; then apt-get update && apt-get install build-essential --no-install-recommends -y; fi
    - !reference [.snippets, move-linux-x86_64-binaries]
    - !reference [.snippets, section-end-install-deps]
  script:
    - uv run --extra dev -m warp.tests --junit-report-xml rspec.xml -s autodetect --failfast

# Benchmark this commit with the current main branch using Airspeed Velocity
# Only runs on merge requests
linux-x86_64 asv:
  stage: test
  needs: []
  image: $WARP_BUILDER_IMAGE_CU12
  extends:
    - .ipp_lnx_x86_64_gpu
  variables:
    GIT_DEPTH: "50"
    CUDA_HOME: "/usr/local/cuda"
  rules:
    - if: $CI_PIPELINE_SOURCE == 'merge_request_event'
      changes:
        - .gitlab-ci.yml
        - asv/**/*
        - asv.conf.json
        - warp/**/*
        - pyproject.toml
        - deps/*
        - build_lib.py
        - build_llvm.py
  before_script:
    - echo -e "\\e[0Ksection_start:`date +%s`:install_dependencies[collapsed=true]\\r\\e[0KInstalling dependencies"
    - df -h
    # Remove /shared/local/bin from PATH - contains musl-linked binaries from the ephemeral
    # runner (v1.5.1+) that are incompatible with this glibc-based container
    - export PATH=$(echo "$PATH" | sed 's|:/shared/local/bin||g; s|/shared/local/bin:||g')
    # Make sure the comparison ref can be checked out by asv
    - COMPARE_REF="${CI_MERGE_REQUEST_TARGET_BRANCH_SHA:-${CI_MERGE_REQUEST_TARGET_BRANCH_NAME:-main}}"
    - 'echo "COMPARE_REF value: ${COMPARE_REF}"'
    - git config --global --add safe.directory $CI_PROJECT_DIR
    - git fetch origin $COMPARE_REF
    - git checkout -f $COMPARE_REF
    - git checkout -f $CI_COMMIT_SHA
    - echo -e "\\e[0Ksection_end:`date +%s`:install_dependencies\\r\\e[0K"
  script:
    - uvx asv machine --yes
    - uvx asv continuous -e --interleave-rounds --append-samples --no-only-changed -f 1.10 --bench '^(?!api)' $COMPARE_REF $CI_COMMIT_SHA 2>&1 || (asv compare --split $COMPARE_REF $CI_COMMIT_SHA 2>&1 && exit 2)
  allow_failure: True

# Ensure cpp examples compile and run
linux-x86_64 cpp examples test:
  stage: test
  image: ${CI_REGISTRY_IMAGE}/warp-cpp-test-env:12.9.1-ubuntu24.04
  needs: [linux-x86_64 build]
  extends:
    - .ipp_lnx_x86_64_gpu
    - .basic_test_changes_rules
  before_script:
    - df -h
    - !reference [.snippets, move-linux-x86_64-binaries]
    - ls -lh warp/bin/*.so  # Verify binaries exist
  script:
    - cd warp/examples/cpp
    - bash test_examples.sh

# ==============================================================================
# Child Pipeline Triggers
#
# Triggers for specialized build/test configurations (debug builds, CUDA 13,
# Kit extensions, Clang compilation). These run conditionally based on schedule,
# tags, or manual trigger. Most developers don't need to worry about these.
# ==============================================================================

# Trigger debug build and test (no code coverage) pipelines
# Workaround from https://gitlab.com/gitlab-org/gitlab/-/issues/284086
trigger debug build and test pipeline:
  stage: test
  image: busybox
  extends:
    - .ipp_lnx_x86_64_cpu_micro
  needs: []
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
    - if: $CI_COMMIT_TAG
    - if: $CI_COMMIT_BRANCH =~ /^release-.*/
    - when: manual # Can be triggered in all other scenarios
      allow_failure: true
  variables:
    GIT_STRATEGY: none
  script:
    - echo "Run this job to test Warp compiled in debug mode."

# Uses the same Python version as the main pipeline.
debug build and test:
  stage: child pipelines
  needs: [trigger debug build and test pipeline]
  trigger:
    include: /.gitlab/ci/debug-build-and-test.yml
  extends:
    - .trigger_common

# Trigger CUDA 13 pipelines
# Workaround from https://gitlab.com/gitlab-org/gitlab/-/issues/284086
trigger cuda 13 pipeline:
  stage: test
  image: busybox
  extends:
    - .ipp_lnx_x86_64_cpu_micro
  needs: [linux-x86_64 cuda 13 build]
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
    - if: $CI_COMMIT_TAG
    - if: $CI_COMMIT_BRANCH =~ /^release-.*/
    - when: manual # Can be triggered in all other scenarios
      allow_failure: true
  variables:
    GIT_STRATEGY: none
  script:
    - echo "Run this job to test Warp compiled with CUDA 13."

# Uses the same Python version as the main pipeline.
cuda 13 build:
  stage: child pipelines
  needs: [trigger cuda 13 pipeline]
  trigger:
    include: /.gitlab/ci/cuda-13-build.yml
  extends:
    - .trigger_common

trigger kit extensions pipeline:
  stage: test
  image: busybox
  needs:
    - linux-aarch64 build
    - linux-x86_64 build
    - windows-x86_64 build
  extends:
    - .ipp_lnx_x86_64_cpu_micro
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
    - if: $CI_COMMIT_TAG
    - if: $CI_COMMIT_BRANCH =~ /^release-.*/
    - changes:
        - deps/kit-deps.packman.xml
        - exts/**/*
    - when: manual # Can be triggered in all other scenarios
      allow_failure: true
  variables:
    GIT_STRATEGY: none
  script:
    - echo "Run this job to test the kit extensions."

# Kit Extensions
kit extensions:
  stage: child pipelines
  needs: [trigger kit extensions pipeline]
  trigger:
    include: /.gitlab/ci/kit-extensions.yml
  extends:
    - .trigger_common

# Test Warp compilation with Clang instead of GCC and NVCC
clang compilation:
  stage: child pipelines
  trigger:
    include: /.gitlab/ci/clang-build-and-test.yml
  needs: []
  extends:
    - .basic_test_changes_rules
    - .trigger_common

# ==============================================================================
# Packaging Jobs
#
# Creates Python wheels for PyPI distribution across all platforms. Produces both
# release wheels (with CUDA variants) and nightly development builds. Typically
# runs on schedules, tags, or manual trigger rather than every merge request.
# ==============================================================================


# Creates wheel files for PyPI
create pypi wheels:
  stage: package
  image: $UV_TRIXIE_SLIM_IMAGE
  needs:
    - linux-aarch64 build
    - linux-x86_64 build
    - windows-x86_64 build
    - mac-aarch64 build
  extends:
    - .ipp_lnx_x86_64_cpu_micro
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
    - if: $CI_COMMIT_TAG
    - if: $CI_COMMIT_BRANCH =~ /^release-.*/
    - if: $CI_PIPELINE_SOURCE == 'merge_request_event'
      changes:
        - pyproject.toml
        - setup.py
    - when: manual # If not auto-triggered, allow any pipeline to run this job manually
      allow_failure: true
  before_script:
    # Move binaries into platform-specific folders. Already done in the build jobs for Linux.
    - !reference [.snippets, move-windows-binaries-to-platform-dir]
    - !reference [.snippets, move-macos-binaries-to-platform-dir]
  script:
    - printf '%s+cu12\n' "$(head -n1 VERSION.md | tr -d '\n\r')" > VERSION.md # Modify VERSION.md with +cu12
    - uv build --wheel -C--build-option=-Pwindows-x86_64
    - uv build --wheel -C--build-option=-Plinux-x86_64
    - uv build --wheel -C--build-option=-Plinux-aarch64 -C--build-option=-Mmanylinux_2_34
    - printf '%s+cpu\n' "$(head -n1 VERSION.md | sed 's/+cu12$//' | tr -d '\n\r')" > VERSION.md # Modify VERSION.md with +cu12 replaced by +cpu
    - uv build --wheel -C--build-option=-Pmacos-aarch64
    - printf '%s\n' "$(head -n1 VERSION.md | sed 's/+cpu$//' | tr -d '\n\r')" > VERSION.md # Revert VERSION.md changes
    - mv dist dist-github
    # Now make the wheels meant for PyPI publishing
    - uv build --wheel -C--build-option=-Pwindows-x86_64
    - uv build --wheel -C--build-option=-Plinux-x86_64
    - uv build --wheel -C--build-option=-Plinux-aarch64 -C--build-option=-Mmanylinux_2_34
    - uv build --wheel -C--build-option=-Pmacos-aarch64
    - find . -type f -exec chmod 664 {} +
    - find . -type d -exec chmod 775 {} +
  artifacts:
    name: $CI_COMMIT_REF_SLUG-$CI_COMMIT_SHORT_SHA
    expose_as: "Python Wheels"
    paths:
      - "dist/"
      - "dist-github/"
    when: always

create dev wheels:
  stage: package
  image: $UV_TRIXIE_SLIM_IMAGE
  needs:
    - linux-aarch64 build
    - linux-x86_64 build
    - windows-x86_64 build
    - mac-aarch64 build
  extends:
    - .ipp_lnx_x86_64_cpu_micro
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule" && $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
  before_script:
    # Move binaries into platform-specific folders. Already done in the build jobs for Linux.
    - !reference [.snippets, move-windows-binaries-to-platform-dir]
    - !reference [.snippets, move-macos-binaries-to-platform-dir]
  script:
    # Version files (VERSION.md, config.py, version.h) are already updated from build artifacts
    - uv build --wheel -C--build-option=-Pwindows-x86_64
    - uv build --wheel -C--build-option=-Plinux-x86_64
    - uv build --wheel -C--build-option=-Plinux-aarch64 -C--build-option=-Mmanylinux_2_34
    - uv build --wheel -C--build-option=-Pmacos-aarch64
    - find . -type f -exec chmod 664 {} +
    - find . -type d -exec chmod 775 {} +
    - cp VERSION.md dist/
  artifacts:
    name: $CI_COMMIT_REF_SLUG-$CI_COMMIT_SHORT_SHA-dev
    paths:
      - "dist/"

# ==============================================================================
# Deployment Jobs
#
# Publishes Python wheels to PyPI (test and production), GitLab registries,
# and GitHub releases. Also builds and deploys documentation to GitLab Pages
# for merge requests and the main branch.
# ==============================================================================

publish wheels to testpypi registry:
  stage: deploy
  image: $UV_TRIXIE_SLIM_IMAGE
  needs: ["create pypi wheels"]
  extends:
    - .ipp_lnx_x86_64_cpu_micro
  rules:
    - if: $CI_COMMIT_BRANCH =~ /release-.*/ || $CI_COMMIT_TAG
      when: manual
      allow_failure: true
  environment:
    name: staging
    url: https://test.pypi.org/project/warp-lang/
  script:
    - uv publish --verbose --index testpypi -t $TESTPYPI_DEPLOY_KEY dist/*

publish wheels to pypi registry:
  stage: deploy
  image: $UV_TRIXIE_SLIM_IMAGE
  needs: ["create pypi wheels"]
  extends:
    - .ipp_lnx_x86_64_cpu_micro
  rules:
    - if: $CI_COMMIT_TAG
      when: manual
      allow_failure: true
  environment:
    name: production
    url: https://pypi.org/project/warp-lang/
  script:
    - uv publish --verbose -t $PYPI_DEPLOY_KEY dist/*

publish wheels to gitlab pypi registry:
  stage: deploy
  image: $UV_TRIXIE_IMAGE
  needs: ["create pypi wheels"]
  extends:
    - .ipp_lnx_x86_64_cpu_micro
  rules:
    - if: $CI_COMMIT_BRANCH =~ /^release-.*/ || $CI_COMMIT_TAG
      when: manual
      allow_failure: true
  script:
    - uv publish --verbose -u gitlab-ci-token -p ${CI_JOB_TOKEN} --publish-url ${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/packages/pypi --check-url ${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/packages/pypi/simple dist-github/*

# Uploads the wheels to the internal GitLab package registry in the Warp project
# Generated files will be in a branch/tag-specific folder
publish wheels to gitlab package registry:
  stage: deploy
  needs: ["create pypi wheels"]
  image: $UV_TRIXIE_IMAGE
  extends:
    - .ipp_lnx_x86_64_cpu_micro
  rules:
    - if: $CI_COMMIT_TAG
    - when: manual # If not auto-triggered, allow any pipeline to run this job manually
      allow_failure: true
  script:
    - |
      # Fail if no wheels found
      wheels=$(find dist -name '*.whl')
      if [ -z "$wheels" ]; then
        echo "ERROR: No wheel files found in dist/"
        exit 1
      fi
      for file in $wheels; do
          filename=$(basename -- "$file")
          curl --header "JOB-TOKEN: $CI_JOB_TOKEN" --upload-file "$file" "${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/packages/generic/warp/${CI_COMMIT_REF_SLUG}/${filename}"
      done
    - echo "See the published files at $CI_PROJECT_URL/-/packages"

publish wheels to github release:
  stage: deploy
  image: $MINIFORGE_IMAGE
  needs: ["create pypi wheels"]
  extends:
    - .ipp_lnx_x86_64_cpu_micro
  rules:
    - if: $CI_COMMIT_TAG
  before_script:
    - conda install gh --channel conda-forge
  script:
    - gh release upload $CI_COMMIT_TAG ./dist-github/*.whl

.build-docs-common:
  stage: deploy
  image: $UV_TRIXIE_IMAGE
  needs: [linux-x86_64 build]
  extends:
    - .ipp_lnx_x86_64_cpu_medium
  artifacts:
    paths:
      - public
  before_script:
    - !reference [.snippets, section-start-install-deps]
    - df -h
    - !reference [.snippets, move-linux-x86_64-binaries]
    - !reference [.snippets, section-end-install-deps]
  script:
    - uv run --extra docs build_docs.py
    - mv docs/_build/html/ ./public/

# Merge requests: Build documentation and save as an artifact
# A link to the generated documentation is added to the merge request.
merge request docs:
  extends:
    - .build-docs-common
  artifacts:
    paths:
      - warp/__init__.pyi
      - docs/api_reference
      - docs/language_reference
      - public
  rules:
    - if: $CI_PIPELINE_SOURCE == 'merge_request_event'
  timeout: 20m
  environment:
    name: review/$CI_MERGE_REQUEST_IID
    url: https://$CI_PROJECT_ROOT_NAMESPACE.$CI_PAGES_DOMAIN/-/$CI_PROJECT_NAME/-/jobs/$CI_JOB_ID/artifacts/public/index.html
  after_script:
    - echo "View the website at https://$CI_PROJECT_ROOT_NAMESPACE.$CI_PAGES_DOMAIN/-/$CI_PROJECT_NAME/-/jobs/$CI_JOB_ID/artifacts/public/index.html"

# Merge requests: Ensure that exports.h, __init__.pyi, and docs reference files have been
# manually added to the MR if they are changed
check generated files:
  stage: deploy
  image: $UV_TRIXIE_IMAGE
  needs:
    - job: linux-x86_64 build
    - job: merge request docs
      optional: true
  extends:
    - .ipp_lnx_x86_64_cpu_micro
  rules:
    - if: $CI_PIPELINE_SOURCE == 'merge_request_event'
  timeout: 10m
  artifacts:
    when: on_failure
    expose_as: "Generated source files"
    paths:
      - warp/native/exports.h
      - warp/native/version.h
      - warp/__init__.pyi
      - docs/api_reference
      - docs/language_reference
  script:
    # Check version consistency across VERSION.md, config.py, and version.h
    - uv run tools/ci/publishing/check_version_consistency.py --verbose
    # Check that generated documentation files are committed
    - >
      git diff --exit-code warp/__init__.pyi docs/api_reference docs/language_reference ||
      (echo "Please run build_docs.py (or download from $CI_JOB_URL/artifacts/browse) and add modified files to your merge request." && false)
    # Check that generated native files are committed
    - >
      git diff --exit-code warp/native/exports.h ||
      (echo "Please run build_lib.py (or download from $CI_JOB_URL/artifacts/browse) and add warp/native/exports.h to your merge request." && false)
    - >
      git diff --exit-code warp/native/version.h ||
      (echo "Please run build_lib.py (or download from $CI_JOB_URL/artifacts/browse) and add warp/native/version.h to your merge request." && false)

# Build documentation and publish on GitLab
# This only runs in the default branch pipeline. The "pages" name is special for GitLab.
pages:
  extends:
    - .build-docs-common
  rules:
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
  timeout: 20m
  environment:
    name: GitLab Pages
    deployment_tier: staging
    url: $CI_PAGES_URL
