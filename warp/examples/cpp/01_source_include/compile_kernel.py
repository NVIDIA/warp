# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Warp Kernel Compilation: Automatic Differentiation Example

This script compiles differentiable Warp kernels to CUDA source code (.cu).
Warp automatically generates both forward and backward kernels for gradient computation.

The generated .cu file is statically included in the C++ program.
(PTX/cubin files are also generated but not used in this example.)
"""

import glob
import os

import warp as wp


@wp.kernel
def compute_loss(
    params: wp.array[float],  # Model parameters [a, b]
    x: wp.array[float],  # Input data
    y_true: wp.array[float],  # Ground truth
    loss: wp.array[float],  # Output: scalar loss
):
    """Compute MSE loss for linear model: y = a*x + b

    This kernel is differentiable - Warp automatically generates the backward pass!

    Forward pass computes:
        y_pred = params[0] * x + params[1]
        loss += (y_pred - y_true)^2

    Backward pass (auto-generated) computes:
        ∂loss/∂params[0] (gradient w.r.t. 'a')
        ∂loss/∂params[1] (gradient w.r.t. 'b')
    """
    tid = wp.tid()

    # Linear model: y = a*x + b
    a = params[0]
    b = params[1]
    y_pred = a * x[tid] + b

    # Compute squared error
    error = y_pred - y_true[tid]
    squared_error = error * error

    # Accumulate to loss
    wp.atomic_add(loss, 0, squared_error)


@wp.kernel
def update_params(learning_rate: float, grads: wp.array[float], params: wp.array[float]):
    """Update parameters using gradient descent: params -= learning_rate * grads

    This kernel updates parameters in-place on the GPU, avoiding CPU round-trips.
    """
    tid = wp.tid()
    params[tid] = params[tid] - learning_rate * grads[tid]


if __name__ == "__main__":
    output_dir = os.path.join(os.path.dirname(__file__), "generated")
    os.makedirs(output_dir, exist_ok=True)

    # Compile the kernel to CUDA source code
    print("Generating Warp differentiable kernel...")
    device = wp.get_device()
    print(f"Target device: {device.name}")
    print(f"Target architecture: sm_{device.get_cuda_compile_arch()}")

    # Compile module: generates .cu source (for C++ inclusion) and .ptx/.cubin (unused)
    # strip_hash=True: Predictable kernel names without hash suffix
    wp.compile_aot_module("__main__", module_dir=output_dir, strip_hash=True)

    # Get kernel names programmatically (after compilation)
    loss_forward = f"{compute_loss.get_mangled_name()}_cuda_kernel_forward"
    loss_backward = f"{compute_loss.get_mangled_name()}_cuda_kernel_backward"
    update_forward = f"{update_params.get_mangled_name()}_cuda_kernel_forward"

    print(f"\nLoss forward kernel:     {loss_forward}")
    print(f"Loss backward kernel:    {loss_backward}")
    print(f"Update params kernel:    {update_forward}")

    # Verify source file was generated
    cu_files = glob.glob(os.path.join(output_dir, "wp___main__.cu"))
    if not cu_files:
        raise RuntimeError(f"No .cu files generated in {output_dir}")

    print(f"\nGenerated source file: {os.path.basename(cu_files[0])}")
    print("✓ Contains forward kernel (compute loss)")
    print("✓ Contains backward kernel (compute gradients) - auto-generated by Warp!")
    print("✓ Contains parameter update kernel")
    print("\nGeneration complete!")
