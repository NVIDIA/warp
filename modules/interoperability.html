

<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Interoperability &#8212; Warp 1.9.1</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/nvidia-sphinx-theme.css?v=df3ac72c" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=8524295a" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=60b3a732"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=fd10adb8"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'modules/interoperability';</script>
    <link rel="icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Configuration" href="../configuration.html" />
    <link rel="prev" title="Tiles" href="tiles.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="1.9.1" />


  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/nvidia-logo-horiz-rgb-blk-for-screen.svg" class="logo__image only-light" alt="Warp 1.9.1 - Home"/>
    <img src="../_static/nvidia-logo-horiz-rgb-wht-for-screen.svg" class="logo__image only-dark pst-js-only" alt="Warp 1.9.1 - Home"/>
  
  
    <p class="title logo__title">Warp 1.9.1</p>
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/NVIDIA/warp" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-square-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/warp-lang" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        



  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/nvidia-logo-horiz-rgb-blk-for-screen.svg" class="logo__image only-light" alt="Warp 1.9.1 - Home"/>
    <img src="../_static/nvidia-logo-horiz-rgb-wht-for-screen.svg" class="logo__image only-dark pst-js-only" alt="Warp 1.9.1 - Home"/>
  
  
    <p class="title logo__title">Warp 1.9.1</p>
  
</a>


  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/NVIDIA/warp" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-square-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/warp-lang" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">



<nav class="bd-docs-nav bd-links"
     aria-label="Table of Contents">
  <p class="bd-links__title" role="heading" aria-level="1">Table of Contents</p>
  <div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">User's Guide</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basics.html">Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="devices.html">Devices</a></li>
<li class="toctree-l1"><a class="reference internal" href="differentiability.html">Differentiability</a></li>
<li class="toctree-l1"><a class="reference internal" href="generics.html">Generics</a></li>
<li class="toctree-l1"><a class="reference internal" href="tiles.html">Tiles</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Interoperability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debugging.html">Debugging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../limitations.html">Limitations</a></li>
<li class="toctree-l1"><a class="reference internal" href="contribution_guide.html">Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../publications.html">Publications using Warp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changelog.html">Changelog</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Topics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../codegen.html">Code Generation</a></li>
<li class="toctree-l1"><a class="reference internal" href="allocators.html">Allocators</a></li>
<li class="toctree-l1"><a class="reference internal" href="concurrency.html">Concurrency</a></li>
<li class="toctree-l1"><a class="reference internal" href="../profiling.html">Profiling</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Core Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="runtime.html">Python Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="functions.html">Built-Ins Reference</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Simulation Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="sim.html">warp.sim</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse.html">warp.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="fem.html">warp.fem</a></li>
<li class="toctree-l1"><a class="reference internal" href="render.html">warp.render</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Project Links</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://github.com/NVIDIA/warp">GitHub</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pypi.org/project/warp-lang">PyPI</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>



      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">Interoperability</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="interoperability">
<h1>Interoperability<a class="headerlink" href="#interoperability" title="Link to this heading">#</a></h1>
<p>Warp can interoperate with other Python-based frameworks such as NumPy through standard interface protocols.</p>
<p>Warp supports passing external arrays to kernels directly, as long as they implement the <code class="docutils literal notranslate"><span class="pre">__array__</span></code>, <code class="docutils literal notranslate"><span class="pre">__array_interface__</span></code>, or <code class="docutils literal notranslate"><span class="pre">__cuda_array_interface__</span></code> protocols.  This works with many common frameworks like NumPy, CuPy, or PyTorch.</p>
<p>For example, we can use NumPy arrays directly when launching Warp kernels on the CPU:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warp</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">wp</span>

<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">saxpy</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span> <span class="n">y</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span> <span class="n">a</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">saxpy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Likewise, we can use CuPy arrays on a CUDA device:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">cupy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">cp</span>

<span class="k">with</span> <span class="n">cp</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Device</span><span class="p">(</span><span class="mi">0</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">cp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">cp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">saxpy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that with CUDA arrays, it’s important to ensure that the device on which the arrays reside is the same as the device on which the kernel is launched.</p>
<p>PyTorch supports both CPU and GPU tensors and both kinds can be passed to Warp kernels on the appropriate device.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">random</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">]):</span>
    <span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda:0&quot;</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">saxpy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<section id="numpy">
<h2>NumPy<a class="headerlink" href="#numpy" title="Link to this heading">#</a></h2>
<p>Warp arrays may be converted to a NumPy array through the <a class="reference internal" href="runtime.html#warp.array.numpy" title="warp.array.numpy"><code class="xref py py-meth docutils literal notranslate"><span class="pre">array.numpy()</span></code></a> method. When the Warp array lives on
the <code class="docutils literal notranslate"><span class="pre">cpu</span></code> device this will return a zero-copy view onto the underlying Warp allocation. If the array lives on a
<code class="docutils literal notranslate"><span class="pre">cuda</span></code> device then it will first be copied back to a temporary buffer and copied to NumPy.</p>
<p>Warp CPU arrays also implement  the <code class="docutils literal notranslate"><span class="pre">__array_interface__</span></code> protocol and so can be used to construct NumPy arrays
directly:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="p">[</span><span class="mf">1.</span> <span class="mf">2.</span> <span class="mf">3.</span><span class="p">]</span>
</pre></div>
</div>
<p>Data type conversion utilities are also available for convenience:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">warp_type</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">float32</span>
<span class="o">...</span>
<span class="n">numpy_type</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">dtype_to_numpy</span><span class="p">(</span><span class="n">warp_type</span><span class="p">)</span>
<span class="o">...</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">warp_type</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">numpy_type</span><span class="p">)</span>
</pre></div>
</div>
<p>To create Warp arrays from NumPy arrays, use <a class="reference internal" href="#warp.from_numpy" title="warp.from_numpy"><code class="xref py py-func docutils literal notranslate"><span class="pre">warp.from_numpy()</span></code></a>
or pass the NumPy array as the <code class="docutils literal notranslate"><span class="pre">data</span></code> argument of the <a class="reference internal" href="runtime.html#warp.array" title="warp.array"><code class="xref py py-class docutils literal notranslate"><span class="pre">warp.array</span></code></a> constructor directly.</p>
<dl class="py function">
<dt class="sig sig-object py" id="warp.from_numpy">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">from_numpy</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">arr</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/context.py#L5659-L5698"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.from_numpy" title="Link to this definition">#</a></dt>
<dd><p>Returns a Warp array created from a NumPy array.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>arr</strong> (<a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v2.3)"><em>ndarray</em></a>) – The NumPy array providing the data to construct the Warp array.</p></li>
<li><p><strong>dtype</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.13)"><em>type</em></a><em> | </em><em>None</em>) – The data type of the new Warp array. If this is not provided, the data type will be inferred.</p></li>
<li><p><strong>shape</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Sequence" title="(in Python v3.13)"><em>Sequence</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>] </em><em>| </em><em>None</em>) – The shape of the Warp array.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="devices.html#warp.context.Device" title="warp.context.Device"><em>Device</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> | </em><em>None</em>) – The device on which the Warp array will be constructed.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference internal" href="functions.html#warp.bool" title="warp.bool"><em>bool</em></a>) – Whether gradients will be tracked for this array.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#RuntimeError" title="(in Python v3.13)"><strong>RuntimeError</strong></a> – The data type of the NumPy array is not supported.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="runtime.html#warp.array" title="warp.types.array"><em>array</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.dtype_from_numpy">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">dtype_from_numpy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">numpy_dtype</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/types.py#L1529-L1536"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.dtype_from_numpy" title="Link to this definition">#</a></dt>
<dd><p>Return the Warp dtype corresponding to a NumPy dtype.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.dtype_to_numpy">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">dtype_to_numpy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">warp_dtype</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/types.py#L1538-L1545"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.dtype_to_numpy" title="Link to this definition">#</a></dt>
<dd><p>Return the NumPy dtype corresponding to a Warp dtype.</p>
</dd></dl>

</section>
<section id="pytorch">
<span id="pytorch-interop"></span><h2>PyTorch<a class="headerlink" href="#pytorch" title="Link to this heading">#</a></h2>
<p>Warp provides helper functions to convert arrays to/from PyTorch:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

<span class="c1"># convert to Torch tensor</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">to_torch</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>

<span class="c1"># convert from Torch tensor</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</pre></div>
</div>
<p>These helper functions allow the conversion of Warp arrays to/from PyTorch tensors without copying the underlying data.
At the same time, if available, gradient arrays and tensors are converted to/from PyTorch autograd tensors, allowing the use of Warp arrays
in PyTorch autograd computations.</p>
<dl class="py function">
<dt class="sig sig-object py" id="warp.from_torch">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">from_torch</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">t</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">return_ctype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/torch.py#L194-L307"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.from_torch" title="Link to this definition">#</a></dt>
<dd><p>Convert a Torch tensor to a Warp array without copying the data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>t</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.8)"><em>torch.Tensor</em></a>) – The torch tensor to wrap.</p></li>
<li><p><strong>dtype</strong> (<em>warp.dtype</em><em>, </em><em>optional</em>) – The target data type of the resulting Warp array. Defaults to the tensor value type mapped to a Warp array value type.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference internal" href="functions.html#warp.bool" title="warp.bool"><em>bool</em></a><em>, </em><em>optional</em>) – Whether the resulting array should wrap the tensor’s gradient, if it exists (the grad tensor will be allocated otherwise). Defaults to the tensor’s <cite>requires_grad</cite> value.</p></li>
<li><p><strong>return_ctype</strong> (<a class="reference internal" href="functions.html#warp.bool" title="warp.bool"><em>bool</em></a><em>, </em><em>optional</em>) – Whether to return a low-level array descriptor instead of a <code class="docutils literal notranslate"><span class="pre">wp.array</span></code> object (faster).  The descriptor can be passed to Warp kernels.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The wrapped array or array descriptor.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="runtime.html#warp.array" title="warp.array">warp.array</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.to_torch">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">to_torch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">a</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/torch.py#L309-L352"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.to_torch" title="Link to this definition">#</a></dt>
<dd><p>Convert a Warp array to a Torch tensor without copying the data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>a</strong> (<a class="reference internal" href="runtime.html#warp.array" title="warp.array"><em>warp.array</em></a>) – The Warp array to convert.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference internal" href="functions.html#warp.bool" title="warp.bool"><em>bool</em></a><em>, </em><em>optional</em>) – Whether the resulting tensor should convert the array’s gradient, if it exists, to a grad tensor. Defaults to the array’s <cite>requires_grad</cite> value.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The converted tensor.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.8)">torch.Tensor</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.device_from_torch">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">device_from_torch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">torch_device</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/torch.py#L25-L56"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.device_from_torch" title="Link to this definition">#</a></dt>
<dd><p>Return the Warp device corresponding to a Torch device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>torch_device</strong> (<cite>torch.device</cite> or <cite>str</cite>) – Torch device identifier</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#RuntimeError" title="(in Python v3.13)"><strong>RuntimeError</strong></a> – Torch device does not have a corresponding Warp device</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="devices.html#warp.context.Device" title="warp.context.Device"><em>Device</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.device_to_torch">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">device_to_torch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">warp_device</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/torch.py#L58-L74"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.device_to_torch" title="Link to this definition">#</a></dt>
<dd><p>Return the Torch device string corresponding to a Warp device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>warp_device</strong> (<a class="reference internal" href="devices.html#warp.context.Device" title="warp.context.Device"><em>Device</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> | </em><em>None</em>) – An identifier that can be resolved to a <a class="reference internal" href="devices.html#warp.context.Device" title="warp.context.Device"><code class="xref py py-class docutils literal notranslate"><span class="pre">warp.context.Device</span></code></a>.</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#RuntimeError" title="(in Python v3.13)"><strong>RuntimeError</strong></a> – The Warp device is not compatible with PyTorch.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)">str</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.dtype_from_torch">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">dtype_from_torch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">torch_dtype</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/torch.py#L113-L150"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.dtype_from_torch" title="Link to this definition">#</a></dt>
<dd><p>Return the Warp dtype corresponding to a Torch dtype.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>torch_dtype</strong> – A <code class="docutils literal notranslate"><span class="pre">torch.dtype</span></code> that has a corresponding Warp data type.
Currently <code class="docutils literal notranslate"><span class="pre">torch.bfloat16</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.complex64</span></code>, and
<code class="docutils literal notranslate"><span class="pre">torch.complex128</span></code> are not supported.</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.13)"><strong>TypeError</strong></a> – Unable to find a corresponding Warp data type.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.dtype_to_torch">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">dtype_to_torch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">warp_dtype</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/torch.py#L76-L111"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.dtype_to_torch" title="Link to this definition">#</a></dt>
<dd><p>Return the Torch dtype corresponding to a Warp dtype.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>warp_dtype</strong> – A Warp data type that has a corresponding <code class="docutils literal notranslate"><span class="pre">torch.dtype</span></code>.
<code class="docutils literal notranslate"><span class="pre">warp.uint16</span></code>, <code class="docutils literal notranslate"><span class="pre">warp.uint32</span></code>, and <code class="docutils literal notranslate"><span class="pre">warp.uint64</span></code> are mapped
to the signed integer <code class="docutils literal notranslate"><span class="pre">torch.dtype</span></code> of the same width.</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.13)"><strong>TypeError</strong></a> – Unable to find a corresponding PyTorch data type.</p>
</dd>
</dl>
</dd></dl>

<p>To convert a PyTorch CUDA stream to a Warp CUDA stream and vice versa, Warp provides the following functions:</p>
<dl class="py function">
<dt class="sig sig-object py" id="warp.stream_from_torch">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">stream_from_torch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stream_or_device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/torch.py#L354-L372"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.stream_from_torch" title="Link to this definition">#</a></dt>
<dd><p>Convert from a Torch CUDA stream to a Warp CUDA stream.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.stream_to_torch">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">stream_to_torch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stream_or_device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/torch.py#L374-L392"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.stream_to_torch" title="Link to this definition">#</a></dt>
<dd><p>Convert from a Warp CUDA stream to a Torch CUDA stream.</p>
</dd></dl>

<section id="example-optimization-using-warp-from-torch">
<h3>Example: Optimization using <code class="docutils literal notranslate"><span class="pre">warp.from_torch()</span></code><a class="headerlink" href="#example-optimization-using-warp-from-torch" title="Link to this heading">#</a></h3>
<p>An example usage of minimizing a loss function over an array of 2D points written in Warp via PyTorch’s Adam optimizer
using <a class="reference internal" href="#warp.from_torch" title="warp.from_torch"><code class="xref py py-func docutils literal notranslate"><span class="pre">warp.from_torch()</span></code></a> is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">warp</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">wp</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>


<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span><span class="p">()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">loss</span><span class="p">(</span><span class="n">xs</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">ndim</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">l</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">atomic_add</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">xs</span><span class="p">[</span><span class="n">tid</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">**</span> <span class="mf">2.0</span> <span class="o">+</span> <span class="n">xs</span><span class="p">[</span><span class="n">tid</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="mf">2.0</span><span class="p">)</span>

<span class="c1"># indicate requires_grad so that Warp can accumulate gradients in the grad buffers</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">l</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">xs</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">wp_xs</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
<span class="n">wp_l</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>

<span class="n">tape</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">Tape</span><span class="p">()</span>
<span class="k">with</span> <span class="n">tape</span><span class="p">:</span>
    <span class="c1"># record the loss function kernel launch on the tape</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">wp_xs</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">wp_l</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">wp_xs</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="n">tape</span><span class="o">.</span><span class="n">zero</span><span class="p">()</span>
    <span class="n">tape</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">wp_l</span><span class="p">)</span>  <span class="c1"># compute gradients</span>
    <span class="c1"># now xs.grad will be populated with the gradients computed by Warp</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># update xs (and thereby wp_xs)</span>

    <span class="c1"># these lines are only needed for evaluating the loss</span>
    <span class="c1"># (the optimization just needs the gradient, not the loss value)</span>
    <span class="n">wp_l</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">wp_xs</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">wp_l</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">wp_xs</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="se">\t</span><span class="s2">loss: </span><span class="si">{</span><span class="n">l</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="example-optimization-using-warp-to-torch">
<h3>Example: Optimization using <code class="docutils literal notranslate"><span class="pre">warp.to_torch</span></code><a class="headerlink" href="#example-optimization-using-warp-to-torch" title="Link to this heading">#</a></h3>
<p>Less code is needed when we declare the optimization variables directly in Warp and use <a class="reference internal" href="#warp.to_torch" title="warp.to_torch"><code class="xref py py-func docutils literal notranslate"><span class="pre">warp.to_torch()</span></code></a> to convert them to PyTorch tensors.
Here, we revisit the same example from above where now only a single conversion to a PyTorch tensor is needed to supply Adam with the optimization variables:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">warp</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">wp</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>


<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span><span class="p">()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">loss</span><span class="p">(</span><span class="n">xs</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">ndim</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">l</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">atomic_add</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">xs</span><span class="p">[</span><span class="n">tid</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">**</span> <span class="mf">2.0</span> <span class="o">+</span> <span class="n">xs</span><span class="p">[</span><span class="n">tid</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="mf">2.0</span><span class="p">)</span>

<span class="c1"># initialize the optimization variables in Warp</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">l</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># just a single wp.to_torch call is needed, Adam optimizes using the Warp array gradients</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">wp</span><span class="o">.</span><span class="n">to_torch</span><span class="p">(</span><span class="n">xs</span><span class="p">)],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">tape</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">Tape</span><span class="p">()</span>
<span class="k">with</span> <span class="n">tape</span><span class="p">:</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">xs</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">xs</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="n">tape</span><span class="o">.</span><span class="n">zero</span><span class="p">()</span>
    <span class="n">tape</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">l</span><span class="p">)</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="n">l</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">xs</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">xs</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="se">\t</span><span class="s2">loss: </span><span class="si">{</span><span class="n">l</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="example-optimization-using-torch-autograd-function-pytorch-2-3-1">
<h3>Example: Optimization using <code class="docutils literal notranslate"><span class="pre">torch.autograd.function</span></code> (PyTorch &lt;= 2.3.1)<a class="headerlink" href="#example-optimization-using-torch-autograd-function-pytorch-2-3-1" title="Link to this heading">#</a></h3>
<p>One can insert Warp kernel launches in a PyTorch graph by defining a <a class="reference external" href="https://docs.pytorch.org/docs/stable/autograd.html#torch.autograd.Function" title="(in PyTorch v2.8)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code></a> class, which
requires forward and backward functions to be defined. After mapping incoming PyTorch tensors to Warp arrays, a Warp kernel
may be launched in the usual way. In the backward pass, the same kernel’s adjoint may be launched by
setting <code class="docutils literal notranslate"><span class="pre">adjoint</span> <span class="pre">=</span> <span class="pre">True</span></code> in <a class="reference internal" href="runtime.html#warp.launch" title="warp.launch"><code class="xref py py-func docutils literal notranslate"><span class="pre">wp.launch()</span></code></a>. Alternatively, the user may choose to rely on Warp’s tape.
In the following example, we demonstrate how Warp may be used to evaluate the Rosenbrock function in an optimization context.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">warp</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">wp</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># Define the Rosenbrock function</span>
<span class="nd">@wp</span><span class="o">.</span><span class="n">func</span>
<span class="k">def</span><span class="w"> </span><span class="nf">rosenbrock</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span> <span class="o">**</span> <span class="mf">2.0</span> <span class="o">+</span> <span class="mf">100.0</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">x</span><span class="o">**</span><span class="mf">2.0</span><span class="p">)</span> <span class="o">**</span> <span class="mf">2.0</span>

<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">eval_rosenbrock</span><span class="p">(</span>
    <span class="n">xs</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">vec2</span><span class="p">),</span>
    <span class="c1"># outputs</span>
    <span class="n">z</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
<span class="p">):</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">xs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">rosenbrock</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>


<span class="k">class</span><span class="w"> </span><span class="nc">Rosenbrock</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">xy</span><span class="p">,</span> <span class="n">num_points</span><span class="p">):</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">xy</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">xy</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">vec2</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">num_points</span> <span class="o">=</span> <span class="n">num_points</span>

        <span class="c1"># allocate output</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_points</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span>
            <span class="n">kernel</span><span class="o">=</span><span class="n">eval_rosenbrock</span><span class="p">,</span>
            <span class="n">dim</span><span class="o">=</span><span class="n">ctx</span><span class="o">.</span><span class="n">num_points</span><span class="p">,</span>
            <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">ctx</span><span class="o">.</span><span class="n">xy</span><span class="p">],</span>
            <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">ctx</span><span class="o">.</span><span class="n">z</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">wp</span><span class="o">.</span><span class="n">to_torch</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">z</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">adj_z</span><span class="p">):</span>
        <span class="c1"># map incoming Torch grads to our output variables</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">z</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">adj_z</span><span class="p">)</span>

        <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span>
            <span class="n">kernel</span><span class="o">=</span><span class="n">eval_rosenbrock</span><span class="p">,</span>
            <span class="n">dim</span><span class="o">=</span><span class="n">ctx</span><span class="o">.</span><span class="n">num_points</span><span class="p">,</span>
            <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">ctx</span><span class="o">.</span><span class="n">xy</span><span class="p">],</span>
            <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">ctx</span><span class="o">.</span><span class="n">z</span><span class="p">],</span>
            <span class="n">adj_inputs</span><span class="o">=</span><span class="p">[</span><span class="n">ctx</span><span class="o">.</span><span class="n">xy</span><span class="o">.</span><span class="n">grad</span><span class="p">],</span>
            <span class="n">adj_outputs</span><span class="o">=</span><span class="p">[</span><span class="n">ctx</span><span class="o">.</span><span class="n">z</span><span class="o">.</span><span class="n">grad</span><span class="p">],</span>
            <span class="n">adjoint</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

        <span class="c1"># return adjoint w.r.t. inputs</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">wp</span><span class="o">.</span><span class="n">to_torch</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">xy</span><span class="o">.</span><span class="n">grad</span><span class="p">),</span> <span class="kc">None</span><span class="p">)</span>


<span class="n">num_points</span> <span class="o">=</span> <span class="mi">1500</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">5e-2</span>

<span class="n">torch_device</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">device_to_torch</span><span class="p">(</span><span class="n">wp</span><span class="o">.</span><span class="n">get_device</span><span class="p">())</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">xy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_points</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch_device</span><span class="p">)</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">xy</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
    <span class="c1"># step</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">Rosenbrock</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">xy</span><span class="p">,</span> <span class="n">num_points</span><span class="p">)</span>
    <span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>

    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="c1"># minimum at (1, 1)</span>
<span class="n">xy_np</span> <span class="o">=</span> <span class="n">xy</span><span class="o">.</span><span class="n">numpy</span><span class="p">(</span><span class="n">force</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">xy_np</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
</pre></div>
</div>
<p>Note that if Warp code is wrapped in a <a class="reference external" href="https://docs.pytorch.org/docs/stable/autograd.html#torch.autograd.Function" title="(in PyTorch v2.8)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code></a> that gets called in <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.compile.html#torch.compile" title="(in PyTorch v2.8)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.compile()</span></code></a>, it will automatically
exclude that function from compiler optimizations. If your script uses <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.compile.html#torch.compile" title="(in PyTorch v2.8)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.compile()</span></code></a>,
we recommend using PyTorch version 2.3.0+, which has improvements that address this scenario.</p>
</section>
<section id="example-optimization-using-pytorch-custom-operators-pytorch-2-4-0">
<span id="pytorch-custom-ops-example"></span><h3>Example: Optimization using PyTorch custom operators (PyTorch &gt;= 2.4.0)<a class="headerlink" href="#example-optimization-using-pytorch-custom-operators-pytorch-2-4-0" title="Link to this heading">#</a></h3>
<p>PyTorch 2.4+ introduced <a class="reference external" href="https://pytorch.org/tutorials/advanced/python_custom_ops.html#python-custom-ops-tutorial">custom operators</a> to replace
PyTorch autograd functions. These treat arbitrary Python functions (including Warp calls) as opaque callables, which prevents
<a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.compile.html#torch.compile" title="(in PyTorch v2.8)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.compile()</span></code></a> from tracing into them. This means that forward PyTorch graph evaluations that include Warp kernel launches can be safely accelerated with
<a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.compile.html#torch.compile" title="(in PyTorch v2.8)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.compile()</span></code></a>. We can re-write the previous example using custom operators as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">warp</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">wp</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># Define the Rosenbrock function</span>
<span class="nd">@wp</span><span class="o">.</span><span class="n">func</span>
<span class="k">def</span><span class="w"> </span><span class="nf">rosenbrock</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span> <span class="o">**</span> <span class="mf">2.0</span> <span class="o">+</span> <span class="mf">100.0</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">x</span><span class="o">**</span><span class="mf">2.0</span><span class="p">)</span> <span class="o">**</span> <span class="mf">2.0</span>


<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">eval_rosenbrock</span><span class="p">(</span>
    <span class="n">xy</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">vec2</span><span class="p">),</span>
    <span class="c1"># outputs</span>
    <span class="n">z</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
<span class="p">):</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">xy</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">rosenbrock</span><span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>


<span class="nd">@torch</span><span class="o">.</span><span class="n">library</span><span class="o">.</span><span class="n">custom_op</span><span class="p">(</span><span class="s2">&quot;wp::warp_rosenbrock&quot;</span><span class="p">,</span> <span class="n">mutates_args</span><span class="o">=</span><span class="p">())</span>
<span class="k">def</span><span class="w"> </span><span class="nf">warp_rosenbrock</span><span class="p">(</span><span class="n">xy</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">num_points</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">wp_xy</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">xy</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">vec2</span><span class="p">)</span>
    <span class="n">wp_z</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_points</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="n">eval_rosenbrock</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">num_points</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">wp_xy</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">wp_z</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">wp</span><span class="o">.</span><span class="n">to_torch</span><span class="p">(</span><span class="n">wp_z</span><span class="p">)</span>


<span class="nd">@warp_rosenbrock</span><span class="o">.</span><span class="n">register_fake</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_</span><span class="p">(</span><span class="n">xy</span><span class="p">,</span> <span class="n">num_points</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">num_points</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>


<span class="nd">@torch</span><span class="o">.</span><span class="n">library</span><span class="o">.</span><span class="n">custom_op</span><span class="p">(</span><span class="s2">&quot;wp::warp_rosenbrock_backward&quot;</span><span class="p">,</span> <span class="n">mutates_args</span><span class="o">=</span><span class="p">())</span>
<span class="k">def</span><span class="w"> </span><span class="nf">warp_rosenbrock_backward</span><span class="p">(</span>
    <span class="n">xy</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">num_points</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">adj_z</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">wp_xy</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">xy</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">vec2</span><span class="p">)</span>
    <span class="n">wp_z</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">wp_adj_z</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">adj_z</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span>
        <span class="n">kernel</span><span class="o">=</span><span class="n">eval_rosenbrock</span><span class="p">,</span>
        <span class="n">dim</span><span class="o">=</span><span class="n">num_points</span><span class="p">,</span>
        <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">wp_xy</span><span class="p">],</span>
        <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">wp_z</span><span class="p">],</span>
        <span class="n">adj_inputs</span><span class="o">=</span><span class="p">[</span><span class="n">wp_xy</span><span class="o">.</span><span class="n">grad</span><span class="p">],</span>
        <span class="n">adj_outputs</span><span class="o">=</span><span class="p">[</span><span class="n">wp_adj_z</span><span class="p">],</span>
        <span class="n">adjoint</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">wp</span><span class="o">.</span><span class="n">to_torch</span><span class="p">(</span><span class="n">wp_xy</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>


<span class="nd">@warp_rosenbrock_backward</span><span class="o">.</span><span class="n">register_fake</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_</span><span class="p">(</span><span class="n">xy</span><span class="p">,</span> <span class="n">num_points</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">adj_z</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">xy</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">adj_z</span><span class="p">):</span>
    <span class="n">ctx</span><span class="o">.</span><span class="n">xy</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">warp_rosenbrock_backward</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">xy</span><span class="p">,</span> <span class="n">ctx</span><span class="o">.</span><span class="n">num_points</span><span class="p">,</span> <span class="n">ctx</span><span class="o">.</span><span class="n">z</span><span class="p">,</span> <span class="n">adj_z</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ctx</span><span class="o">.</span><span class="n">xy</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="kc">None</span>


<span class="k">def</span><span class="w"> </span><span class="nf">setup_context</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
    <span class="n">ctx</span><span class="o">.</span><span class="n">xy</span><span class="p">,</span> <span class="n">ctx</span><span class="o">.</span><span class="n">num_points</span> <span class="o">=</span> <span class="n">inputs</span>
    <span class="n">ctx</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">output</span>


<span class="n">warp_rosenbrock</span><span class="o">.</span><span class="n">register_autograd</span><span class="p">(</span><span class="n">backward</span><span class="p">,</span> <span class="n">setup_context</span><span class="o">=</span><span class="n">setup_context</span><span class="p">)</span>

<span class="n">num_points</span> <span class="o">=</span> <span class="mi">1500</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">5e-2</span>

<span class="n">torch_device</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">device_to_torch</span><span class="p">(</span><span class="n">wp</span><span class="o">.</span><span class="n">get_device</span><span class="p">())</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">xy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_points</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch_device</span><span class="p">)</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">xy</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

<span class="nd">@torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">fullgraph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">():</span>
    <span class="k">global</span> <span class="n">xy</span><span class="p">,</span> <span class="n">num_points</span>

    <span class="n">z</span> <span class="o">=</span> <span class="n">warp_rosenbrock</span><span class="p">(</span><span class="n">xy</span><span class="p">,</span> <span class="n">num_points</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">z</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
    <span class="c1"># step</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">forward</span><span class="p">()</span>
    <span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="c1"># minimum at (1, 1)</span>
<span class="n">xy_np</span> <span class="o">=</span> <span class="n">xy</span><span class="o">.</span><span class="n">numpy</span><span class="p">(</span><span class="n">force</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">xy_np</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="performance-notes">
<h3>Performance Notes<a class="headerlink" href="#performance-notes" title="Link to this heading">#</a></h3>
<p>The <a class="reference internal" href="#warp.from_torch" title="warp.from_torch"><code class="xref py py-func docutils literal notranslate"><span class="pre">wp.from_torch()</span></code></a> function creates a Warp array object that shares data with a PyTorch tensor.
Although this function does not copy the data, there is always some CPU overhead during the conversion.
If these conversions happen frequently, the overall program performance may suffer.
As a general rule, repeated conversions of the same tensor should be avoided.  Instead of:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">y_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">x_w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">x_t</span><span class="p">)</span>
    <span class="n">y_w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">y_t</span><span class="p">)</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">saxpy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x_w</span><span class="p">,</span> <span class="n">y_w</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<p>Try converting the arrays only once and reuse them:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">y_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="n">x_w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">x_t</span><span class="p">)</span>
<span class="n">y_w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">y_t</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">saxpy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x_w</span><span class="p">,</span> <span class="n">y_w</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<p>If reusing arrays is not possible (e.g., a new PyTorch tensor is constructed on every iteration), passing <code class="docutils literal notranslate"><span class="pre">return_ctype=True</span></code>
to <a class="reference internal" href="#warp.from_torch" title="warp.from_torch"><code class="xref py py-func docutils literal notranslate"><span class="pre">wp.from_torch()</span></code></a> should yield better performance.
Setting this argument to <code class="docutils literal notranslate"><span class="pre">True</span></code> avoids constructing a <code class="docutils literal notranslate"><span class="pre">wp.array</span></code> object and instead returns a low-level array descriptor.
This descriptor is a simple C structure that can be passed to Warp kernels instead of a <code class="docutils literal notranslate"><span class="pre">wp.array</span></code>,
but cannot be used in other places that require a <code class="docutils literal notranslate"><span class="pre">wp.array</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">):</span>
    <span class="c1"># get Torch tensors for this iteration</span>
    <span class="n">x_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">y_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># get Warp array descriptors</span>
    <span class="n">x_ctype</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">return_ctype</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">y_ctype</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">y_t</span><span class="p">,</span> <span class="n">return_ctype</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">saxpy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x_ctype</span><span class="p">,</span> <span class="n">y_ctype</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<p>An alternative approach is to pass the PyTorch tensors to Warp kernels directly.
This avoids constructing temporary Warp arrays by leveraging standard array interfaces (like <code class="docutils literal notranslate"><span class="pre">__cuda_array_interface__</span></code>) supported by both PyTorch and Warp.
The main advantage of this approach is convenience, since there is no need to call any conversion functions.
The main limitation is that it does not handle gradients, because gradient information is not included in the standard array interfaces.
This technique is therefore most suitable for algorithms that do not involve differentiation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">saxpy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>warp.examples.benchmarks.benchmark_interop_torch
</pre></div>
</div>
<p>Sample output:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">5095</span> <span class="n">ms</span>  <span class="n">from_torch</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="mi">2113</span> <span class="n">ms</span>  <span class="n">from_torch</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">return_ctype</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="mi">2950</span> <span class="n">ms</span>  <span class="n">direct</span> <span class="kn">from</span><span class="w"> </span><span class="nn">torch</span>
</pre></div>
</div>
<p>The default <a class="reference internal" href="#warp.from_torch" title="warp.from_torch"><code class="xref py py-func docutils literal notranslate"><span class="pre">wp.from_torch()</span></code></a> conversion is the slowest.
Passing <code class="docutils literal notranslate"><span class="pre">return_ctype=True</span></code> is the fastest, because it skips creating temporary Warp array objects.
Passing PyTorch tensors to Warp kernels directly falls somewhere in between.
It skips creating temporary Warp arrays, but accessing the <code class="docutils literal notranslate"><span class="pre">__cuda_array_interface__</span></code> attributes of PyTorch tensors
adds overhead because they are initialized on-demand.</p>
</section>
</section>
<section id="cupy-numba">
<h2>CuPy/Numba<a class="headerlink" href="#cupy-numba" title="Link to this heading">#</a></h2>
<p>Warp GPU arrays support the <code class="docutils literal notranslate"><span class="pre">__cuda_array_interface__</span></code> protocol for sharing data with other Python GPU frameworks.
This allows frameworks like CuPy to use Warp GPU arrays directly.</p>
<p>Likewise, Warp arrays can be created from any object that exposes the <code class="docutils literal notranslate"><span class="pre">__cuda_array_interface__</span></code>.
Such objects can also be passed to Warp kernels directly without creating a Warp array object.</p>
</section>
<section id="jax">
<span id="jax-interop"></span><h2>JAX<a class="headerlink" href="#jax" title="Link to this heading">#</a></h2>
<p>Interoperability with JAX arrays is supported through the following methods.
Internally these use the DLPack protocol to exchange data in a zero-copy way with JAX:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">warp_array</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_jax</span><span class="p">(</span><span class="n">jax_array</span><span class="p">)</span>
<span class="n">jax_array</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">to_jax</span><span class="p">(</span><span class="n">warp_array</span><span class="p">)</span>
</pre></div>
</div>
<p>It may be preferable to use the <a class="reference internal" href="#dlpack"><span class="std std-ref">DLPack</span></a> protocol directly for better performance and control over stream synchronization .</p>
<dl class="py function">
<dt class="sig sig-object py" id="warp.from_jax">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">from_jax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">jax_array</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/jax.py#L175-L187"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.from_jax" title="Link to this definition">#</a></dt>
<dd><p>Convert a Jax array to a Warp array without copying the data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>jax_array</strong> (<a class="reference external" href="https://docs.jax.dev/en/latest/_autosummary/jax.Array.html#jax.Array" title="(in JAX)"><em>jax.Array</em></a>) – The Jax array to convert.</p></li>
<li><p><strong>dtype</strong> (<em>optional</em>) – The target data type of the resulting Warp array. Defaults to the Jax array’s data type mapped to a Warp data type.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The converted Warp array.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="runtime.html#warp.array" title="warp.array">warp.array</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.to_jax">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">to_jax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">warp_array</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/jax.py#L160-L173"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.to_jax" title="Link to this definition">#</a></dt>
<dd><p>Convert a Warp array to a Jax array without copying the data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>warp_array</strong> (<a class="reference internal" href="runtime.html#warp.array" title="warp.array"><em>warp.array</em></a>) – The Warp array to convert.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The converted Jax array.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.jax.dev/en/latest/_autosummary/jax.Array.html#jax.Array" title="(in JAX)">jax.Array</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.device_from_jax">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">device_from_jax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">jax_device</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/jax.py#L44-L59"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.device_from_jax" title="Link to this definition">#</a></dt>
<dd><p>Return the Warp device corresponding to a Jax device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>jax_device</strong> (<a class="reference external" href="https://docs.jax.dev/en/latest/_autosummary/jax.Device.html#jax.Device" title="(in JAX)"><em>jax.Device</em></a>) – A Jax device descriptor.</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#RuntimeError" title="(in Python v3.13)"><strong>RuntimeError</strong></a> – The Jax device is neither a CPU nor GPU device.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="devices.html#warp.context.Device" title="warp.context.Device"><em>Device</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.device_to_jax">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">device_to_jax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">warp_device</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/jax.py#L19-L42"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.device_to_jax" title="Link to this definition">#</a></dt>
<dd><p>Return the Jax device corresponding to a Warp device.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.jax.dev/en/latest/_autosummary/jax.Device.html#jax.Device" title="(in JAX)"><code class="xref py py-class docutils literal notranslate"><span class="pre">jax.Device</span></code></a></p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#RuntimeError" title="(in Python v3.13)"><strong>RuntimeError</strong></a> – Failed to find the corresponding Jax device.</p>
</dd>
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>warp_device</strong> (<a class="reference internal" href="devices.html#warp.context.Device" title="warp.context.Device"><em>Device</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> | </em><em>None</em>)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.dtype_from_jax">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">dtype_from_jax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">jax_dtype</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/jax.py#L109-L153"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.dtype_from_jax" title="Link to this definition">#</a></dt>
<dd><p>Return the Warp dtype corresponding to a Jax dtype.</p>
<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.13)"><strong>TypeError</strong></a> – Unable to find a corresponding Warp data type.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.dtype_to_jax">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">dtype_to_jax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">warp_dtype</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/jax.py#L74-L107"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.dtype_to_jax" title="Link to this definition">#</a></dt>
<dd><p>Return the Jax dtype corresponding to a Warp dtype.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>warp_dtype</strong> – A Warp data type that has a corresponding Jax data type.</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.13)"><strong>TypeError</strong></a> – Unable to find a corresponding Jax data type.</p>
</dd>
</dl>
</dd></dl>

<section id="using-warp-kernels-as-jax-primitives">
<h3>Using Warp kernels as JAX primitives<a class="headerlink" href="#using-warp-kernels-as-jax-primitives" title="Link to this heading">#</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This version of <a class="reference internal" href="#warp.jax_experimental.jax_kernel" title="warp.jax_experimental.jax_kernel"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax_kernel()</span></code></a> is based on JAX features that are now deprecated.</p>
<p>For JAX version 0.5.0 or newer, users are encouraged to switch to the new FFI version of
<a class="reference internal" href="#warp.jax_experimental.ffi.jax_kernel" title="warp.jax_experimental.ffi.jax_kernel"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax_kernel()</span></code></a>
based on the new <a class="reference internal" href="#jax-ffi"><span class="std std-ref">Foreign Function Interface (FFI)</span></a>.</p>
<p>In Warp version 1.10, the FFI version will become the default implementation of <cite>jax_kernel()</cite>.
The older version will continue to be available as <cite>warp.jax_experimental.custom_call.jax_kernel</cite>.</p>
</div>
<p>Warp kernels can be used as JAX primitives, which allows calling them inside of jitted JAX functions:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">warp</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">wp</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jnp</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">warp.jax_experimental</span><span class="w"> </span><span class="kn">import</span> <span class="n">jax_kernel</span>

<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">triple_kernel</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span> <span class="n">output</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">output</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="mf">3.0</span> <span class="o">*</span> <span class="nb">input</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span>

<span class="c1"># create a Jax primitive from a Warp kernel</span>
<span class="n">jax_triple</span> <span class="o">=</span> <span class="n">jax_kernel</span><span class="p">(</span><span class="n">triple_kernel</span><span class="p">)</span>

<span class="c1"># use the Warp kernel in a Jax jitted function</span>
<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">():</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jax_triple</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="p">())</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="warp.jax_experimental.jax_kernel">
<span class="sig-prename descclassname"><span class="pre">warp.jax_experimental.</span></span><span class="sig-name descname"><span class="pre">jax_kernel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">launch_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quiet</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/jax_experimental/custom_call.py#L32-L86"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.jax_experimental.jax_kernel" title="Link to this definition">#</a></dt>
<dd><p>Create a Jax primitive from a Warp kernel.</p>
<p>NOTE: This is an experimental feature under development.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>kernel</strong> – The Warp kernel to be wrapped.</p></li>
<li><p><strong>launch_dims</strong> – Optional. Specify the kernel launch dimensions. If None,
dimensions are inferred from the shape of the first argument.
This option when set will specify the output dimensions.</p></li>
<li><p><strong>quiet</strong> – Optional. If True, suppress deprecation warnings with newer JAX versions.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Limitations:</dt><dd><ul class="simple">
<li><p>All kernel arguments must be contiguous arrays.</p></li>
<li><p>Input arguments are followed by output arguments in the Warp kernel definition.</p></li>
<li><p>There must be at least one input argument and at least one output argument.</p></li>
<li><p>Only the CUDA backend is supported.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<section id="input-and-output-semantics">
<h4>Input and Output Semantics<a class="headerlink" href="#input-and-output-semantics" title="Link to this heading">#</a></h4>
<p>All kernel arguments must be contiguous arrays.
Input arguments must come before output arguments in the kernel definition.
At least one input array and one output array are required.
Here is a kernel with three inputs and two outputs:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">warp</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">wp</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jnp</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">warp.jax_experimental</span><span class="w"> </span><span class="kn">import</span> <span class="n">jax_kernel</span>

<span class="c1"># kernel with multiple inputs and outputs</span>
<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">multiarg_kernel</span><span class="p">(</span>
    <span class="c1"># inputs</span>
    <span class="n">a</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
    <span class="n">b</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
    <span class="n">c</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
    <span class="c1"># outputs</span>
    <span class="n">ab</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
    <span class="n">bc</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
<span class="p">):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">ab</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span>
    <span class="n">bc</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">b</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">+</span> <span class="n">c</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span>

<span class="c1"># create a Jax primitive from a Warp kernel</span>
<span class="n">jax_multiarg</span> <span class="o">=</span> <span class="n">jax_kernel</span><span class="p">(</span><span class="n">multiarg_kernel</span><span class="p">)</span>

<span class="c1"># use the Warp kernel in a Jax jitted function with three inputs and two outputs</span>
<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">():</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jax_multiarg</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>

<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="kernel-launch-and-output-dimensions">
<h4>Kernel Launch and Output Dimensions<a class="headerlink" href="#kernel-launch-and-output-dimensions" title="Link to this heading">#</a></h4>
<p>By default, the launch dimensions are inferred from the shape of the first input array.
When that’s not appropriate, the <code class="docutils literal notranslate"><span class="pre">launch_dims</span></code> argument can be used to override this behavior.
The launch dimensions also determine the shape of the output arrays.
Here is a simple matrix multiplication kernel that multiplies an NxK matrix by a KxM matrix.
The launch dimensions and output shape must be (N, M), which is different than the shape of the input arrays:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">warp</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">wp</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jnp</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">warp</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">wp</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">warp.jax_experimental</span><span class="w"> </span><span class="kn">import</span> <span class="n">jax_kernel</span>

<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">matmul_kernel</span><span class="p">(</span>
    <span class="n">a</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array2d</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>  <span class="c1"># NxK input</span>
    <span class="n">b</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array2d</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>  <span class="c1"># KxM input</span>
    <span class="n">c</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array2d</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>  <span class="c1"># NxM output</span>
<span class="p">):</span>
    <span class="c1"># launch dims should be (N, M)</span>
    <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">M</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="ow">and</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">M</span><span class="p">:</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">b</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
        <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">s</span>

<span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">K</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span>

<span class="c1"># specify custom launch dimensions</span>
<span class="n">jax_matmul</span> <span class="o">=</span> <span class="n">jax_kernel</span><span class="p">(</span><span class="n">matmul_kernel</span><span class="p">,</span> <span class="n">launch_dims</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">))</span>

<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">():</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">K</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">K</span><span class="p">,</span> <span class="n">M</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="c1"># use default launch dims</span>
    <span class="k">return</span> <span class="n">jax_matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="p">())</span>
</pre></div>
</div>
</section>
<section id="jax-foreign-function-interface-ffi">
<span id="jax-ffi"></span><h4>JAX Foreign Function Interface (FFI)<a class="headerlink" href="#jax-foreign-function-interface-ffi" title="Link to this heading">#</a></h4>
<div class="versionadded">
<p><span class="versionmodified added">Added in version 1.7.</span></p>
</div>
<p>JAX v0.5.0 introduced a new <a class="reference external" href="https://docs.jax.dev/en/latest/ffi.html">foreign function interface</a> that supersedes
the older custom call mechanism. One important benefit is that it allows the foreign function to be captured in a CUDA
graph together with other JAX operations. This can lead to significant performance improvements.</p>
<p>Users of newer JAX versions are encouraged to switch to the new implementation of
<a class="reference internal" href="#warp.jax_experimental.ffi.jax_kernel" title="warp.jax_experimental.ffi.jax_kernel"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax_kernel()</span></code></a> based on FFI.
The old implementation is still available to avoid breaking existing code,
but future development will likely focus on the FFI version.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">warp.jax_experimental.ffi</span><span class="w"> </span><span class="kn">import</span> <span class="n">jax_kernel</span>  <span class="c1"># new FFI-based jax_kernel()</span>
</pre></div>
</div>
<p>The new implementation is likely to be faster and it is also more flexible.</p>
<dl class="py function">
<dt class="sig sig-object py" id="warp.jax_experimental.ffi.jax_kernel">
<span class="sig-prename descclassname"><span class="pre">warp.jax_experimental.ffi.</span></span><span class="sig-name descname"><span class="pre">jax_kernel</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">kernel</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">num_outputs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">vmap_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'broadcast_all'</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">launch_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">output_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">in_out_argnames</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/jax_experimental/ffi.py#L655-L701"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.jax_experimental.ffi.jax_kernel" title="Link to this definition">#</a></dt>
<dd><p>Create a JAX callback from a Warp kernel.</p>
<p>NOTE: This is an experimental feature under development.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>kernel</strong> – The Warp kernel to launch.</p></li>
<li><p><strong>num_outputs</strong> – Optional. Specify the number of output arguments if greater than 1.
This must include the number of <code class="docutils literal notranslate"><span class="pre">in_out_arguments</span></code>.</p></li>
<li><p><strong>vmap_method</strong> – Optional. String specifying how the callback transforms under <code class="docutils literal notranslate"><span class="pre">vmap()</span></code>.
This argument can also be specified for individual calls.</p></li>
<li><p><strong>launch_dims</strong> – Optional. Specify the default kernel launch dimensions. If None, launch
dimensions are inferred from the shape of the first array argument.
This argument can also be specified for individual calls.</p></li>
<li><p><strong>output_dims</strong> – Optional. Specify the default dimensions of output arrays.  If None, output
dimensions are inferred from the launch dimensions.
This argument can also be specified for individual calls.</p></li>
<li><p><strong>in_out_argnames</strong> – Optional. Names of input-output arguments.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Limitations:</dt><dd><ul class="simple">
<li><p>All kernel arguments must be contiguous arrays or scalars.</p></li>
<li><p>Scalars must be static arguments in JAX.</p></li>
<li><p>Input and input-output arguments must precede the output arguments in the <code class="docutils literal notranslate"><span class="pre">kernel</span></code> definition.</p></li>
<li><p>There must be at least one output or input-output argument.</p></li>
<li><p>Only the CUDA backend is supported.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="id1">
<h4>Input and Output Semantics<a class="headerlink" href="#id1" title="Link to this heading">#</a></h4>
<p>Input arguments must come before output arguments in the kernel definition.
At least one output array is required, but it’s ok to have kernels with no inputs.
The new <a class="reference internal" href="#warp.jax_experimental.ffi.jax_kernel" title="warp.jax_experimental.ffi.jax_kernel"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax_kernel()</span></code></a> allows specifying the number of outputs using the
<code class="docutils literal notranslate"><span class="pre">num_outputs</span></code> argument.
It defaults to one, so this argument is only needed for kernels with multiple outputs.</p>
<p>Here’s a kernel with two inputs and one output:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">jax</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jnp</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">warp</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">wp</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">warp.jax_experimental.ffi</span><span class="w"> </span><span class="kn">import</span> <span class="n">jax_kernel</span>

<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">add_kernel</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">),</span>
               <span class="n">b</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">),</span>
               <span class="n">output</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">output</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span>

<span class="n">jax_add</span> <span class="o">=</span> <span class="n">jax_kernel</span><span class="p">(</span><span class="n">add_kernel</span><span class="p">)</span>

<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">():</span>
    <span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jax_add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="p">())</span>
</pre></div>
</div>
<p>One input and two outputs:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">math</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">jax</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jnp</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">warp</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">wp</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">warp.jax_experimental.ffi</span><span class="w"> </span><span class="kn">import</span> <span class="n">jax_kernel</span>

<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">sincos_kernel</span><span class="p">(</span><span class="n">angle</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
                  <span class="c1"># outputs</span>
                  <span class="n">sin_out</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
                  <span class="n">cos_out</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">sin_out</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">angle</span><span class="p">[</span><span class="n">tid</span><span class="p">])</span>
    <span class="n">cos_out</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">angle</span><span class="p">[</span><span class="n">tid</span><span class="p">])</span>

<span class="n">jax_sincos</span> <span class="o">=</span> <span class="n">jax_kernel</span><span class="p">(</span><span class="n">sincos_kernel</span><span class="p">,</span> <span class="n">num_outputs</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># specify multiple outputs</span>

<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">():</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jax_sincos</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="n">s</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">f</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
<p>Here is a kernel with no inputs that initializes an array of 3x3 matrices with the diagonal values (1, 2, 3).
With no inputs, specifying the launch dimensions is required to determine the shape of the output array:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">diagonal_kernel</span><span class="p">(</span><span class="n">output</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">mat33</span><span class="p">)):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">output</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">mat33</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">)</span>

<span class="n">jax_diagonal</span> <span class="o">=</span> <span class="n">jax_kernel</span><span class="p">(</span><span class="n">diagonal_kernel</span><span class="p">)</span>

<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">():</span>
    <span class="c1"># launch dimensions determine the output shape</span>
    <span class="k">return</span> <span class="n">jax_diagonal</span><span class="p">(</span><span class="n">launch_dims</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="p">())</span>
</pre></div>
</div>
<section id="scalar-inputs">
<h5>Scalar Inputs<a class="headerlink" href="#scalar-inputs" title="Link to this heading">#</a></h5>
<p>Scalar input arguments are supported, although there are some limitations. Currently, scalars passed to Warp kernels must be constant or static values in JAX:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">scale_kernel</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
                 <span class="n">s</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>  <span class="c1"># scalar input</span>
                 <span class="n">output</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">output</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">*</span> <span class="n">s</span>


<span class="n">jax_scale</span> <span class="o">=</span> <span class="n">jax_kernel</span><span class="p">(</span><span class="n">scale_kernel</span><span class="p">)</span>

<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">():</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jax_scale</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">)</span>  <span class="c1"># ok: constant scalar argument</span>

<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="p">())</span>
</pre></div>
</div>
<p>Trying to use a traced scalar value will result in an exception:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jax_scale</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>  <span class="c1"># ERROR: traced scalar argument</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">))</span>
</pre></div>
</div>
<p>JAX static arguments to the rescue:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># make scalar arguments static</span>
<span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnames</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;s&quot;</span><span class="p">])</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jax_scale</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>  <span class="c1"># ok: static scalar argument</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">))</span>
</pre></div>
</div>
</section>
</section>
<section id="id2">
<h4>Kernel Launch and Output Dimensions<a class="headerlink" href="#id2" title="Link to this heading">#</a></h4>
<p>By default, the launch dimensions are inferred from the shape of the first input array.
When that’s not appropriate, the <code class="docutils literal notranslate"><span class="pre">launch_dims</span></code> argument can be used to override this behavior.
The launch dimensions also determine the shape of the output arrays.</p>
<p>Here is a simple matrix multiplication kernel that multiplies an NxK matrix by a KxM matrix.
The launch dimensions and output shape must be (N, M), which is different than the shape of the input arrays.</p>
<p>Note that the new <a class="reference internal" href="#warp.jax_experimental.ffi.jax_kernel" title="warp.jax_experimental.ffi.jax_kernel"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax_kernel()</span></code></a> allows specifying custom launch
dimensions with each call, which is more flexible than the old implementation, although the old approach is still supported:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">matmul_kernel</span><span class="p">(</span>
    <span class="n">a</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array2d</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>  <span class="c1"># NxK input</span>
    <span class="n">b</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array2d</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>  <span class="c1"># KxM input</span>
    <span class="n">c</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array2d</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>  <span class="c1"># NxM output</span>
<span class="p">):</span>
    <span class="c1"># launch dimensions should be (N, M)</span>
    <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">M</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="ow">and</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">M</span><span class="p">:</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">b</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
        <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">s</span>

<span class="c1"># no need to specify launch dims here</span>
<span class="n">jax_matmul</span> <span class="o">=</span> <span class="n">jax_kernel</span><span class="p">(</span><span class="n">matmul_kernel</span><span class="p">)</span>

<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">():</span>
    <span class="n">N1</span><span class="p">,</span> <span class="n">M1</span><span class="p">,</span> <span class="n">K1</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span>
    <span class="n">a1</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">N1</span><span class="p">,</span> <span class="n">K1</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">K1</span><span class="p">,</span> <span class="n">M1</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="c1"># use custom launch dims</span>
    <span class="n">result1</span> <span class="o">=</span> <span class="n">jax_matmul</span><span class="p">(</span><span class="n">a1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">launch_dims</span><span class="o">=</span><span class="p">(</span><span class="n">N1</span><span class="p">,</span> <span class="n">M1</span><span class="p">))</span>

    <span class="n">N2</span><span class="p">,</span> <span class="n">M2</span><span class="p">,</span> <span class="n">K2</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span>
    <span class="n">a2</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">N2</span><span class="p">,</span> <span class="n">K2</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">K2</span><span class="p">,</span> <span class="n">M2</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="c1"># use custom launch dims</span>
    <span class="n">result2</span> <span class="o">=</span> <span class="n">jax_matmul</span><span class="p">(</span><span class="n">a2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">launch_dims</span><span class="o">=</span><span class="p">(</span><span class="n">N2</span><span class="p">,</span> <span class="n">M2</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">result1</span><span class="p">,</span> <span class="n">result2</span>

<span class="n">r1</span><span class="p">,</span> <span class="n">r2</span> <span class="o">=</span> <span class="n">f</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">r1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">r2</span><span class="p">)</span>
</pre></div>
</div>
<p>By default, output array shapes are determined from the launch dimensions, but it’s possible to specify custom output
dimensions using the <code class="docutils literal notranslate"><span class="pre">output_dims</span></code> argument. Consider a kernel like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">funky_kernel</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
                 <span class="c1"># outputs</span>
                 <span class="n">b</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
                 <span class="n">c</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)):</span>
    <span class="o">...</span>

<span class="n">jax_funky</span> <span class="o">=</span> <span class="n">jax_kernel</span><span class="p">(</span><span class="n">funky_kernel</span><span class="p">,</span> <span class="n">num_outputs</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>Specify a custom output shape used for all outputs:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">b</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">jax_funky</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">output_dims</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
</pre></div>
</div>
<p>Specify different output dimensions for each output using a dictionary:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">b</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">jax_funky</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">output_dims</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="n">n</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">:</span> <span class="n">m</span><span class="p">})</span>
</pre></div>
</div>
<p>Specify custom launch and output dimensions together:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">b</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">jax_funky</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">launch_dims</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">output_dims</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="n">n</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">:</span> <span class="n">m</span><span class="p">})</span>
</pre></div>
</div>
<p>One-dimensional shapes can be specified using an integer. Multi-dimensional shapes can be specified using tuples or lists of integers.</p>
<section id="vector-and-matrix-arrays">
<h5>Vector and Matrix Arrays<a class="headerlink" href="#vector-and-matrix-arrays" title="Link to this heading">#</a></h5>
<p>Arrays of Warp vector and matrix types are supported.
Since JAX does not have corresponding data types, the components are packed into extra inner dimensions of JAX arrays.
For example, a Warp array of <code class="docutils literal notranslate"><span class="pre">wp.vec3</span></code> will have a JAX array shape of (…, 3) and a Warp array of <code class="docutils literal notranslate"><span class="pre">wp.mat22</span></code> will
have a JAX array shape of (…, 2, 2):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">vecmat_kernel</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
                  <span class="n">b</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">vec3</span><span class="p">),</span>
                  <span class="n">c</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">mat22</span><span class="p">),</span>
                  <span class="c1"># outputs</span>
                  <span class="n">d</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
                  <span class="n">e</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">vec3</span><span class="p">),</span>
                  <span class="n">f</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">mat22</span><span class="p">)):</span>
    <span class="o">...</span>

<span class="n">jax_vecmat</span> <span class="o">=</span> <span class="n">jax_kernel</span><span class="p">(</span><span class="n">vecmat_kernel</span><span class="p">,</span> <span class="n">num_outputs</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">():</span>
    <span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>          <span class="c1"># scalar array</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>     <span class="c1"># vec3 array</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># mat22 array</span>

    <span class="n">d</span><span class="p">,</span> <span class="n">e</span><span class="p">,</span> <span class="n">f</span> <span class="o">=</span> <span class="n">vecmat_kernel</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
<p>It’s important to recognize that the Warp and JAX array shapes are different for vector and matrix types.
In the above snippet, Warp sees <code class="docutils literal notranslate"><span class="pre">a</span></code>, <code class="docutils literal notranslate"><span class="pre">b</span></code>, and <code class="docutils literal notranslate"><span class="pre">c</span></code> as one-dimensional arrays of <code class="docutils literal notranslate"><span class="pre">wp.float32</span></code>, <code class="docutils literal notranslate"><span class="pre">wp.vec3</span></code>, and
<code class="docutils literal notranslate"><span class="pre">wp.mat22</span></code>, respectively. In JAX, <code class="docutils literal notranslate"><span class="pre">a</span></code> is a one-dimensional array with length n, <code class="docutils literal notranslate"><span class="pre">b</span></code> is a two-dimensional array
with shape (n, 3), and <code class="docutils literal notranslate"><span class="pre">c</span></code> is a three-dimensional array with shape (n, 2, 2).</p>
<p>When specifying custom output dimensions, it’s possible to use either convention. The following calls are equivalent:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">d</span><span class="p">,</span> <span class="n">e</span><span class="p">,</span> <span class="n">f</span> <span class="o">=</span> <span class="n">vecmat_kernel</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">output_dims</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">d</span><span class="p">,</span> <span class="n">e</span><span class="p">,</span> <span class="n">f</span> <span class="o">=</span> <span class="n">vecmat_kernel</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">output_dims</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;d&quot;</span><span class="p">:</span> <span class="n">n</span><span class="p">,</span> <span class="s2">&quot;e&quot;</span><span class="p">:</span> <span class="n">n</span><span class="p">,</span> <span class="s2">&quot;f&quot;</span><span class="p">:</span> <span class="n">n</span><span class="p">})</span>
<span class="n">d</span><span class="p">,</span> <span class="n">e</span><span class="p">,</span> <span class="n">f</span> <span class="o">=</span> <span class="n">vecmat_kernel</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">output_dims</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;d&quot;</span><span class="p">:</span> <span class="n">n</span><span class="p">,</span> <span class="s2">&quot;e&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="s2">&quot;f&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)})</span>
</pre></div>
</div>
<p>This is a convenience feature meant to simplify writing code.
For example, when Warp expects the arrays to be of the same shape, we only need to specify the shape once without
worrying about the extra vector and matrix dimensions required by JAX:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">d</span><span class="p">,</span> <span class="n">e</span><span class="p">,</span> <span class="n">f</span> <span class="o">=</span> <span class="n">vecmat_kernel</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">output_dims</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
</pre></div>
</div>
<p>On the other hand, JAX dimensions are also accepted to allow passing shapes directly from JAX:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">d</span><span class="p">,</span> <span class="n">e</span><span class="p">,</span> <span class="n">f</span> <span class="o">=</span> <span class="n">vecmat_kernel</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">output_dims</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;d&quot;</span><span class="p">:</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;e&quot;</span><span class="p">:</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;f&quot;</span><span class="p">:</span> <span class="n">c</span><span class="o">.</span><span class="n">shape</span><span class="p">})</span>
</pre></div>
</div>
<p>See <a class="reference external" href="https://github.com/NVIDIA/warp/tree/main/warp/examples/interop/example_jax_kernel.py">example_jax_kernel.py</a> for examples.</p>
</section>
</section>
<section id="jax-vmap-support">
<h4>JAX VMAP Support<a class="headerlink" href="#jax-vmap-support" title="Link to this heading">#</a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">vmap_method</span></code> argument can be used to specify how the callback transforms under <a class="reference external" href="https://docs.jax.dev/en/latest/_autosummary/jax.vmap.html#jax.vmap" title="(in JAX)"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax.vmap()</span></code></a>.
The default is <code class="docutils literal notranslate"><span class="pre">&quot;broadcast_all&quot;</span></code>. This argument can be passed to <a class="reference internal" href="#warp.jax_experimental.ffi.jax_kernel" title="warp.jax_experimental.ffi.jax_kernel"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax_kernel()</span></code></a>,
and it can also be passed to each call:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># set default vmap behavior</span>
<span class="n">jax_callback</span> <span class="o">=</span> <span class="n">jax_kernel</span><span class="p">(</span><span class="n">my_kernel</span><span class="p">,</span> <span class="n">vmap_method</span><span class="o">=</span><span class="s2">&quot;sequential&quot;</span><span class="p">)</span>

<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">():</span>
    <span class="o">...</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">jax_callback</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>  <span class="c1"># uses &quot;sequential&quot;</span>
    <span class="o">...</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">jax_callback</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">vmap_method</span><span class="o">=</span><span class="s2">&quot;expand_dims&quot;</span><span class="p">)</span>  <span class="c1"># uses &quot;expand_dims&quot;</span>
    <span class="o">...</span>
</pre></div>
</div>
</section>
<section id="calling-annotated-python-functions">
<h4>Calling Annotated Python Functions<a class="headerlink" href="#calling-annotated-python-functions" title="Link to this heading">#</a></h4>
<p>The <a class="reference internal" href="#warp.jax_experimental.ffi.jax_kernel" title="warp.jax_experimental.ffi.jax_kernel"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax_kernel()</span></code></a> mechanism can be used to launch a single Warp kernel
from JAX, but it’s also possible to call a Python function that launches multiple kernels.
The target Python function should have argument type annotations as if it were a Warp kernel.
To call this function from JAX, use <a class="reference internal" href="#warp.jax_experimental.ffi.jax_callable" title="warp.jax_experimental.ffi.jax_callable"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax_callable()</span></code></a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">warp.jax_experimental.ffi</span><span class="w"> </span><span class="kn">import</span> <span class="n">jax_callable</span>

<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">scale_kernel</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span> <span class="n">s</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">output</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">output</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">*</span> <span class="n">s</span>

<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">scale_vec_kernel</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">vec2</span><span class="p">),</span> <span class="n">s</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">output</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">vec2</span><span class="p">)):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">output</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">*</span> <span class="n">s</span>


<span class="c1"># The Python function to call.</span>
<span class="c1"># Note the argument type annotations, just like Warp kernels.</span>
<span class="k">def</span><span class="w"> </span><span class="nf">example_func</span><span class="p">(</span>
    <span class="c1"># inputs</span>
    <span class="n">a</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
    <span class="n">b</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">vec2</span><span class="p">),</span>
    <span class="n">s</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="c1"># outputs</span>
    <span class="n">c</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
    <span class="n">d</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">vec2</span><span class="p">),</span>
<span class="p">):</span>
    <span class="c1"># launch multiple kernels</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">scale_kernel</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">a</span><span class="p">,</span> <span class="n">s</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">c</span><span class="p">])</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">scale_vec_kernel</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">s</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">d</span><span class="p">])</span>


<span class="n">jax_func</span> <span class="o">=</span> <span class="n">jax_callable</span><span class="p">(</span><span class="n">example_func</span><span class="p">,</span> <span class="n">num_outputs</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">():</span>
    <span class="c1"># inputs</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>  <span class="c1"># wp.vec2</span>
    <span class="n">s</span> <span class="o">=</span> <span class="mf">2.0</span>

    <span class="c1"># output shapes</span>
    <span class="n">output_dims</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;c&quot;</span><span class="p">:</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;d&quot;</span><span class="p">:</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">}</span>

    <span class="n">c</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">jax_func</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">output_dims</span><span class="o">=</span><span class="n">output_dims</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span>

<span class="n">r1</span><span class="p">,</span> <span class="n">r2</span> <span class="o">=</span> <span class="n">f</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">r1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">r2</span><span class="p">)</span>
</pre></div>
</div>
<p>The input and output semantics of <a class="reference internal" href="#warp.jax_experimental.ffi.jax_callable" title="warp.jax_experimental.ffi.jax_callable"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax_callable()</span></code></a> are similar to
<a class="reference internal" href="#warp.jax_experimental.ffi.jax_kernel" title="warp.jax_experimental.ffi.jax_kernel"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax_kernel()</span></code></a>, so we won’t recap everything here,
just focus on the differences:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#warp.jax_experimental.ffi.jax_callable" title="warp.jax_experimental.ffi.jax_callable"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax_callable()</span></code></a> does not take a <code class="docutils literal notranslate"><span class="pre">launch_dims</span></code> argument,
since the target function is responsible for launching kernels using appropriate dimensions.</p></li>
<li><p><a class="reference internal" href="#warp.jax_experimental.ffi.jax_callable" title="warp.jax_experimental.ffi.jax_callable"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax_callable()</span></code></a> takes an optional <code class="docutils literal notranslate"><span class="pre">graph_mode</span></code> argument, which determines how the callable can be captured in a CUDA graph.
Graphs are generally desirable, since they can greatly improve the application performance.
<code class="docutils literal notranslate"><span class="pre">GraphMode.JAX</span></code> (default) lets JAX capture the graph, which may be used as a subgraph in an enclosing capture for maximal benefit.
<code class="docutils literal notranslate"><span class="pre">GraphMode.WARP</span></code> lets Warp capture the graph. Use this mode when the callable cannot be used as a subgraph, such as when the callable uses conditional graph nodes.
<code class="docutils literal notranslate"><span class="pre">GraphMode.NONE</span></code> disables graph capture. Use this mode if the callable performs operations that are not allowed during graph capture, such as host synchronization.</p></li>
</ul>
<p>See <a class="reference external" href="https://github.com/NVIDIA/warp/tree/main/warp/examples/interop/example_jax_callable.py">example_jax_callable.py</a> for examples.</p>
<dl class="py function">
<dt class="sig sig-object py" id="warp.jax_experimental.ffi.jax_callable">
<span class="sig-prename descclassname"><span class="pre">warp.jax_experimental.ffi.</span></span><span class="sig-name descname"><span class="pre">jax_callable</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">func</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">num_outputs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">graph_compatible</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">graph_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">GraphMode.JAX</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">vmap_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'broadcast_all'</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">output_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">in_out_argnames</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/jax_experimental/ffi.py#L703-L770"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.jax_experimental.ffi.jax_callable" title="Link to this definition">#</a></dt>
<dd><p>Create a JAX callback from an annotated Python function.</p>
<p>The Python function arguments must have type annotations like Warp kernels.</p>
<p>NOTE: This is an experimental feature under development.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>func</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.13)"><em>Callable</em></a>) – The Python function to call.</p></li>
<li><p><strong>num_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Optional. Specify the number of output arguments if greater than 1.
This must include the number of <code class="docutils literal notranslate"><span class="pre">in_out_arguments</span></code>.</p></li>
<li><p><strong>graph_compatible</strong> (<a class="reference internal" href="functions.html#warp.bool" title="warp.bool"><em>bool</em></a><em> | </em><em>None</em>) – Optional. Whether the function can be called during CUDA graph capture.
This argument is deprecated, use <code class="docutils literal notranslate"><span class="pre">graph_mode</span></code> instead.</p></li>
<li><p><strong>graph_mode</strong> (<em>GraphMode</em>) – Optional. CUDA graph capture mode.
<code class="docutils literal notranslate"><span class="pre">GraphMode.JAX</span></code> (default): Let JAX capture the graph, which may be used as a subgraph in an enclosing capture.
<code class="docutils literal notranslate"><span class="pre">GraphMode.WARP</span></code>: Let Warp capture the graph. Use this mode when the callable cannot be used as a subraph,
such as when the callable uses conditional graph nodes.
<code class="docutils literal notranslate"><span class="pre">GraphMode.NONE</span></code>: Disable graph capture. Use when the callable performs operations that are not legal in a graph,
such as host synchronization.</p></li>
<li><p><strong>vmap_method</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> | </em><em>None</em>) – Optional. String specifying how the callback transforms under <code class="docutils literal notranslate"><span class="pre">vmap()</span></code>.
This argument can also be specified for individual calls.</p></li>
<li><p><strong>output_dims</strong> – Optional. Specify the default dimensions of output arrays.
If <code class="docutils literal notranslate"><span class="pre">None</span></code>, output dimensions are inferred from the launch dimensions.
This argument can also be specified for individual calls.</p></li>
<li><p><strong>in_out_argnames</strong> – Optional. Names of input-output arguments.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Limitations:</dt><dd><ul class="simple">
<li><p>All kernel arguments must be contiguous arrays or scalars.</p></li>
<li><p>Scalars must be static arguments in JAX.</p></li>
<li><p>Input and input-output arguments must precede the output arguments in the <code class="docutils literal notranslate"><span class="pre">func</span></code> definition.</p></li>
<li><p>There must be at least one output or input-output argument.</p></li>
<li><p>Only the CUDA backend is supported.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="generic-jax-ffi-callbacks">
<h4>Generic JAX FFI Callbacks<a class="headerlink" href="#generic-jax-ffi-callbacks" title="Link to this heading">#</a></h4>
<p>Another way to call Python functions is to use
<a class="reference internal" href="#warp.jax_experimental.ffi.register_ffi_callback" title="warp.jax_experimental.ffi.register_ffi_callback"><code class="xref py py-func docutils literal notranslate"><span class="pre">register_ffi_callback()</span></code></a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">warp.jax_experimental.ffi</span><span class="w"> </span><span class="kn">import</span> <span class="n">register_ffi_callback</span>
</pre></div>
</div>
<p>This allows calling functions that don’t have Warp-style type annotations, but must have the form:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">func</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">attrs</span><span class="p">,</span> <span class="n">ctx</span><span class="p">)</span>
</pre></div>
</div>
<p>where:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">inputs</span></code> is a list of input buffers.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">outputs</span></code> is a list of output buffers.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">attrs</span></code> is a dictionary of attributes.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ctx</span></code> is the execution context, including the CUDA stream.</p></li>
</ul>
<p>The input and output buffers are neither JAX nor Warp arrays.
They are objects that expose the <code class="docutils literal notranslate"><span class="pre">__cuda_array_interface__</span></code>, which can be passed to Warp kernels directly.
Here is an example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">warp.jax_experimental.ffi</span><span class="w"> </span><span class="kn">import</span> <span class="n">register_ffi_callback</span>

<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">scale_kernel</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span> <span class="n">s</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">output</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">output</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">*</span> <span class="n">s</span>

<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">scale_vec_kernel</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">vec2</span><span class="p">),</span> <span class="n">s</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">output</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">vec2</span><span class="p">)):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">output</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">*</span> <span class="n">s</span>

<span class="c1"># the Python function to call</span>
<span class="k">def</span><span class="w"> </span><span class="nf">warp_func</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">attrs</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
    <span class="c1"># input arrays</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># scalar attributes</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">attrs</span><span class="p">[</span><span class="s2">&quot;scale&quot;</span><span class="p">]</span>

    <span class="c1"># output arrays</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">device</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">device_from_jax</span><span class="p">(</span><span class="n">get_jax_device</span><span class="p">())</span>
    <span class="n">stream</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">Stream</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">cuda_stream</span><span class="o">=</span><span class="n">ctx</span><span class="o">.</span><span class="n">stream</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">wp</span><span class="o">.</span><span class="n">ScopedStream</span><span class="p">(</span><span class="n">stream</span><span class="p">):</span>
        <span class="c1"># launch with arrays of scalars</span>
        <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">scale_kernel</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">a</span><span class="p">,</span> <span class="n">s</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">c</span><span class="p">])</span>

        <span class="c1"># launch with arrays of vec2</span>
        <span class="c1"># NOTE: the input shapes are from JAX arrays, so we need to strip the inner dimension for vec2 arrays</span>
        <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">scale_vec_kernel</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">s</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">d</span><span class="p">])</span>

<span class="c1"># register callback</span>
<span class="n">register_ffi_callback</span><span class="p">(</span><span class="s2">&quot;warp_func&quot;</span><span class="p">,</span> <span class="n">warp_func</span><span class="p">)</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>

<span class="c1"># inputs</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">n</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>  <span class="c1"># array of wp.vec2</span>
<span class="n">s</span> <span class="o">=</span> <span class="mf">2.0</span>

<span class="c1"># set up the call</span>
<span class="n">out_types</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">jax</span><span class="o">.</span><span class="n">ShapeDtypeStruct</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
    <span class="n">jax</span><span class="o">.</span><span class="n">ShapeDtypeStruct</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>  <span class="c1"># array of wp.vec2</span>
<span class="p">]</span>
<span class="n">call</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">ffi</span><span class="o">.</span><span class="n">ffi_call</span><span class="p">(</span><span class="s2">&quot;warp_func&quot;</span><span class="p">,</span> <span class="n">out_types</span><span class="p">)</span>

<span class="c1"># call it</span>
<span class="n">c</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">call</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">s</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
</pre></div>
</div>
<p>This is a more low-level approach to JAX FFI callbacks.
A proposal was made to incorporate such a mechanism in JAX, but for now we have a prototype here.
This approach leaves a lot of work up to the user, such as verifying argument types and shapes,
but it can be used when other utilities like <a class="reference internal" href="#warp.jax_experimental.ffi.jax_kernel" title="warp.jax_experimental.ffi.jax_kernel"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax_kernel()</span></code></a> and
<a class="reference internal" href="#warp.jax_experimental.ffi.jax_callable" title="warp.jax_experimental.ffi.jax_callable"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax_callable()</span></code></a> are not sufficient.</p>
<p>See <a class="reference external" href="https://github.com/NVIDIA/warp/tree/main/warp/examples/interop/example_jax_ffi_callback.py">example_jax_ffi_callback.py</a> for examples.</p>
<dl class="py function">
<dt class="sig sig-object py" id="warp.jax_experimental.ffi.register_ffi_callback">
<span class="sig-prename descclassname"><span class="pre">warp.jax_experimental.ffi.</span></span><span class="sig-name descname"><span class="pre">register_ffi_callback</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">func</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">graph_compatible</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/jax_experimental/ffi.py#L780-L844"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.jax_experimental.ffi.register_ffi_callback" title="Link to this definition">#</a></dt>
<dd><p>Create a JAX callback from a Python function.</p>
<p>The Python function must have the form <code class="docutils literal notranslate"><span class="pre">func(inputs,</span> <span class="pre">outputs,</span> <span class="pre">attrs,</span> <span class="pre">ctx)</span></code>.</p>
<p>NOTE: This is an experimental feature under development.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – A unique FFI callback name.</p></li>
<li><p><strong>func</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.13)"><em>Callable</em></a>) – The Python function to call.</p></li>
<li><p><strong>graph_compatible</strong> (<a class="reference internal" href="functions.html#warp.bool" title="warp.bool"><em>bool</em></a>) – Optional. Whether the function can be called during CUDA graph capture.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="distributed-computation">
<h3>Distributed Computation<a class="headerlink" href="#distributed-computation" title="Link to this heading">#</a></h3>
<p>Warp can be used in conjunction with JAX’s <a class="reference external" href="https://jax.readthedocs.io/en/latest/jep/14273-shard-map.html">shard_map</a>
to perform distributed multi-GPU computations.</p>
<p>To achieve this, the JAX distributed environment must be initialized
(see <a class="reference external" href="https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html">Distributed Arrays and Automatic Parallelization</a>
for more details):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">jax</span>
<span class="n">jax</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">initialize</span><span class="p">()</span>
</pre></div>
</div>
<p>This initialization must be called at the beginning of your program, before any other JAX operations.</p>
<p>Here’s an example of how to use <code class="docutils literal notranslate"><span class="pre">shard_map</span></code> with a Warp kernel:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">warp</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">wp</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jnp</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">jax.sharding</span><span class="w"> </span><span class="kn">import</span> <span class="n">PartitionSpec</span> <span class="k">as</span> <span class="n">P</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">jax.experimental.multihost_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">process_allgather</span> <span class="k">as</span> <span class="n">allgather</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">jax.experimental.shard_map</span><span class="w"> </span><span class="kn">import</span> <span class="n">shard_map</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">warp.jax_experimental</span><span class="w"> </span><span class="kn">import</span> <span class="n">jax_kernel</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># Initialize JAX distributed environment</span>
<span class="n">jax</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">initialize</span><span class="p">()</span>
<span class="n">num_gpus</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>

<span class="k">def</span><span class="w"> </span><span class="nf">print_on_process_0</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">jax</span><span class="o">.</span><span class="n">process_index</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="n">print_on_process_0</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Running on </span><span class="si">{</span><span class="n">num_gpus</span><span class="si">}</span><span class="s2"> GPU(s)&quot;</span><span class="p">)</span>

<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">multiply_by_two_kernel</span><span class="p">(</span>
    <span class="n">a_in</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
    <span class="n">a_out</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
<span class="p">):</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">a_out</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">a_in</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">*</span> <span class="mf">2.0</span>

<span class="n">jax_warp_multiply</span> <span class="o">=</span> <span class="n">jax_kernel</span><span class="p">(</span><span class="n">multiply_by_two_kernel</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">warp_multiply</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">jax_warp_multiply</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>

    <span class="c1"># a_in here is the full sharded array with shape (M,)</span>
    <span class="c1"># The output will also be a sharded array with shape (M,)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">warp_distributed_operator</span><span class="p">(</span><span class="n">a_in</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_sharded_operator</span><span class="p">(</span><span class="n">a_in</span><span class="p">):</span>
        <span class="c1"># Inside the sharded operator, a_in is a local shard on each device</span>
        <span class="c1"># If we have N devices and input size M, each shard has shape (M/N,)</span>

        <span class="c1"># warp_multiply applies the Warp kernel to the local shard</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">warp_multiply</span><span class="p">(</span><span class="n">a_in</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># result has the same shape as the input shard (M/N,)</span>
        <span class="k">return</span> <span class="n">result</span>

    <span class="c1"># shard_map distributes the computation across devices</span>
    <span class="k">return</span> <span class="n">shard_map</span><span class="p">(</span>
        <span class="n">_sharded_operator</span><span class="p">,</span>
        <span class="n">mesh</span><span class="o">=</span><span class="n">jax</span><span class="o">.</span><span class="n">sharding</span><span class="o">.</span><span class="n">Mesh</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">devices</span><span class="p">()),</span> <span class="s2">&quot;x&quot;</span><span class="p">),</span>
        <span class="n">in_specs</span><span class="o">=</span><span class="p">(</span><span class="n">P</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">),),</span>  <span class="c1"># Input is sharded along the &#39;x&#39; axis</span>
        <span class="n">out_specs</span><span class="o">=</span><span class="n">P</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">),</span>    <span class="c1"># Output is also sharded along the &#39;x&#39; axis</span>
        <span class="n">check_rep</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)(</span><span class="n">a_in</span><span class="p">)</span>

<span class="n">print_on_process_0</span><span class="p">(</span><span class="s2">&quot;Test distributed multiplication using JAX + Warp&quot;</span><span class="p">)</span>

<span class="n">devices</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">devices</span><span class="p">()</span>
<span class="n">mesh</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">sharding</span><span class="o">.</span><span class="n">Mesh</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">devices</span><span class="p">),</span> <span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">sharding_spec</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">sharding</span><span class="o">.</span><span class="n">NamedSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">P</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">))</span>

<span class="n">input_size</span> <span class="o">=</span> <span class="n">num_gpus</span> <span class="o">*</span> <span class="mi">5</span>  <span class="c1"># 5 elements per device</span>
<span class="n">single_device_arrays</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># Define the shape of the input array based on the total input size</span>
<span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_size</span><span class="p">,)</span>

<span class="c1"># Create a list of arrays by distributing the single_device_arrays across the available devices</span>
<span class="c1"># Each device will receive a portion of the input data</span>
<span class="n">arrays</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">jax</span><span class="o">.</span><span class="n">device_put</span><span class="p">(</span><span class="n">single_device_arrays</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">d</span><span class="p">)</span>  <span class="c1"># Place each element on the corresponding device</span>
    <span class="k">for</span> <span class="n">d</span><span class="p">,</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">sharding_spec</span><span class="o">.</span><span class="n">addressable_devices_indices_map</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
<span class="p">]</span>

<span class="c1"># Combine the individual device arrays into a single sharded array</span>
<span class="n">sharded_array</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">make_array_from_single_device_arrays</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">sharding_spec</span><span class="p">,</span> <span class="n">arrays</span><span class="p">)</span>

<span class="c1"># sharded_array has shape (input_size,) but is distributed across devices</span>
<span class="n">print_on_process_0</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input array: </span><span class="si">{</span><span class="n">allgather</span><span class="p">(</span><span class="n">sharded_array</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># warp_result has the same shape and sharding as sharded_array</span>
<span class="n">warp_result</span> <span class="o">=</span> <span class="n">warp_distributed_operator</span><span class="p">(</span><span class="n">sharded_array</span><span class="p">)</span>

<span class="c1"># allgather collects results from all devices, resulting in a full array of shape (input_size,)</span>
<span class="n">print_on_process_0</span><span class="p">(</span><span class="s2">&quot;Warp Output:&quot;</span><span class="p">,</span> <span class="n">allgather</span><span class="p">(</span><span class="n">warp_result</span><span class="p">))</span>
</pre></div>
</div>
<p>In this example, <code class="docutils literal notranslate"><span class="pre">shard_map</span></code> is used to distribute the computation across available devices.
The input array <code class="docutils literal notranslate"><span class="pre">a_in</span></code> is sharded along the ‘x’ axis, and each device processes its local shard.
The Warp kernel <code class="docutils literal notranslate"><span class="pre">multiply_by_two_kernel</span></code> is applied to each shard, and the results are combined to form the final output.</p>
<p>This approach allows for efficient parallel processing of large arrays, as each device works on a portion of the data simultaneously.</p>
<p>To run this program on multiple GPUs, you must have Open MPI installed.
You can consult the <a class="reference external" href="https://docs.open-mpi.org/en/main/installing-open-mpi/quickstart.html">OpenMPI installation guide</a>
for instructions on how to install it.
Once Open MPI is installed, you can use <code class="docutils literal notranslate"><span class="pre">mpirun</span></code> with the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mpirun<span class="w"> </span>-np<span class="w"> </span>&lt;NUM_OF_GPUS&gt;<span class="w"> </span>python<span class="w"> </span>&lt;filename&gt;.py
</pre></div>
</div>
</section>
</section>
<section id="dlpack">
<span id="id3"></span><h2>DLPack<a class="headerlink" href="#dlpack" title="Link to this heading">#</a></h2>
<p>Warp supports the DLPack protocol included in the Python Array API standard v2022.12.
See the <a class="reference external" href="https://dmlc.github.io/dlpack/latest/python_spec.html">Python Specification for DLPack</a> for reference.</p>
<p>The canonical way to import an external array into Warp is using the <a class="reference internal" href="#warp.from_dlpack" title="warp.from_dlpack"><code class="xref py py-func docutils literal notranslate"><span class="pre">warp.from_dlpack()</span></code></a> function:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">warp_array</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">external_array</span><span class="p">)</span>
</pre></div>
</div>
<p>The external array can be a PyTorch tensor, Jax array, or any other array type compatible with this version of the DLPack protocol.
For CUDA arrays, this approach requires the producer to perform stream synchronization which ensures that operations on the array
are ordered correctly.  The <a class="reference internal" href="#warp.from_dlpack" title="warp.from_dlpack"><code class="xref py py-func docutils literal notranslate"><span class="pre">warp.from_dlpack()</span></code></a> function asks the producer to synchronize the current Warp stream on the device where
the array resides.  Thus it should be safe to use the array in Warp kernels on that device without any additional synchronization.</p>
<p>The canonical way to export a Warp array to an external framework is to use the <code class="docutils literal notranslate"><span class="pre">from_dlpack()</span></code> function in that framework:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">jax_array</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">dlpack</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">warp_array</span><span class="p">)</span>
<span class="n">torch_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">dlpack</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">warp_array</span><span class="p">)</span>
<span class="n">paddle_tensor</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">dlpack</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">warp_array</span><span class="p">)</span>
</pre></div>
</div>
<p>For CUDA arrays, this will synchronize the current stream of the consumer framework with the current Warp stream on the array’s device.
Thus it should be safe to use the wrapped array in the consumer framework, even if the array was previously used in a Warp kernel
on the device.</p>
<p>Alternatively, arrays can be shared by explicitly creating PyCapsules using a <code class="docutils literal notranslate"><span class="pre">to_dlpack()</span></code> function provided by the producer framework.
This approach may be used for older versions of frameworks that do not support the v2022.12 standard:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">warp_array1</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">jax_array</span><span class="p">)</span>
<span class="n">warp_array2</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">dlpack</span><span class="o">.</span><span class="n">to_dlpack</span><span class="p">(</span><span class="n">torch_tensor</span><span class="p">))</span>
<span class="n">warp_array3</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">paddle</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">dlpack</span><span class="o">.</span><span class="n">to_dlpack</span><span class="p">(</span><span class="n">paddle_tensor</span><span class="p">))</span>

<span class="n">jax_array</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">dlpack</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">wp</span><span class="o">.</span><span class="n">to_dlpack</span><span class="p">(</span><span class="n">warp_array</span><span class="p">))</span>
<span class="n">torch_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">dlpack</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">wp</span><span class="o">.</span><span class="n">to_dlpack</span><span class="p">(</span><span class="n">warp_array</span><span class="p">))</span>
<span class="n">paddle_tensor</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">dlpack</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">wp</span><span class="o">.</span><span class="n">to_dlpack</span><span class="p">(</span><span class="n">warp_array</span><span class="p">))</span>
</pre></div>
</div>
<p>This approach is generally faster because it skips any stream synchronization, but another solution must be used to ensure correct
ordering of operations.  In situations where no synchronization is required, using this approach can yield better performance.
This may be a good choice in situations like these:</p>
<ul class="simple">
<li><p>The external framework is using the synchronous CUDA default stream.</p></li>
<li><p>Warp and the external framework are using the same CUDA stream.</p></li>
<li><p>Another synchronization mechanism is already in place.</p></li>
</ul>
<dl class="py function">
<dt class="sig sig-object py" id="warp.from_dlpack">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">from_dlpack</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">source</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/dlpack.py#L425-L463"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.from_dlpack" title="Link to this definition">#</a></dt>
<dd><p>Convert a source array or DLPack capsule into a Warp array without copying.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>source</strong> – A DLPack-compatible array or PyCapsule</p></li>
<li><p><strong>dtype</strong> – An optional Warp data type to interpret the source data.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A new Warp array that uses the same underlying memory as the input
pycapsule.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="runtime.html#warp.array" title="warp.types.array"><em>array</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.to_dlpack">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">to_dlpack</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">wp_array</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/dlpack.py#L224-L314"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.to_dlpack" title="Link to this definition">#</a></dt>
<dd><p>Convert a Warp array to another type of DLPack-compatible array.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>wp_array</strong> (<a class="reference internal" href="runtime.html#warp.array" title="warp.types.array"><em>array</em></a>) – The source Warp array that will be converted.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A capsule containing a DLManagedTensor that can be converted
to another array type without copying the underlying memory.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="paddle">
<span id="paddle-interop"></span><h2>Paddle<a class="headerlink" href="#paddle" title="Link to this heading">#</a></h2>
<p>Warp provides helper functions to convert arrays to/from Paddle:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

<span class="c1"># convert to Paddle tensor</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">to_paddle</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>

<span class="c1"># convert from Paddle tensor</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_paddle</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</pre></div>
</div>
<p>These helper functions allow the conversion of Warp arrays to/from Paddle tensors without copying the underlying data.
At the same time, if available, gradient arrays and tensors are converted to/from Paddle autograd tensors, allowing the use of Warp arrays
in Paddle autograd computations.</p>
<dl class="py function">
<dt class="sig sig-object py" id="warp.from_paddle">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">from_paddle</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">t</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">return_ctype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/paddle.py#L219-L340"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.from_paddle" title="Link to this definition">#</a></dt>
<dd><p>Convert a Paddle tensor to a Warp array without copying the data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>t</strong> (<em>paddle.Tensor</em>) – The paddle tensor to wrap.</p></li>
<li><p><strong>dtype</strong> (<em>warp.dtype</em><em>, </em><em>optional</em>) – The target data type of the resulting Warp array. Defaults to the tensor value type mapped to a Warp array value type.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference internal" href="functions.html#warp.bool" title="warp.bool"><em>bool</em></a><em>, </em><em>optional</em>) – Whether the resulting array should wrap the tensor’s gradient, if it exists (the grad tensor will be allocated otherwise). Defaults to the tensor’s <cite>requires_grad</cite> value.</p></li>
<li><p><strong>grad</strong> (<em>paddle.Tensor</em><em>, </em><em>optional</em>) – The grad attached to given tensor. Defaults to None.</p></li>
<li><p><strong>return_ctype</strong> (<a class="reference internal" href="functions.html#warp.bool" title="warp.bool"><em>bool</em></a><em>, </em><em>optional</em>) – Whether to return a low-level array descriptor instead of a <code class="docutils literal notranslate"><span class="pre">wp.array</span></code> object (faster).  The descriptor can be passed to Warp kernels.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The wrapped array or array descriptor.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="runtime.html#warp.array" title="warp.array">warp.array</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.to_paddle">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">to_paddle</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">a</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/paddle.py#L342-L387"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.to_paddle" title="Link to this definition">#</a></dt>
<dd><p>Convert a Warp array to a Paddle tensor without copying the data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>a</strong> (<a class="reference internal" href="runtime.html#warp.array" title="warp.array"><em>warp.array</em></a>) – The Warp array to convert.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference internal" href="functions.html#warp.bool" title="warp.bool"><em>bool</em></a><em>, </em><em>optional</em>) – Whether the resulting tensor should convert the array’s gradient, if it exists, to a grad tensor. Defaults to the array’s <cite>requires_grad</cite> value.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The converted tensor.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>paddle.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.device_from_paddle">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">device_from_paddle</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">paddle_device</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/paddle.py#L32-L81"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.device_from_paddle" title="Link to this definition">#</a></dt>
<dd><p>Return the Warp device corresponding to a Paddle device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>paddle_device</strong> (<cite>Place</cite>, <cite>CPUPlace</cite>, <cite>CUDAPinnedPlace</cite>, <cite>CUDAPlace</cite>, or <cite>str</cite>) – Paddle device identifier</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#RuntimeError" title="(in Python v3.13)"><strong>RuntimeError</strong></a> – Paddle device does not have a corresponding Warp device</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="devices.html#warp.context.Device" title="warp.context.Device">warp.context.Device</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.device_to_paddle">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">device_to_paddle</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">warp_device</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/paddle.py#L83-L99"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.device_to_paddle" title="Link to this definition">#</a></dt>
<dd><p>Return the Paddle device string corresponding to a Warp device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>warp_device</strong> (<a class="reference internal" href="devices.html#warp.context.Device" title="warp.context.Device"><em>Device</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> | </em><em>None</em>) – An identifier that can be resolved to a <a class="reference internal" href="devices.html#warp.context.Device" title="warp.context.Device"><code class="xref py py-class docutils literal notranslate"><span class="pre">warp.context.Device</span></code></a>.</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#RuntimeError" title="(in Python v3.13)"><strong>RuntimeError</strong></a> – The Warp device is not compatible with PyPaddle.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)">str</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.dtype_from_paddle">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">dtype_from_paddle</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">paddle_dtype</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/paddle.py#L138-L175"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.dtype_from_paddle" title="Link to this definition">#</a></dt>
<dd><p>Return the Warp dtype corresponding to a Paddle dtype.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>paddle_dtype</strong> – A <code class="docutils literal notranslate"><span class="pre">paddle.dtype</span></code> that has a corresponding Warp data type.
Currently <code class="docutils literal notranslate"><span class="pre">paddle.bfloat16</span></code>, <code class="docutils literal notranslate"><span class="pre">paddle.complex64</span></code>, and
<code class="docutils literal notranslate"><span class="pre">paddle.complex128</span></code> are not supported.</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.13)"><strong>TypeError</strong></a> – Unable to find a corresponding Warp data type.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.dtype_to_paddle">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">dtype_to_paddle</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">warp_dtype</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/paddle.py#L101-L136"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.dtype_to_paddle" title="Link to this definition">#</a></dt>
<dd><p>Return the Paddle dtype corresponding to a Warp dtype.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>warp_dtype</strong> – A Warp data type that has a corresponding <code class="docutils literal notranslate"><span class="pre">paddle.dtype</span></code>.
<code class="docutils literal notranslate"><span class="pre">warp.uint16</span></code>, <code class="docutils literal notranslate"><span class="pre">warp.uint32</span></code>, and <code class="docutils literal notranslate"><span class="pre">warp.uint64</span></code> are mapped
to the signed integer <code class="docutils literal notranslate"><span class="pre">paddle.dtype</span></code> of the same width.</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.13)"><strong>TypeError</strong></a> – Unable to find a corresponding PyPaddle data type.</p>
</dd>
</dl>
</dd></dl>

<p>To convert a Paddle CUDA stream to a Warp CUDA stream and vice versa, Warp provides the following function:</p>
<dl class="py function">
<dt class="sig sig-object py" id="warp.stream_from_paddle">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">stream_from_paddle</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stream_or_device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/paddle.py#L389-L407"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.stream_from_paddle" title="Link to this definition">#</a></dt>
<dd><p>Convert from a Paddle CUDA stream to a Warp CUDA stream.</p>
</dd></dl>

<section id="example-optimization-using-warp-from-paddle">
<h3>Example: Optimization using <code class="docutils literal notranslate"><span class="pre">warp.from_paddle()</span></code><a class="headerlink" href="#example-optimization-using-warp-from-paddle" title="Link to this heading">#</a></h3>
<p>An example usage of minimizing a loss function over an array of 2D points written in Warp via Paddle’s Adam optimizer
using <a class="reference internal" href="#warp.from_paddle" title="warp.from_paddle"><code class="xref py py-func docutils literal notranslate"><span class="pre">warp.from_paddle()</span></code></a> is as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">warp</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">wp</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">paddle</span>

<span class="c1"># init warp context at beginning</span>
<span class="n">wp</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>

<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span><span class="p">()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">loss</span><span class="p">(</span><span class="n">xs</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">ndim</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">l</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">atomic_add</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">xs</span><span class="p">[</span><span class="n">tid</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">**</span> <span class="mf">2.0</span> <span class="o">+</span> <span class="n">xs</span><span class="p">[</span><span class="n">tid</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="mf">2.0</span><span class="p">)</span>

<span class="c1"># indicate requires_grad so that Warp can accumulate gradients in the grad buffers</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">randn</span><span class="p">([</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">xs</span><span class="o">.</span><span class="n">stop_gradient</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">l</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
<span class="n">l</span><span class="o">.</span><span class="n">stop_gradient</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">parameters</span><span class="o">=</span><span class="p">[</span><span class="n">xs</span><span class="p">])</span>

<span class="n">wp_xs</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_paddle</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
<span class="n">wp_l</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_paddle</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>

<span class="n">tape</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">Tape</span><span class="p">()</span>
<span class="k">with</span> <span class="n">tape</span><span class="p">:</span>
    <span class="c1"># record the loss function kernel launch on the tape</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">wp_xs</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">wp_l</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">wp_xs</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="n">tape</span><span class="o">.</span><span class="n">zero</span><span class="p">()</span>
    <span class="n">tape</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">wp_l</span><span class="p">)</span>  <span class="c1"># compute gradients</span>
    <span class="c1"># now xs.grad will be populated with the gradients computed by Warp</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># update xs (and thereby wp_xs)</span>

    <span class="c1"># these lines are only needed for evaluating the loss</span>
    <span class="c1"># (the optimization just needs the gradient, not the loss value)</span>
    <span class="n">wp_l</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">wp_xs</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">wp_l</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">wp_xs</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="se">\t</span><span class="s2">loss: </span><span class="si">{</span><span class="n">l</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="example-optimization-using-warp-to-paddle">
<h3>Example: Optimization using <code class="docutils literal notranslate"><span class="pre">warp.to_paddle</span></code><a class="headerlink" href="#example-optimization-using-warp-to-paddle" title="Link to this heading">#</a></h3>
<p>Less code is needed when we declare the optimization variables directly in Warp and use <a class="reference internal" href="#warp.to_paddle" title="warp.to_paddle"><code class="xref py py-func docutils literal notranslate"><span class="pre">warp.to_paddle()</span></code></a> to convert them to Paddle tensors.
Here, we revisit the same example from above where now only a single conversion to a Paddle tensor is needed to supply Adam with the optimization variables:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">warp</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">wp</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">paddle</span>

<span class="c1"># init warp context at beginning</span>
<span class="n">wp</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>

<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span><span class="p">()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">loss</span><span class="p">(</span><span class="n">xs</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">ndim</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">l</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">atomic_add</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">xs</span><span class="p">[</span><span class="n">tid</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">**</span> <span class="mf">2.0</span> <span class="o">+</span> <span class="n">xs</span><span class="p">[</span><span class="n">tid</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="mf">2.0</span><span class="p">)</span>

<span class="c1"># initialize the optimization variables in Warp</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">l</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># just a single wp.to_paddle call is needed, Adam optimizes using the Warp array gradients</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">parameters</span><span class="o">=</span><span class="p">[</span><span class="n">wp</span><span class="o">.</span><span class="n">to_paddle</span><span class="p">(</span><span class="n">xs</span><span class="p">)])</span>

<span class="n">tape</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">Tape</span><span class="p">()</span>
<span class="k">with</span> <span class="n">tape</span><span class="p">:</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">xs</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">xs</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="n">tape</span><span class="o">.</span><span class="n">zero</span><span class="p">()</span>
    <span class="n">tape</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">l</span><span class="p">)</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="n">l</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">xs</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">xs</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="se">\t</span><span class="s2">loss: </span><span class="si">{</span><span class="n">l</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id4">
<h3>Performance Notes<a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<p>The <a class="reference internal" href="#warp.from_paddle" title="warp.from_paddle"><code class="xref py py-func docutils literal notranslate"><span class="pre">wp.from_paddle()</span></code></a> function creates a Warp array object that shares data with a Paddle tensor.
Although this function does not copy the data, there is always some CPU overhead during the conversion.
If these conversions happen frequently, the overall program performance may suffer.
As a general rule, it’s good to avoid repeated conversions of the same tensor.
Instead of:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_t</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">device_to_paddle</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
<span class="n">y_t</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">n</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">device_to_paddle</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">x_w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_paddle</span><span class="p">(</span><span class="n">x_t</span><span class="p">)</span>
    <span class="n">y_w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_paddle</span><span class="p">(</span><span class="n">y_t</span><span class="p">)</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">saxpy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x_w</span><span class="p">,</span> <span class="n">y_w</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<p>Try converting the arrays only once and reuse them:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_t</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">device_to_paddle</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
<span class="n">y_t</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">n</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">device_to_paddle</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>

<span class="n">x_w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_paddle</span><span class="p">(</span><span class="n">x_t</span><span class="p">)</span>
<span class="n">y_w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_paddle</span><span class="p">(</span><span class="n">y_t</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">saxpy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x_w</span><span class="p">,</span> <span class="n">y_w</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<p>If reusing arrays is not possible (e.g., a new Paddle tensor is constructed on every iteration), passing <code class="docutils literal notranslate"><span class="pre">return_ctype=True</span></code> to
<a class="reference internal" href="#warp.from_paddle" title="warp.from_paddle"><code class="xref py py-func docutils literal notranslate"><span class="pre">wp.from_paddle()</span></code></a> should yield faster performance.
Setting this argument to <code class="docutils literal notranslate"><span class="pre">True</span></code> avoids constructing a <code class="docutils literal notranslate"><span class="pre">wp.array</span></code> object and instead returns a low-level array descriptor.
This descriptor is a simple C structure that can be passed to Warp kernels instead of a <code class="docutils literal notranslate"><span class="pre">wp.array</span></code>, but cannot be used in other places that require a <code class="docutils literal notranslate"><span class="pre">wp.array</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">):</span>
    <span class="c1"># get Paddle tensors for this iteration</span>
    <span class="n">x_t</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">device_to_paddle</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
    <span class="n">y_t</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">n</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">device_to_paddle</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>

    <span class="c1"># get Warp array descriptors</span>
    <span class="n">x_ctype</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_paddle</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">return_ctype</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">y_ctype</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_paddle</span><span class="p">(</span><span class="n">y_t</span><span class="p">,</span> <span class="n">return_ctype</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">saxpy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x_ctype</span><span class="p">,</span> <span class="n">y_ctype</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<p>An alternative approach is to pass the Paddle tensors to Warp kernels directly.
This avoids constructing temporary Warp arrays by leveraging standard array interfaces (like <code class="docutils literal notranslate"><span class="pre">__cuda_array_interface__</span></code>) supported by both Paddle and Warp.
The main advantage of this approach is convenience, since there is no need to call any conversion functions.
The main limitation is that it does not handle gradients, because gradient information is not included in the standard array interfaces.
This technique is therefore most suitable for algorithms that do not involve differentiation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">device_to_paddle</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">n</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">device_to_paddle</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">saxpy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>warp.examples.benchmarks.benchmark_interop_paddle
</pre></div>
</div>
<p>Sample output:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">13990</span> <span class="n">ms</span>  <span class="n">from_paddle</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
 <span class="mi">5990</span> <span class="n">ms</span>  <span class="n">from_paddle</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">return_ctype</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="mi">35167</span> <span class="n">ms</span>  <span class="n">direct</span> <span class="kn">from</span><span class="w"> </span><span class="nn">paddle</span>
</pre></div>
</div>
<p>The default <code class="docutils literal notranslate"><span class="pre">wp.from_paddle()</span></code> conversion is the slowest.
Passing <code class="docutils literal notranslate"><span class="pre">return_ctype=True</span></code> is the fastest, because it skips creating temporary Warp array objects.
Passing Paddle tensors to Warp kernels directly falls somewhere in between.
It skips creating temporary Warp arrays, but accessing the <code class="docutils literal notranslate"><span class="pre">__cuda_array_interface__</span></code> attributes of Paddle tensors adds overhead because they are initialized on-demand.</p>
</section>
</section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="tiles.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Tiles</p>
      </div>
    </a>
    <a class="right-next"
       href="../configuration.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Configuration</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            


              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#numpy">NumPy</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.from_numpy"><code class="docutils literal notranslate"><span class="pre">from_numpy()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.dtype_from_numpy"><code class="docutils literal notranslate"><span class="pre">dtype_from_numpy()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.dtype_to_numpy"><code class="docutils literal notranslate"><span class="pre">dtype_to_numpy()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch">PyTorch</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.from_torch"><code class="docutils literal notranslate"><span class="pre">from_torch()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.to_torch"><code class="docutils literal notranslate"><span class="pre">to_torch()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.device_from_torch"><code class="docutils literal notranslate"><span class="pre">device_from_torch()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.device_to_torch"><code class="docutils literal notranslate"><span class="pre">device_to_torch()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.dtype_from_torch"><code class="docutils literal notranslate"><span class="pre">dtype_from_torch()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.dtype_to_torch"><code class="docutils literal notranslate"><span class="pre">dtype_to_torch()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.stream_from_torch"><code class="docutils literal notranslate"><span class="pre">stream_from_torch()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.stream_to_torch"><code class="docutils literal notranslate"><span class="pre">stream_to_torch()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-optimization-using-warp-from-torch">Example: Optimization using <code class="docutils literal notranslate"><span class="pre">warp.from_torch()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-optimization-using-warp-to-torch">Example: Optimization using <code class="docutils literal notranslate"><span class="pre">warp.to_torch</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-optimization-using-torch-autograd-function-pytorch-2-3-1">Example: Optimization using <code class="docutils literal notranslate"><span class="pre">torch.autograd.function</span></code> (PyTorch &lt;= 2.3.1)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-optimization-using-pytorch-custom-operators-pytorch-2-4-0">Example: Optimization using PyTorch custom operators (PyTorch &gt;= 2.4.0)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-notes">Performance Notes</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cupy-numba">CuPy/Numba</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#jax">JAX</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.from_jax"><code class="docutils literal notranslate"><span class="pre">from_jax()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.to_jax"><code class="docutils literal notranslate"><span class="pre">to_jax()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.device_from_jax"><code class="docutils literal notranslate"><span class="pre">device_from_jax()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.device_to_jax"><code class="docutils literal notranslate"><span class="pre">device_to_jax()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.dtype_from_jax"><code class="docutils literal notranslate"><span class="pre">dtype_from_jax()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.dtype_to_jax"><code class="docutils literal notranslate"><span class="pre">dtype_to_jax()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-warp-kernels-as-jax-primitives">Using Warp kernels as JAX primitives</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.jax_experimental.jax_kernel"><code class="docutils literal notranslate"><span class="pre">jax_kernel()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#input-and-output-semantics">Input and Output Semantics</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-launch-and-output-dimensions">Kernel Launch and Output Dimensions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#jax-foreign-function-interface-ffi">JAX Foreign Function Interface (FFI)</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.jax_experimental.ffi.jax_kernel"><code class="docutils literal notranslate"><span class="pre">jax_kernel()</span></code></a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Input and Output Semantics</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#scalar-inputs">Scalar Inputs</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Kernel Launch and Output Dimensions</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-and-matrix-arrays">Vector and Matrix Arrays</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#jax-vmap-support">JAX VMAP Support</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#calling-annotated-python-functions">Calling Annotated Python Functions</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.jax_experimental.ffi.jax_callable"><code class="docutils literal notranslate"><span class="pre">jax_callable()</span></code></a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#generic-jax-ffi-callbacks">Generic JAX FFI Callbacks</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.jax_experimental.ffi.register_ffi_callback"><code class="docutils literal notranslate"><span class="pre">register_ffi_callback()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distributed-computation">Distributed Computation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dlpack">DLPack</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.from_dlpack"><code class="docutils literal notranslate"><span class="pre">from_dlpack()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.to_dlpack"><code class="docutils literal notranslate"><span class="pre">to_dlpack()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#paddle">Paddle</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.from_paddle"><code class="docutils literal notranslate"><span class="pre">from_paddle()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.to_paddle"><code class="docutils literal notranslate"><span class="pre">to_paddle()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.device_from_paddle"><code class="docutils literal notranslate"><span class="pre">device_from_paddle()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.device_to_paddle"><code class="docutils literal notranslate"><span class="pre">device_to_paddle()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.dtype_from_paddle"><code class="docutils literal notranslate"><span class="pre">dtype_from_paddle()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.dtype_to_paddle"><code class="docutils literal notranslate"><span class="pre">dtype_to_paddle()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.stream_from_paddle"><code class="docutils literal notranslate"><span class="pre">stream_from_paddle()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-optimization-using-warp-from-paddle">Example: Optimization using <code class="docutils literal notranslate"><span class="pre">warp.from_paddle()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-optimization-using-warp-to-paddle">Example: Optimization using <code class="docutils literal notranslate"><span class="pre">warp.to_paddle</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Performance Notes</a></li>
</ul>
</li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">

  
  <div class="tocsection editthispage">
    <a href="https://github.com/NVIDIA/warp/edit/v1.9.1/docs/modules/interoperability.rst">
      <i class="fa-solid fa-pencil"></i>
      
      
        
          Edit on GitHub
        
      
    </a>
  </div>
</div>

</div></div>
              
            

          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">
<a class="footer-brand logo" href="https://www.nvidia.com">
  <img src="../_static/nvidia-logo-horiz-rgb-1c-blk-for-screen.svg" class="logo__image only-light" alt="NVIDIA"/>
  <img src="../_static/nvidia-logo-horiz-rgb-1c-wht-for-screen.svg" class="logo__image only-dark" alt="NVIDIA"/>
</a></div>
      
        <div class="footer-item">




  <p class="copyright">
    
      Copyright © 2022-2025 NVIDIA Corporation.
      <br/>
    
  </p>
</div>
      
    </div>
  
  
  
</div>

  </footer>
  </body>
</html>