

<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Devices &#8212; Warp 1.9.1</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/nvidia-sphinx-theme.css?v=df3ac72c" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=8524295a" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=60b3a732"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=fd10adb8"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'modules/devices';</script>
    <link rel="icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Differentiability" href="differentiability.html" />
    <link rel="prev" title="Basics" href="../basics.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="1.9.1" />


  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/nvidia-logo-horiz-rgb-blk-for-screen.svg" class="logo__image only-light" alt="Warp 1.9.1 - Home"/>
    <img src="../_static/nvidia-logo-horiz-rgb-wht-for-screen.svg" class="logo__image only-dark pst-js-only" alt="Warp 1.9.1 - Home"/>
  
  
    <p class="title logo__title">Warp 1.9.1</p>
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/NVIDIA/warp" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-square-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/warp-lang" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        



  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/nvidia-logo-horiz-rgb-blk-for-screen.svg" class="logo__image only-light" alt="Warp 1.9.1 - Home"/>
    <img src="../_static/nvidia-logo-horiz-rgb-wht-for-screen.svg" class="logo__image only-dark pst-js-only" alt="Warp 1.9.1 - Home"/>
  
  
    <p class="title logo__title">Warp 1.9.1</p>
  
</a>


  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/NVIDIA/warp" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-square-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/warp-lang" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">



<nav class="bd-docs-nav bd-links"
     aria-label="Table of Contents">
  <p class="bd-links__title" role="heading" aria-level="1">Table of Contents</p>
  <div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">User's Guide</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basics.html">Basics</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Devices</a></li>
<li class="toctree-l1"><a class="reference internal" href="differentiability.html">Differentiability</a></li>
<li class="toctree-l1"><a class="reference internal" href="generics.html">Generics</a></li>
<li class="toctree-l1"><a class="reference internal" href="tiles.html">Tiles</a></li>
<li class="toctree-l1"><a class="reference internal" href="interoperability.html">Interoperability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debugging.html">Debugging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../limitations.html">Limitations</a></li>
<li class="toctree-l1"><a class="reference internal" href="contribution_guide.html">Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../publications.html">Publications using Warp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changelog.html">Changelog</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Topics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../codegen.html">Code Generation</a></li>
<li class="toctree-l1"><a class="reference internal" href="allocators.html">Allocators</a></li>
<li class="toctree-l1"><a class="reference internal" href="concurrency.html">Concurrency</a></li>
<li class="toctree-l1"><a class="reference internal" href="../profiling.html">Profiling</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Core Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="runtime.html">Python Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="functions.html">Built-Ins Reference</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Simulation Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="sim.html">warp.sim</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse.html">warp.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="fem.html">warp.fem</a></li>
<li class="toctree-l1"><a class="reference internal" href="render.html">warp.render</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Project Links</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://github.com/NVIDIA/warp">GitHub</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pypi.org/project/warp-lang">PyPI</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>



      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">Devices</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="devices">
<h1>Devices<a class="headerlink" href="#devices" title="Link to this heading">#</a></h1>
<p>Warp assigns unique string aliases to all supported compute devices in the system.  There is currently a single CPU device exposed as <code class="docutils literal notranslate"><span class="pre">&quot;cpu&quot;</span></code>.  Each CUDA-capable GPU gets an alias of the form <code class="docutils literal notranslate"><span class="pre">&quot;cuda:i&quot;</span></code>, where <code class="docutils literal notranslate"><span class="pre">i</span></code> is the CUDA device ordinal.  This convention should be familiar to users of other popular frameworks like PyTorch.</p>
<p>It is possible to explicitly target a specific device with each Warp API call using the <code class="docutils literal notranslate"><span class="pre">device</span></code> argument:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">a</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">)</span>
<span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">b</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">b</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">)</span>

<span class="n">c</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda:1&quot;</span><span class="p">)</span>
<span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">c</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">c</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda:1&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A Warp CUDA device (<code class="docutils literal notranslate"><span class="pre">&quot;cuda:i&quot;</span></code>) corresponds to the primary CUDA context of device <code class="docutils literal notranslate"><span class="pre">i</span></code>.
This is compatible with frameworks like PyTorch and other software that uses the CUDA Runtime API.
It makes interoperability easy because GPU resources like memory can be shared with Warp.</p>
</div>
<dl class="py class">
<dt class="sig sig-object py" id="warp.context.Device">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">warp.context.</span></span><span class="sig-name descname"><span class="pre">Device</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">runtime</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alias</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ordinal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_primary</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">context</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/context.py#L2996-L3375"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.context.Device" title="Link to this definition">#</a></dt>
<dd><p>A device to allocate Warp arrays and to launch kernels on.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="warp.context.Device.ordinal">
<span class="sig-name descname"><span class="pre">ordinal</span></span><a class="headerlink" href="#warp.context.Device.ordinal" title="Link to this definition">#</a></dt>
<dd><p>A Warp-specific label for the device. <code class="docutils literal notranslate"><span class="pre">-1</span></code> for CPU devices.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)">int</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="warp.context.Device.name">
<span class="sig-name descname"><span class="pre">name</span></span><a class="headerlink" href="#warp.context.Device.name" title="Link to this definition">#</a></dt>
<dd><p>A label for the device. By default, CPU devices will be named according to the processor name,
or <code class="docutils literal notranslate"><span class="pre">&quot;CPU&quot;</span></code> if the processor name cannot be determined.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)">str</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="warp.context.Device.arch">
<span class="sig-name descname"><span class="pre">arch</span></span><a class="headerlink" href="#warp.context.Device.arch" title="Link to this definition">#</a></dt>
<dd><p>The compute capability version number calculated as <code class="docutils literal notranslate"><span class="pre">10</span> <span class="pre">*</span> <span class="pre">major</span> <span class="pre">+</span> <span class="pre">minor</span></code>.
<code class="docutils literal notranslate"><span class="pre">0</span></code> for CPU devices.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)">int</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="warp.context.Device.sm_count">
<span class="sig-name descname"><span class="pre">sm_count</span></span><a class="headerlink" href="#warp.context.Device.sm_count" title="Link to this definition">#</a></dt>
<dd><p>The number of streaming multiprocessors on the CUDA device.
<code class="docutils literal notranslate"><span class="pre">0</span></code> for CPU devices.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)">int</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="warp.context.Device.is_uva">
<span class="sig-name descname"><span class="pre">is_uva</span></span><a class="headerlink" href="#warp.context.Device.is_uva" title="Link to this definition">#</a></dt>
<dd><p>Indicates whether the device supports unified addressing.
<code class="docutils literal notranslate"><span class="pre">False</span></code> for CPU devices.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="functions.html#warp.bool" title="warp.bool">bool</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="warp.context.Device.is_cubin_supported">
<span class="sig-name descname"><span class="pre">is_cubin_supported</span></span><a class="headerlink" href="#warp.context.Device.is_cubin_supported" title="Link to this definition">#</a></dt>
<dd><p>Indicates whether Warp’s version of NVRTC can directly
generate CUDA binary files (cubin) for this device’s architecture. <code class="docutils literal notranslate"><span class="pre">False</span></code> for CPU devices.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="functions.html#warp.bool" title="warp.bool">bool</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="warp.context.Device.is_mempool_supported">
<span class="sig-name descname"><span class="pre">is_mempool_supported</span></span><a class="headerlink" href="#warp.context.Device.is_mempool_supported" title="Link to this definition">#</a></dt>
<dd><p>Indicates whether the device supports using the <code class="docutils literal notranslate"><span class="pre">cuMemAllocAsync</span></code> and
<code class="docutils literal notranslate"><span class="pre">cuMemPool</span></code> family of APIs for stream-ordered memory allocations. <code class="docutils literal notranslate"><span class="pre">False</span></code> for CPU devices.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="functions.html#warp.bool" title="warp.bool">bool</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="warp.context.Device.is_ipc_supported">
<span class="sig-name descname"><span class="pre">is_ipc_supported</span></span><a class="headerlink" href="#warp.context.Device.is_ipc_supported" title="Link to this definition">#</a></dt>
<dd><p>Indicates whether the device supports IPC.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">True</span></code> if supported.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">False</span></code> if not supported.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> if IPC support could not be determined (e.g. CUDA 11).</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Optional[<a class="reference internal" href="functions.html#warp.bool" title="warp.bool">bool</a>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="warp.context.Device.is_primary">
<span class="sig-name descname"><span class="pre">is_primary</span></span><a class="headerlink" href="#warp.context.Device.is_primary" title="Link to this definition">#</a></dt>
<dd><p>Indicates whether this device’s CUDA context is also the device’s primary context.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="functions.html#warp.bool" title="warp.bool">bool</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="warp.context.Device.uuid">
<span class="sig-name descname"><span class="pre">uuid</span></span><a class="headerlink" href="#warp.context.Device.uuid" title="Link to this definition">#</a></dt>
<dd><p>The UUID of the CUDA device. The UUID is in the same format used by <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span> <span class="pre">-L</span></code>.
<code class="docutils literal notranslate"><span class="pre">None</span></code> for CPU devices.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)">str</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="warp.context.Device.pci_bus_id">
<span class="sig-name descname"><span class="pre">pci_bus_id</span></span><a class="headerlink" href="#warp.context.Device.pci_bus_id" title="Link to this definition">#</a></dt>
<dd><p>An identifier for the CUDA device in the format <code class="docutils literal notranslate"><span class="pre">[domain]:[bus]:[device]</span></code>, in which
<code class="docutils literal notranslate"><span class="pre">domain</span></code>, <code class="docutils literal notranslate"><span class="pre">bus</span></code>, and <code class="docutils literal notranslate"><span class="pre">device</span></code> are all hexadecimal values. <code class="docutils literal notranslate"><span class="pre">None</span></code> for CPU devices.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)">str</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="warp.context.Device.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">runtime</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">alias</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">ordinal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">is_primary</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">context</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/context.py#L3026-L3126"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.context.Device.__init__" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="warp.context.Device.get_allocator">
<span class="sig-name descname"><span class="pre">get_allocator</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pinned</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/context.py#L3127-L3141"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.context.Device.get_allocator" title="Link to this definition">#</a></dt>
<dd><p>Get the memory allocator for this device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>pinned</strong> (<a class="reference internal" href="functions.html#warp.bool" title="warp.bool"><em>bool</em></a>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, an allocator for pinned memory will be
returned. Only applicable when this device is a CPU device.</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="warp.context.Device.is_cpu">
<em class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">is_cpu</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="functions.html#warp.bool" title="warp.bool"><span class="pre">bool</span></a></em><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/context.py#L3150-L3154"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.context.Device.is_cpu" title="Link to this definition">#</a></dt>
<dd><p>A boolean indicating whether the device is a CPU device.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="warp.context.Device.is_cuda">
<em class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">is_cuda</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="functions.html#warp.bool" title="warp.bool"><span class="pre">bool</span></a></em><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/context.py#L3155-L3159"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.context.Device.is_cuda" title="Link to this definition">#</a></dt>
<dd><p>A boolean indicating whether the device is a CUDA device.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="warp.context.Device.is_capturing">
<em class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">is_capturing</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="functions.html#warp.bool" title="warp.bool"><span class="pre">bool</span></a></em><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/context.py#L3160-L3171"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.context.Device.is_capturing" title="Link to this definition">#</a></dt>
<dd><p>A boolean indicating whether this device’s default stream is currently capturing a graph.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="warp.context.Device.context">
<em class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">context</span></span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/context.py#L3172-L3189"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.context.Device.context" title="Link to this definition">#</a></dt>
<dd><p>The context associated with the device.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="warp.context.Device.has_context">
<em class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">has_context</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="functions.html#warp.bool" title="warp.bool"><span class="pre">bool</span></a></em><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/context.py#L3190-L3194"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.context.Device.has_context" title="Link to this definition">#</a></dt>
<dd><p>A boolean indicating whether the device has a CUDA context associated with it.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="warp.context.Device.stream">
<em class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">stream</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="runtime.html#warp.Stream" title="warp.context.Stream"><span class="pre">Stream</span></a></em><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/context.py#L3195-L3206"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.context.Device.stream" title="Link to this definition">#</a></dt>
<dd><p>The stream associated with a CUDA device.</p>
<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#RuntimeError" title="(in Python v3.13)"><strong>RuntimeError</strong></a> – The device is not a CUDA device.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="warp.context.Device.set_stream">
<span class="sig-name descname"><span class="pre">set_stream</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stream</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/context.py#L3211-L3235"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.context.Device.set_stream" title="Link to this definition">#</a></dt>
<dd><p>Set the current stream for this CUDA device.</p>
<p>The current stream will be used by default for all kernel launches and
memory operations on this device.</p>
<p>If this is an external stream, the caller is responsible for
guaranteeing the lifetime of the stream.</p>
<p>Consider using <a class="reference internal" href="runtime.html#warp.ScopedStream" title="warp.ScopedStream"><code class="xref py py-class docutils literal notranslate"><span class="pre">warp.ScopedStream</span></code></a> instead.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>stream</strong> (<a class="reference internal" href="runtime.html#warp.Stream" title="warp.context.Stream"><em>Stream</em></a>) – The stream to set as this device’s current stream.</p></li>
<li><p><strong>sync</strong> (<a class="reference internal" href="functions.html#warp.bool" title="warp.bool"><em>bool</em></a>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, then <code class="docutils literal notranslate"><span class="pre">stream</span></code> will perform a device-side
synchronization with the device’s previous current stream.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="warp.context.Device.has_stream">
<em class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">has_stream</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="functions.html#warp.bool" title="warp.bool"><span class="pre">bool</span></a></em><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/context.py#L3236-L3240"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.context.Device.has_stream" title="Link to this definition">#</a></dt>
<dd><p>A boolean indicating whether the device has a stream associated with it.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="warp.context.Device.total_memory">
<em class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">total_memory</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></em><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/context.py#L3241-L3254"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.context.Device.total_memory" title="Link to this definition">#</a></dt>
<dd><p>The total amount of device memory available in bytes.</p>
<p>This function is currently only implemented for CUDA devices. 0 will be returned if called on a CPU device.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="warp.context.Device.free_memory">
<em class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">free_memory</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></em><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/context.py#L3255-L3268"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.context.Device.free_memory" title="Link to this definition">#</a></dt>
<dd><p>The amount of memory on the device that is free according to the OS in bytes.</p>
<p>This function is currently only implemented for CUDA devices. 0 will be returned if called on a CPU device.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="warp.context.Device.make_current">
<span class="sig-name descname"><span class="pre">make_current</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/context.py#L3288-L3291"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.context.Device.make_current" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="warp.context.Device.can_access">
<span class="sig-name descname"><span class="pre">can_access</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/context.py#L3292-L3303"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.context.Device.can_access" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="warp.context.Device.get_cuda_output_format">
<span class="sig-name descname"><span class="pre">get_cuda_output_format</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">preferred_cuda_output</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/context.py#L3304-L3343"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.context.Device.get_cuda_output_format" title="Link to this definition">#</a></dt>
<dd><p>Determine the CUDA output format to use for this device.</p>
<p>This method is intended for internal use by Warp’s compilation system.
External users should not need to call this method directly.</p>
<p>It determines whether to use PTX or CUBIN output based on device capabilities,
caller preferences, and runtime constraints.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>preferred_cuda_output</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> | </em><em>None</em>) – Caller’s preferred format (<code class="docutils literal notranslate"><span class="pre">&quot;ptx&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;cubin&quot;</span></code>, or <code class="docutils literal notranslate"><span class="pre">None</span></code>).
If <code class="docutils literal notranslate"><span class="pre">None</span></code>, falls back to global config or automatic determination.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><code class="docutils literal notranslate"><span class="pre">&quot;ptx&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;cubin&quot;</span></code>, or <code class="docutils literal notranslate"><span class="pre">None</span></code> for CPU devices.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>The output format to use</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="warp.context.Device.get_cuda_compile_arch">
<span class="sig-name descname"><span class="pre">get_cuda_compile_arch</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/context.py#L3344-L3375"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.context.Device.get_cuda_compile_arch" title="Link to this definition">#</a></dt>
<dd><p>Get the CUDA architecture to use when compiling code for this device.</p>
<p>This method is intended for internal use by Warp’s compilation system.
External users should not need to call this method directly.</p>
<p>Determines the appropriate compute capability version to use when compiling
CUDA kernels for this device. The architecture depends on the device’s
CUDA output format preference and available target architectures.</p>
<p>For PTX output format, uses the minimum of the device’s architecture and
the configured PTX target architecture to ensure compatibility.
For CUBIN output format, uses the device’s exact architecture.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The compute capability version (e.g., 75 for <code class="docutils literal notranslate"><span class="pre">sm_75</span></code>) to use for compilation,
or <code class="docutils literal notranslate"><span class="pre">None</span></code> for CPU devices which don’t use CUDA compilation.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)">int</a> | None</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<p>Warp also provides functions that can be used to query the available devices on the system:</p>
<dl class="py function">
<dt class="sig sig-object py" id="warp.get_devices">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">get_devices</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/context.py#L4696-L4707"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.get_devices" title="Link to this definition">#</a></dt>
<dd><p>Returns a list of devices supported in this environment.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)">list</a>[<a class="reference internal" href="#warp.context.Device" title="warp.context.Device"><em>Device</em></a>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.get_cuda_devices">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">get_cuda_devices</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/context.py#L4728-L4734"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.get_cuda_devices" title="Link to this definition">#</a></dt>
<dd><p>Returns a list of CUDA devices supported in this environment.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)">list</a>[<a class="reference internal" href="#warp.context.Device" title="warp.context.Device"><em>Device</em></a>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.get_cuda_device_count">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">get_cuda_device_count</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/context.py#L4709-L4715"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.get_cuda_device_count" title="Link to this definition">#</a></dt>
<dd><p>Returns the number of CUDA devices supported in this environment.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)">int</a></p>
</dd>
</dl>
</dd></dl>

<section id="default-device">
<h2>Default Device<a class="headerlink" href="#default-device" title="Link to this heading">#</a></h2>
<p>To simplify writing code, Warp has the concept of <strong>default device</strong>.  When the <code class="docutils literal notranslate"><span class="pre">device</span></code> argument is omitted from a Warp API call, the default device will be used.</p>
<p>Calling <a class="reference internal" href="#warp.get_device" title="warp.get_device"><code class="xref py py-func docutils literal notranslate"><span class="pre">wp.get_device()</span></code></a> without an argument
will return an instance of <a class="reference internal" href="#warp.context.Device" title="warp.context.Device"><code class="xref py py-class docutils literal notranslate"><span class="pre">warp.context.Device</span></code></a> for the default device.</p>
<p>During Warp initialization, the default device is set to <code class="docutils literal notranslate"><span class="pre">&quot;cuda:0&quot;</span></code> if CUDA is available.  Otherwise, the default device is <code class="docutils literal notranslate"><span class="pre">&quot;cpu&quot;</span></code>.
If the default device is changed, <a class="reference internal" href="#warp.get_preferred_device" title="warp.get_preferred_device"><code class="xref py py-func docutils literal notranslate"><span class="pre">wp.get_preferred_device()</span></code></a> can be used to get
the <em>original</em> default device.</p>
<p><a class="reference internal" href="#warp.set_device" title="warp.set_device"><code class="xref py py-func docutils literal notranslate"><span class="pre">wp.set_device()</span></code></a> can be used to change the default device:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">wp</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">a</span><span class="p">])</span>

<span class="n">wp</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">b</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">b</span><span class="p">])</span>

<span class="n">wp</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="s2">&quot;cuda:1&quot;</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">c</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">c</span><span class="p">])</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For CUDA devices, <a class="reference internal" href="#warp.set_device" title="warp.set_device"><code class="xref py py-func docutils literal notranslate"><span class="pre">wp.set_device()</span></code></a> does two things: It sets the Warp default device and it makes the device’s CUDA context current.  This helps to minimize the number of CUDA context switches in blocks of code targeting a single device.</p>
</div>
<p>For PyTorch users, this function is similar to <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.cuda.set_device.html#torch.cuda.set_device" title="(in PyTorch v2.8)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cuda.set_device()</span></code></a>.
It is still possible to specify a different device in individual API calls, like in this snippet:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># set default device</span>
<span class="n">wp</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">)</span>

<span class="c1"># use default device</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="c1"># use explicit devices</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda:1&quot;</span><span class="p">)</span>

<span class="c1"># use default device</span>
<span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">a</span><span class="p">])</span>

<span class="n">wp</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
<span class="n">wp</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="warp.set_device">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">set_device</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ident</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/context.py#L4757-L4765"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.set_device" title="Link to this definition">#</a></dt>
<dd><p>Sets the default device identified by the argument.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>ident</strong> (<a class="reference internal" href="#warp.context.Device" title="warp.context.Device"><em>Device</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> | </em><em>None</em>)</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.get_device">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">get_device</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ident</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/context.py#L4749-L4755"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.get_device" title="Link to this definition">#</a></dt>
<dd><p>Returns the device identified by the argument.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>ident</strong> (<a class="reference internal" href="#warp.context.Device" title="warp.context.Device"><em>Device</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> | </em><em>None</em>)</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="#warp.context.Device" title="warp.context.Device"><em>Device</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.get_preferred_device">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">get_preferred_device</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/context.py#L4736-L4747"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.get_preferred_device" title="Link to this definition">#</a></dt>
<dd><p>Returns the preferred compute device, <code class="docutils literal notranslate"><span class="pre">cuda:0</span></code> if available and <code class="docutils literal notranslate"><span class="pre">cpu</span></code> otherwise.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#warp.context.Device" title="warp.context.Device"><em>Device</em></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="scoped-devices">
<h2>Scoped Devices<a class="headerlink" href="#scoped-devices" title="Link to this heading">#</a></h2>
<p>Another way to manage the default device is using <a class="reference internal" href="#warp.ScopedDevice" title="warp.ScopedDevice"><code class="xref py py-class docutils literal notranslate"><span class="pre">wp.ScopedDevice</span></code></a> objects.
They can be arbitrarily nested and restore the previous default device on exit:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">wp</span><span class="o">.</span><span class="n">ScopedDevice</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">):</span>
    <span class="c1"># alloc and launch on &quot;cpu&quot;</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">a</span><span class="p">])</span>

<span class="k">with</span> <span class="n">wp</span><span class="o">.</span><span class="n">ScopedDevice</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">):</span>
    <span class="c1"># alloc on &quot;cuda:0&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">wp</span><span class="o">.</span><span class="n">ScopedDevice</span><span class="p">(</span><span class="s2">&quot;cuda:1&quot;</span><span class="p">):</span>
        <span class="c1"># alloc and launch on &quot;cuda:1&quot;</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
        <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">c</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">c</span><span class="p">])</span>

    <span class="c1"># launch on &quot;cuda:0&quot;</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">b</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">b</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py class">
<dt class="sig sig-object py" id="warp.ScopedDevice">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">ScopedDevice</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/utils.py#L1194-L1236"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.ScopedDevice" title="Link to this definition">#</a></dt>
<dd><p>A context manager to temporarily change the current default device.</p>
<p>For CUDA devices, this context manager makes the device’s CUDA context
current and restores the previous CUDA context on exit. This is handy when
running Warp scripts as part of a bigger pipeline because it avoids any side
effects of changing the CUDA context in the enclosed code.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>device</strong> (<em>Devicelike</em>)</p>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="warp.ScopedDevice.device">
<span class="sig-name descname"><span class="pre">device</span></span><a class="headerlink" href="#warp.ScopedDevice.device" title="Link to this definition">#</a></dt>
<dd><p>The device that will temporarily become the default
device within the context.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#warp.context.Device" title="warp.context.Device">Device</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="warp.ScopedDevice.saved_device">
<span class="sig-name descname"><span class="pre">saved_device</span></span><a class="headerlink" href="#warp.ScopedDevice.saved_device" title="Link to this definition">#</a></dt>
<dd><p>The previous default device. This is restored as
the default device on exiting the context.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#warp.context.Device" title="warp.context.Device">Device</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="warp.ScopedDevice.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/utils.py#L1209-L1217"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.ScopedDevice.__init__" title="Link to this definition">#</a></dt>
<dd><p>Initializes the context manager with a device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference internal" href="#warp.context.Device" title="warp.context.Device"><em>Device</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> | </em><em>None</em>) – The device that will temporarily become the default device
within the context.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<section id="example-using-wp-scopeddevice-with-multiple-gpus">
<h3>Example: Using <code class="docutils literal notranslate"><span class="pre">wp.ScopedDevice</span></code> with multiple GPUs<a class="headerlink" href="#example-using-wp-scopeddevice-with-multiple-gpus" title="Link to this heading">#</a></h3>
<p>The following example shows how to allocate arrays and launch kernels on all available CUDA devices.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">warp</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">wp</span>


<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">inc</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">a</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">+</span> <span class="mf">1.0</span>


<span class="c1"># get all CUDA devices</span>
<span class="n">devices</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">get_cuda_devices</span><span class="p">()</span>
<span class="n">device_count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">devices</span><span class="p">)</span>

<span class="c1"># number of launches</span>
<span class="n">iters</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># list of arrays, one per device</span>
<span class="n">arrs</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># loop over all devices</span>
<span class="k">for</span> <span class="n">device</span> <span class="ow">in</span> <span class="n">devices</span><span class="p">:</span>
    <span class="c1"># use a ScopedDevice to set the target device</span>
    <span class="k">with</span> <span class="n">wp</span><span class="o">.</span><span class="n">ScopedDevice</span><span class="p">(</span><span class="n">device</span><span class="p">):</span>
        <span class="c1"># allocate array</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">250</span> <span class="o">*</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
        <span class="n">arrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

        <span class="c1"># launch kernels</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iters</span><span class="p">):</span>
            <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">inc</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">a</span><span class="p">])</span>

<span class="c1"># synchronize all devices</span>
<span class="n">wp</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>

<span class="c1"># print results</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">device_count</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">arrs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2"> -&gt; </span><span class="si">{</span><span class="n">arrs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="current-cuda-device">
<h2>Current CUDA Device<a class="headerlink" href="#current-cuda-device" title="Link to this heading">#</a></h2>
<p>Warp uses the device alias <code class="docutils literal notranslate"><span class="pre">&quot;cuda&quot;</span></code> to target the current CUDA device.  This allows external code to manage the CUDA device on which to execute Warp scripts.  It is analogous to the PyTorch <code class="docutils literal notranslate"><span class="pre">&quot;cuda&quot;</span></code> device, which should be familiar to Torch users and simplify interoperation.</p>
<p>In this snippet, we use PyTorch to manage the current CUDA device and invoke a Warp kernel on that device:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">example_function</span><span class="p">():</span>
    <span class="c1"># create a Torch tensor on the current CUDA device</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>

    <span class="n">a</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>

    <span class="c1"># launch a Warp kernel on the current CUDA device</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">a</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>

<span class="c1"># use Torch to set the current CUDA device and run example_function() on that device</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">example_function</span><span class="p">()</span>

<span class="c1"># use Torch to change the current CUDA device and re-run example_function() on that device</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">example_function</span><span class="p">()</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Using the device alias <code class="docutils literal notranslate"><span class="pre">&quot;cuda&quot;</span></code> can be problematic if the code runs in an environment where another part of the code can unpredictably change the CUDA context.  Using an explicit CUDA device like <code class="docutils literal notranslate"><span class="pre">&quot;cuda:i&quot;</span></code> is recommended to avoid such issues.</p>
</div>
</section>
<section id="device-synchronization">
<h2>Device Synchronization<a class="headerlink" href="#device-synchronization" title="Link to this heading">#</a></h2>
<p>CUDA kernel launches and memory operations can execute asynchronously.
This allows for overlapping compute and memory operations on different devices.
Warp allows synchronizing the host with outstanding asynchronous operations on a specific device:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">wp</span><span class="o">.</span><span class="n">synchronize_device</span><span class="p">(</span><span class="s2">&quot;cuda:1&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><a class="reference internal" href="#warp.synchronize_device" title="warp.synchronize_device"><code class="xref py py-func docutils literal notranslate"><span class="pre">wp.synchronize_device()</span></code></a> offers more fine-grained synchronization than
<a class="reference internal" href="#warp.synchronize" title="warp.synchronize"><code class="xref py py-func docutils literal notranslate"><span class="pre">wp.synchronize()</span></code></a>, as the latter waits for <em>all</em> devices to complete their work.</p>
<dl class="py function">
<dt class="sig sig-object py" id="warp.synchronize_device">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">synchronize_device</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/context.py#L6446-L6462"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.synchronize_device" title="Link to this definition">#</a></dt>
<dd><p>Synchronize the calling CPU thread with any outstanding CUDA work on the specified device</p>
<p>This function allows the host application code to ensure that all kernel launches
and memory copies have completed on the device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference internal" href="#warp.context.Device" title="warp.context.Device"><em>Device</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> | </em><em>None</em>) – Device to synchronize.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.synchronize">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">synchronize</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/context.py#L6422-L6444"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.synchronize" title="Link to this definition">#</a></dt>
<dd><p>Manually synchronize the calling CPU thread with any outstanding CUDA work on all devices</p>
<p>This method allows the host application code to ensure that any kernel launches
or memory copies have completed.</p>
</dd></dl>

</section>
<section id="custom-cuda-contexts">
<h2>Custom CUDA Contexts<a class="headerlink" href="#custom-cuda-contexts" title="Link to this heading">#</a></h2>
<p>Warp is designed to work with arbitrary CUDA contexts so it can easily integrate into different workflows.</p>
<p>Applications built on the CUDA Runtime API target the <em>primary context</em> of each device.  The Runtime API hides CUDA context management under the hood.  In Warp, device <code class="docutils literal notranslate"><span class="pre">&quot;cuda:i&quot;</span></code> represents the primary context of device <code class="docutils literal notranslate"><span class="pre">i</span></code>, which aligns with the CUDA Runtime API.</p>
<p>Applications built on the CUDA Driver API work with CUDA contexts directly and can create custom CUDA contexts on any device.  Custom CUDA contexts can be created with specific affinity or interop features that benefit the application.  Warp can work with these CUDA contexts as well.</p>
<p>The special device alias <code class="docutils literal notranslate"><span class="pre">&quot;cuda&quot;</span></code> can be used to target the current CUDA context, whether this is a primary or custom context.</p>
<p>In addition, Warp allows registering new device aliases for custom CUDA contexts using
<a class="reference internal" href="#warp.map_cuda_device" title="warp.map_cuda_device"><code class="xref py py-func docutils literal notranslate"><span class="pre">wp.map_cuda_device()</span></code></a> so that they can be explicitly targeted by name.
If the <code class="docutils literal notranslate"><span class="pre">CUcontext</span></code> pointer is available, it can be used to create a new device alias like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">wp</span><span class="o">.</span><span class="n">map_cuda_device</span><span class="p">(</span><span class="s2">&quot;foo&quot;</span><span class="p">,</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">c_void_p</span><span class="p">(</span><span class="n">context_ptr</span><span class="p">))</span>
</pre></div>
</div>
<p>Alternatively, if the custom CUDA context was made current by the application, the pointer can be omitted:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">wp</span><span class="o">.</span><span class="n">map_cuda_device</span><span class="p">(</span><span class="s2">&quot;foo&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>In either case, mapping the custom CUDA context allows us to target the context directly using the assigned alias:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">wp</span><span class="o">.</span><span class="n">ScopedDevice</span><span class="p">(</span><span class="s2">&quot;foo&quot;</span><span class="p">):</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">a</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="warp.map_cuda_device">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">map_cuda_device</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">alias</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">context</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/context.py#L4767-L4784"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.map_cuda_device" title="Link to this definition">#</a></dt>
<dd><p>Assign a device alias to a CUDA context.</p>
<p>This function can be used to create a wp.Device for an external CUDA context.
If a wp.Device already exists for the given context, it’s alias will change to the given value.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>alias</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – A unique string to identify the device.</p></li>
<li><p><strong>context</strong> (<a class="reference external" href="https://docs.python.org/3/library/ctypes.html#ctypes.c_void_p" title="(in Python v3.13)"><em>c_void_p</em></a><em> | </em><em>None</em>) – A CUDA context pointer (CUcontext).  If None, the currently bound CUDA context will be used.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The associated wp.Device.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#warp.context.Device" title="warp.context.Device"><em>Device</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.unmap_cuda_device">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">unmap_cuda_device</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">alias</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/context.py#L4786-L4792"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.unmap_cuda_device" title="Link to this definition">#</a></dt>
<dd><p>Remove a CUDA device with the given alias.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>alias</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>)</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="cuda-peer-access">
<span id="peer-access"></span><h2>CUDA Peer Access<a class="headerlink" href="#cuda-peer-access" title="Link to this heading">#</a></h2>
<p>CUDA allows direct memory access between different GPUs if the system hardware configuration supports it.  Typically, the GPUs should be of the same type and a special interconnect may be required (e.g., NVLINK or PCIe topology).</p>
<p>During initialization, Warp reports whether peer access is supported on multi-GPU systems:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Warp 0.15.1 initialized:
   CUDA Toolkit 11.5, Driver 12.2
   Devices:
     &quot;cpu&quot;      : &quot;x86_64&quot;
     &quot;cuda:0&quot;   : &quot;NVIDIA L40&quot; (48 GiB, sm_89, mempool enabled)
     &quot;cuda:1&quot;   : &quot;NVIDIA L40&quot; (48 GiB, sm_89, mempool enabled)
     &quot;cuda:2&quot;   : &quot;NVIDIA L40&quot; (48 GiB, sm_89, mempool enabled)
     &quot;cuda:3&quot;   : &quot;NVIDIA L40&quot; (48 GiB, sm_89, mempool enabled)
   CUDA peer access:
     Supported fully (all-directional)
</pre></div>
</div>
<p>If the message reports that CUDA peer access is <code class="docutils literal notranslate"><span class="pre">Supported</span> <span class="pre">fully</span></code>, it means that every CUDA device can access every other CUDA device in the system.  If it says <code class="docutils literal notranslate"><span class="pre">Supported</span> <span class="pre">partially</span></code>, it will be followed by the access matrix that shows which devices can access each other.  If it says <code class="docutils literal notranslate"><span class="pre">Not</span> <span class="pre">supported</span></code>, it means that access is not supported between any devices.</p>
<p>In code, we can check support and enable peer access like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">wp</span><span class="o">.</span><span class="n">is_peer_access_supported</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">,</span> <span class="s2">&quot;cuda:1&quot;</span><span class="p">):</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">set_peer_access_enabled</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">,</span> <span class="s2">&quot;cuda:1&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">):</span>
</pre></div>
</div>
<p>This will allow the memory of device <code class="docutils literal notranslate"><span class="pre">cuda:0</span></code> to be directly accessed on device <code class="docutils literal notranslate"><span class="pre">cuda:1</span></code>.  Peer access is directional, which means that enabling access to <code class="docutils literal notranslate"><span class="pre">cuda:0</span></code> from <code class="docutils literal notranslate"><span class="pre">cuda:1</span></code> does not automatically enable access to <code class="docutils literal notranslate"><span class="pre">cuda:1</span></code> from <code class="docutils literal notranslate"><span class="pre">cuda:0</span></code>.</p>
<p>The benefit of enabling peer access is that it allows direct memory transfers (DMA) between the devices.  This is generally a faster way to copy data, since otherwise the transfer needs to be done using a CPU staging buffer.</p>
<p>The drawback is that enabling peer access can reduce the performance of allocations and deallocations.  Programs that don’t rely on peer-to-peer memory transfers should leave this setting disabled.</p>
<p>It’s possible to temporarily enable or disable peer access using a scoped manager:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">wp</span><span class="o">.</span><span class="n">ScopedPeerAccess</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">,</span> <span class="s2">&quot;cuda:1&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">):</span>
    <span class="o">...</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Peer access does not accelerate memory transfers between arrays allocated using the <a class="reference internal" href="allocators.html#mempool-allocators"><span class="std std-ref">stream-ordered memory pool allocators</span></a> introduced in Warp 0.14.0.
To accelerate memory pool transfers, <a class="reference internal" href="allocators.html#mempool-access"><span class="std std-ref">memory pool access</span></a> should be enabled instead.</p>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="warp.is_peer_access_supported">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">is_peer_access_supported</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target_device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">peer_device</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/context.py#L4992-L5011"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.is_peer_access_supported" title="Link to this definition">#</a></dt>
<dd><p>Check if <cite>peer_device</cite> can directly access the memory of <cite>target_device</cite> on this system.</p>
<p>This applies to memory allocated using default CUDA allocators.  For memory allocated using
CUDA pooled allocators, use <a class="reference internal" href="allocators.html#warp.is_mempool_access_supported" title="warp.is_mempool_access_supported"><code class="xref py py-func docutils literal notranslate"><span class="pre">is_mempool_access_supported()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A Boolean value indicating if this peer access is supported by the system.</p>
</dd>
<dt class="field-even">Parameters<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>target_device</strong> (<a class="reference internal" href="#warp.context.Device" title="warp.context.Device"><em>Device</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> | </em><em>None</em>)</p></li>
<li><p><strong>peer_device</strong> (<a class="reference internal" href="#warp.context.Device" title="warp.context.Device"><em>Device</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> | </em><em>None</em>)</p></li>
</ul>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="functions.html#warp.bool" title="warp.bool">bool</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.is_peer_access_enabled">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">is_peer_access_enabled</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target_device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">peer_device</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/context.py#L5013-L5032"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.is_peer_access_enabled" title="Link to this definition">#</a></dt>
<dd><p>Check if <cite>peer_device</cite> can currently access the memory of <cite>target_device</cite>.</p>
<p>This applies to memory allocated using default CUDA allocators.  For memory allocated using
CUDA pooled allocators, use <a class="reference internal" href="allocators.html#warp.is_mempool_access_enabled" title="warp.is_mempool_access_enabled"><code class="xref py py-func docutils literal notranslate"><span class="pre">is_mempool_access_enabled()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A Boolean value indicating if this peer access is currently enabled.</p>
</dd>
<dt class="field-even">Parameters<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>target_device</strong> (<a class="reference internal" href="#warp.context.Device" title="warp.context.Device"><em>Device</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> | </em><em>None</em>)</p></li>
<li><p><strong>peer_device</strong> (<a class="reference internal" href="#warp.context.Device" title="warp.context.Device"><em>Device</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> | </em><em>None</em>)</p></li>
</ul>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="functions.html#warp.bool" title="warp.bool">bool</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.set_peer_access_enabled">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">set_peer_access_enabled</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target_device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">peer_device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.9.1/warp/context.py#L5034-L5064"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.set_peer_access_enabled" title="Link to this definition">#</a></dt>
<dd><p>Enable or disable direct access from <cite>peer_device</cite> to the memory of <cite>target_device</cite>.</p>
<p>Enabling peer access can improve the speed of peer-to-peer memory transfers, but can have
a negative impact on memory consumption and allocation performance.</p>
<p>This applies to memory allocated using default CUDA allocators.  For memory allocated using
CUDA pooled allocators, use <a class="reference internal" href="allocators.html#warp.set_mempool_access_enabled" title="warp.set_mempool_access_enabled"><code class="xref py py-func docutils literal notranslate"><span class="pre">set_mempool_access_enabled()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>target_device</strong> (<a class="reference internal" href="#warp.context.Device" title="warp.context.Device"><em>Device</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> | </em><em>None</em>)</p></li>
<li><p><strong>peer_device</strong> (<a class="reference internal" href="#warp.context.Device" title="warp.context.Device"><em>Device</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> | </em><em>None</em>)</p></li>
<li><p><strong>enable</strong> (<a class="reference internal" href="functions.html#warp.bool" title="warp.bool"><em>bool</em></a>)</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
</dd></dl>

</section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../basics.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Basics</p>
      </div>
    </a>
    <a class="right-next"
       href="differentiability.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Differentiability</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            


              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.context.Device"><code class="docutils literal notranslate"><span class="pre">Device</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.context.Device.ordinal"><code class="docutils literal notranslate"><span class="pre">ordinal</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.context.Device.name"><code class="docutils literal notranslate"><span class="pre">name</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.context.Device.arch"><code class="docutils literal notranslate"><span class="pre">arch</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.context.Device.sm_count"><code class="docutils literal notranslate"><span class="pre">sm_count</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.context.Device.is_uva"><code class="docutils literal notranslate"><span class="pre">is_uva</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.context.Device.is_cubin_supported"><code class="docutils literal notranslate"><span class="pre">is_cubin_supported</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.context.Device.is_mempool_supported"><code class="docutils literal notranslate"><span class="pre">is_mempool_supported</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.context.Device.is_ipc_supported"><code class="docutils literal notranslate"><span class="pre">is_ipc_supported</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.context.Device.is_primary"><code class="docutils literal notranslate"><span class="pre">is_primary</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.context.Device.uuid"><code class="docutils literal notranslate"><span class="pre">uuid</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.context.Device.pci_bus_id"><code class="docutils literal notranslate"><span class="pre">pci_bus_id</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.context.Device.__init__"><code class="docutils literal notranslate"><span class="pre">__init__()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.context.Device.get_allocator"><code class="docutils literal notranslate"><span class="pre">get_allocator()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.context.Device.is_cpu"><code class="docutils literal notranslate"><span class="pre">is_cpu</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.context.Device.is_cuda"><code class="docutils literal notranslate"><span class="pre">is_cuda</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.context.Device.is_capturing"><code class="docutils literal notranslate"><span class="pre">is_capturing</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.context.Device.context"><code class="docutils literal notranslate"><span class="pre">context</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.context.Device.has_context"><code class="docutils literal notranslate"><span class="pre">has_context</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.context.Device.stream"><code class="docutils literal notranslate"><span class="pre">stream</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.context.Device.set_stream"><code class="docutils literal notranslate"><span class="pre">set_stream()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.context.Device.has_stream"><code class="docutils literal notranslate"><span class="pre">has_stream</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.context.Device.total_memory"><code class="docutils literal notranslate"><span class="pre">total_memory</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.context.Device.free_memory"><code class="docutils literal notranslate"><span class="pre">free_memory</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.context.Device.make_current"><code class="docutils literal notranslate"><span class="pre">make_current()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.context.Device.can_access"><code class="docutils literal notranslate"><span class="pre">can_access()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.context.Device.get_cuda_output_format"><code class="docutils literal notranslate"><span class="pre">get_cuda_output_format()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.context.Device.get_cuda_compile_arch"><code class="docutils literal notranslate"><span class="pre">get_cuda_compile_arch()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.get_devices"><code class="docutils literal notranslate"><span class="pre">get_devices()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.get_cuda_devices"><code class="docutils literal notranslate"><span class="pre">get_cuda_devices()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.get_cuda_device_count"><code class="docutils literal notranslate"><span class="pre">get_cuda_device_count()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#default-device">Default Device</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.set_device"><code class="docutils literal notranslate"><span class="pre">set_device()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.get_device"><code class="docutils literal notranslate"><span class="pre">get_device()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.get_preferred_device"><code class="docutils literal notranslate"><span class="pre">get_preferred_device()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scoped-devices">Scoped Devices</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.ScopedDevice"><code class="docutils literal notranslate"><span class="pre">ScopedDevice</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.ScopedDevice.device"><code class="docutils literal notranslate"><span class="pre">device</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.ScopedDevice.saved_device"><code class="docutils literal notranslate"><span class="pre">saved_device</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.ScopedDevice.__init__"><code class="docutils literal notranslate"><span class="pre">__init__()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-using-wp-scopeddevice-with-multiple-gpus">Example: Using <code class="docutils literal notranslate"><span class="pre">wp.ScopedDevice</span></code> with multiple GPUs</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#current-cuda-device">Current CUDA Device</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#device-synchronization">Device Synchronization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.synchronize_device"><code class="docutils literal notranslate"><span class="pre">synchronize_device()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.synchronize"><code class="docutils literal notranslate"><span class="pre">synchronize()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#custom-cuda-contexts">Custom CUDA Contexts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.map_cuda_device"><code class="docutils literal notranslate"><span class="pre">map_cuda_device()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.unmap_cuda_device"><code class="docutils literal notranslate"><span class="pre">unmap_cuda_device()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cuda-peer-access">CUDA Peer Access</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.is_peer_access_supported"><code class="docutils literal notranslate"><span class="pre">is_peer_access_supported()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.is_peer_access_enabled"><code class="docutils literal notranslate"><span class="pre">is_peer_access_enabled()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warp.set_peer_access_enabled"><code class="docutils literal notranslate"><span class="pre">set_peer_access_enabled()</span></code></a></li>
</ul>
</li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">

  
  <div class="tocsection editthispage">
    <a href="https://github.com/NVIDIA/warp/edit/v1.9.1/docs/modules/devices.rst">
      <i class="fa-solid fa-pencil"></i>
      
      
        
          Edit on GitHub
        
      
    </a>
  </div>
</div>

</div></div>
              
            

          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">
<a class="footer-brand logo" href="https://www.nvidia.com">
  <img src="../_static/nvidia-logo-horiz-rgb-1c-blk-for-screen.svg" class="logo__image only-light" alt="NVIDIA"/>
  <img src="../_static/nvidia-logo-horiz-rgb-1c-wht-for-screen.svg" class="logo__image only-dark" alt="NVIDIA"/>
</a></div>
      
        <div class="footer-item">




  <p class="copyright">
    
      Copyright © 2022-2025 NVIDIA Corporation.
      <br/>
    
  </p>
</div>
      
    </div>
  
  
  
</div>

  </footer>
  </body>
</html>